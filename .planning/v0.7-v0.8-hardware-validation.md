# Hardware Validation Plan: v0.7.0 & v0.8.0

**Created:** 2026-02-04
**Target:** Metal cluster hardware validation
**Milestones:** v0.7.0 (State Management) + v0.8.0 (Code Quality)

---

## Overview

Neither v0.7.0 nor v0.8.0 have been deployed and validated on the metal cluster. This plan covers:
1. Building and deploying v0.8.0 image
2. Validating v0.7.0 features (VolumeAttachment-based state rebuild, migration metrics)
3. Validating v0.8.0 features (logging cleanup, error handling)

---

## Part 1: Build and Deploy v0.8.0

### Step 1: Build v0.8.0 Image

```bash
# Checkout v0.8.0 tag
git checkout v0.8.0

# Build Docker image with v0.8.0 tag
make docker IMAGE_TAG=v0.8.0

# Or build with full image specification
docker build -t ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.0 .

# Tag as latest (optional)
docker tag ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.0 ghcr.io/3whiskeywhiskey/rds-csi-driver:latest
```

### Step 2: Push Image to Registry

```bash
# Login to GitHub Container Registry
echo $GITHUB_TOKEN | docker login ghcr.io -u 3whiskeywhiskey --password-stdin

# Push v0.8.0 image
docker push ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.0

# Push latest (optional)
docker push ghcr.io/3whiskeywhiskey/rds-csi-driver:latest
```

### Step 3: Update Deployment Manifests

**Option A: Temporary override (testing)**

```bash
# Edit controller.yaml to use v0.8.0 tag
kubectl set image deployment/rds-csi-controller \
  rds-csi-plugin=ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.0 \
  -n kube-system

# Edit node daemonset
kubectl set image daemonset/rds-csi-node \
  rds-csi-plugin=ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.0 \
  -n kube-system
```

**Option B: Update manifests (persistent)**

Edit `deploy/kubernetes/controller.yaml` and `deploy/kubernetes/node.yaml`:
- Change `image: ghcr.io/3whiskeywhiskey/rds-csi:dev`
- To: `image: ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.0`

Then apply:
```bash
kubectl apply -f deploy/kubernetes/
```

### Step 4: Verify Deployment

```bash
# Check controller is running with new image
kubectl get pods -n kube-system -l app=rds-csi-controller -o jsonpath='{.items[*].spec.containers[0].image}'

# Check node plugin is running with new image
kubectl get pods -n kube-system -l app=rds-csi-node -o jsonpath='{.items[*].spec.containers[0].image}'

# Check logs for startup
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin --tail=50

# Verify all pods are ready
kubectl get pods -n kube-system -l 'app in (rds-csi-controller,rds-csi-node)'
```

---

## Part 2: Validate v0.7.0 Features

### Test 1: VolumeAttachment-Based State Rebuild

**Feature:** Controller rebuilds attachment state from VolumeAttachment objects (not PV annotations)

**Test Procedure:**

1. Create a test PVC and attach it to a pod:
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-va-rebuild
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-va-rebuild-pod
spec:
  containers:
  - name: test
    image: busybox
    command: [sleep, "3600"]
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: test-va-rebuild
EOF
```

2. Wait for pod to be running and volume attached:
```bash
kubectl wait --for=condition=Ready pod/test-va-rebuild-pod --timeout=120s
```

3. Check VolumeAttachment object exists:
```bash
kubectl get volumeattachment
kubectl describe volumeattachment | grep test-va-rebuild
```

4. Restart controller to trigger state rebuild:
```bash
kubectl rollout restart deployment/rds-csi-controller -n kube-system
kubectl rollout status deployment/rds-csi-controller -n kube-system
```

5. Verify state was rebuilt from VolumeAttachment:
```bash
# Check controller logs for "rebuilding state from VolumeAttachments"
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin | grep -i "rebuild"

# Verify attachment still tracked after restart
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin | grep test-va-rebuild
```

6. Verify pod still works (no disruption):
```bash
kubectl exec test-va-rebuild-pod -- touch /data/test-file
kubectl exec test-va-rebuild-pod -- ls -la /data/test-file
```

**Success Criteria:**
- ✅ Controller logs show "rebuilding state from VolumeAttachments" after restart
- ✅ Pod continues working without interruption
- ✅ No errors about lost attachment state
- ✅ PV annotations are NOT read (only written)

**Cleanup:**
```bash
kubectl delete pod test-va-rebuild-pod
kubectl delete pvc test-va-rebuild
```

---

### Test 2: Migration Metrics Emission

**Feature:** AttachmentManager exports migration metrics (count, duration, active)

**Test Procedure:**

1. Create a block-mode PVC for migration test:
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-migration-metrics
spec:
  accessModes: [ReadWriteMany]
  volumeMode: Block
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 2Gi
EOF
```

2. Create a KubeVirt VM using the PVC:
```bash
cat <<EOF | kubectl apply -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: test-migration-vm
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          disks:
          - name: datadisk
            disk:
              bus: virtio
        resources:
          requests:
            memory: 512Mi
      volumes:
      - name: datadisk
        persistentVolumeClaim:
          claimName: test-migration-metrics
EOF
```

3. Wait for VM to be running:
```bash
kubectl wait --for=condition=Ready vmi/test-migration-vm --timeout=180s
```

4. Query Prometheus metrics before migration:
```bash
# Port-forward to controller metrics endpoint
kubectl port-forward -n kube-system deployment/rds-csi-controller 9809:9809 &
PF_PID=$!

# Query metrics
curl -s http://localhost:9809/metrics | grep rds_csi_migration

# Expected metrics:
# - rds_csi_migration_total
# - rds_csi_migration_duration_seconds
# - rds_csi_migration_active

kill $PF_PID
```

5. Trigger VM live migration:
```bash
cat <<EOF | kubectl apply -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: test-migration
spec:
  vmiName: test-migration-vm
EOF
```

6. Monitor migration:
```bash
kubectl get vmim test-migration -w
```

7. Query metrics after migration completes:
```bash
kubectl port-forward -n kube-system deployment/rds-csi-controller 9809:9809 &
PF_PID=$!

curl -s http://localhost:9809/metrics | grep rds_csi_migration

# Verify:
# - rds_csi_migration_total incremented
# - rds_csi_migration_duration_seconds recorded
# - rds_csi_migration_active returned to 0

kill $PF_PID
```

**Success Criteria:**
- ✅ Metrics endpoint accessible on port 9809
- ✅ `rds_csi_migration_total` counter increments after migration
- ✅ `rds_csi_migration_duration_seconds` histogram has samples
- ✅ `rds_csi_migration_active` gauge shows 1 during migration, 0 after
- ✅ Controller logs show migration events

**Cleanup:**
```bash
kubectl delete vmim test-migration
kubectl delete vm test-migration-vm
kubectl delete pvc test-migration-metrics
```

---

### Test 3: PV Annotations Informational-Only

**Feature:** PV annotations are written but never read (VolumeAttachment is source of truth)

**Test Procedure:**

1. Create and attach a volume:
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-annotation
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-annotation-pod
spec:
  containers:
  - name: test
    image: busybox
    command: [sleep, "3600"]
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: test-annotation
EOF
```

2. Wait for attachment and check PV has annotations:
```bash
kubectl wait --for=condition=Ready pod/test-annotation-pod --timeout=120s

PV_NAME=$(kubectl get pvc test-annotation -o jsonpath='{.spec.volumeName}')
kubectl get pv $PV_NAME -o jsonpath='{.metadata.annotations}' | jq .
```

3. Manually corrupt PV annotations (should have no effect):
```bash
kubectl annotate pv $PV_NAME rds-csi.srvlab.io/attachments='{"fake-node": {"timestamp": "2020-01-01T00:00:00Z"}}' --overwrite
```

4. Restart controller:
```bash
kubectl rollout restart deployment/rds-csi-controller -n kube-system
kubectl rollout status deployment/rds-csi-controller -n kube-system
```

5. Verify pod still works (corrupted annotation ignored):
```bash
kubectl exec test-annotation-pod -- touch /data/test-file
kubectl exec test-annotation-pod -- ls -la /data/test-file
```

6. Check controller logs don't mention reading annotations:
```bash
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin | grep -i "annotation" | grep -i "read"
# Should show writes but NO reads
```

**Success Criteria:**
- ✅ Corrupted PV annotation does NOT affect attachment state
- ✅ Pod continues working normally
- ✅ Controller logs show annotation writes but no annotation reads
- ✅ VolumeAttachment is used for state rebuild

**Cleanup:**
```bash
kubectl delete pod test-annotation-pod
kubectl delete pvc test-annotation
```

---

## Part 3: Validate v0.8.0 Features

### Test 4: Logging Cleanup Validation

**Feature:** Production logs contain only actionable information, V(3) eliminated

**Test Procedure:**

1. Set controller to production log level (V=2):
```bash
# Check current verbosity setting
kubectl get deployment rds-csi-controller -n kube-system -o jsonpath='{.spec.template.spec.containers[0].args}' | jq .

# If not already V=2, update it
kubectl set env deployment/rds-csi-controller -n kube-system -c rds-csi-plugin VERBOSITY=2
```

2. Perform volume operations:
```bash
# Create volume
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-logging
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 1Gi
EOF

# Wait for provisioning
kubectl wait --for=jsonpath='{.status.phase}'=Bound pvc/test-logging --timeout=60s

# Delete volume
kubectl delete pvc test-logging
```

3. Check logs for V(2) compliance:
```bash
# Get logs from CreateVolume and DeleteVolume operations
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin --tail=100 > /tmp/v08-logs.txt

# Verify DeleteVolume has max 1 V(2) log statement (not 6)
grep -i "deletevolume" /tmp/v08-logs.txt | grep -v "V(4)" | wc -l
# Expected: ~1 line (down from 6 in v0.7.0)

# Verify NO V(3) logs present
grep "V(3)" /tmp/v08-logs.txt
# Expected: no matches

# Verify logs are clean and actionable
cat /tmp/v08-logs.txt | grep -E "(Created|Deleted|Failed)" | head -20
```

4. Test security logger consolidation:
```bash
# Trigger security events
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-security-logging
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 1Gi
EOF

# Check security logs use consolidated table-driven helper
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin | grep -i "security" | tail -20

# Cleanup
kubectl delete pvc test-security-logging
```

**Success Criteria:**
- ✅ DeleteVolume operation produces max 1 log at V(2) level (down from 6)
- ✅ NO V(3) logs present in output (100% elimination verified)
- ✅ All logs at V(2) are actionable (outcomes, errors, security events only)
- ✅ Security logger uses consolidated table-driven helper
- ✅ Log output is clean and easy to parse

**Observed Output Example:**
```
I0204 12:00:00.123456       1 controller.go:145] Created volume pvc-abc123: 1GiB on RDS
I0204 12:00:05.234567       1 controller.go:198] Deleted volume pvc-abc123
```

---

### Test 5: Error Handling Validation

**Feature:** Errors use %w wrapping, sentinel errors, proper context

**Test Procedure:**

1. Trigger a "volume not found" error:
```bash
# Try to delete a non-existent volume (simulate controller crash during cleanup)
# This requires manual SSH to RDS - document expected behavior

# Expected: Error should be wrapped with ErrVolumeNotFound sentinel
# Expected: Error should include context (volume ID, operation, reason)
```

2. Trigger a "resource exhausted" error:
```bash
# Create a PVC larger than available space on RDS
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-capacity-error
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 10Ti  # Way larger than available
EOF

# Wait and check error event
kubectl describe pvc test-capacity-error | grep -A 5 Events

# Expected error message should include:
# - "resource exhausted" or similar
# - Volume size requested
# - Available capacity
# - Operation that failed (CreateVolume)

# Cleanup
kubectl delete pvc test-capacity-error
```

3. Check logs for proper error wrapping:
```bash
kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin | grep -i error | tail -20

# Verify error messages include:
# - Operation context (CreateVolume, DeleteVolume, etc.)
# - Volume ID (if applicable)
# - Node ID (if applicable)
# - Specific reason for failure
```

4. Test sentinel error integration:
```bash
# Check controller code uses errors.Is() for type-safe checks
# This is a code review check, not runtime test

# Runtime: trigger errors and verify they're properly classified
# (already done in capacity error test above)
```

**Success Criteria:**
- ✅ Errors include full context (operation, volume ID, reason)
- ✅ Resource exhausted errors properly detected and reported
- ✅ Error messages are clear and actionable for operators
- ✅ No silent failures or missing context
- ✅ Kubernetes events contain helpful error information

---

## Part 4: Regression Testing

### Test 6: Basic Volume Lifecycle

**Purpose:** Ensure v0.8.0 doesn't break core functionality

```bash
# Create PVC
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-basic-lifecycle
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 5Gi
EOF

# Create pod using PVC
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-lifecycle-pod
spec:
  containers:
  - name: test
    image: busybox
    command: [sleep, "3600"]
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: test-basic-lifecycle
EOF

# Wait and test
kubectl wait --for=condition=Ready pod/test-lifecycle-pod --timeout=120s
kubectl exec test-lifecycle-pod -- dd if=/dev/zero of=/data/testfile bs=1M count=100
kubectl exec test-lifecycle-pod -- ls -lh /data/testfile

# Cleanup
kubectl delete pod test-lifecycle-pod
kubectl delete pvc test-basic-lifecycle
```

**Success Criteria:**
- ✅ PVC provisions successfully
- ✅ Pod mounts volume without errors
- ✅ Can write and read data
- ✅ Cleanup completes without errors

---

### Test 7: Block Volume Support

**Purpose:** Verify v0.6.0 block volume features still work

```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-block-volume
spec:
  accessModes: [ReadWriteOnce]
  volumeMode: Block
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-block-pod
spec:
  containers:
  - name: test
    image: busybox
    command: [sleep, "3600"]
    volumeDevices:
    - name: data
      devicePath: /dev/xvda
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: test-block-volume
EOF

kubectl wait --for=condition=Ready pod/test-block-pod --timeout=120s
kubectl exec test-block-pod -- ls -l /dev/xvda

# Cleanup
kubectl delete pod test-block-pod
kubectl delete pvc test-block-volume
```

**Success Criteria:**
- ✅ Block PVC provisions
- ✅ Block device accessible in pod
- ✅ No metadata files created (CSI spec compliance)

---

## Part 5: Verification Checklist

### v0.7.0 Features
- [ ] VolumeAttachment-based state rebuild works after controller restart
- [ ] Migration metrics visible on :9809/metrics endpoint
- [ ] PV annotations written but not read (informational-only)
- [ ] No stale attachment state bugs

### v0.8.0 Features
- [ ] DeleteVolume produces max 1 log at V(2) (down from 6)
- [ ] Zero V(3) logs in production (100% elimination)
- [ ] Errors include full context and proper wrapping
- [ ] Security logger consolidated (table-driven helper)
- [ ] Logs are clean and actionable

### Regression Testing
- [ ] Basic volume lifecycle works (create, attach, write, delete)
- [ ] Block volumes work (v0.6.0 feature)
- [ ] KubeVirt VM live migration works (v0.5.0 feature)
- [ ] No performance degradation

---

## Success Criteria

**All tests pass:**
- 7/7 validation tests successful
- No regressions in existing features
- Production logs demonstrate V(2) compliance
- Error handling demonstrates proper wrapping and context

**Hardware confidence:**
- v0.7.0 state management validated on real cluster
- v0.8.0 logging/error improvements verified in production
- Ready to proceed with next milestone development

---

**Created:** 2026-02-04
**Status:** Ready to execute
**Estimated time:** 2-3 hours for full validation suite
