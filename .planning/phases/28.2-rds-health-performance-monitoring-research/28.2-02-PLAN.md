---
phase: 28.2-rds-health-performance-monitoring-research
plan: 02
type: execute
wave: 2
depends_on: ["28.2-01"]
files_modified:
  - pkg/observability/prometheus.go
  - pkg/observability/prometheus_test.go
  - pkg/driver/driver.go
autonomous: true

must_haves:
  truths:
    - "RDS disk performance metrics (rds_disk_*) appear in /metrics endpoint from SSH polling"
    - "RDS hardware health metrics (rds_hardware_*) appear in /metrics endpoint from SNMP polling"
    - "Disk metrics use rds_disk namespace with slot label (e.g., rds_disk_read_ops_per_second{slot=storage-pool})"
    - "Hardware metrics use rds_hardware namespace (e.g., rds_hardware_cpu_temperature_celsius)"
    - "GaugeFunc polls RDS via SSH and SNMP only when Prometheus scrapes (no background goroutines)"
    - "Metrics do NOT appear in node plugin /metrics (only controller has RDS client)"
    - "SSH/SNMP errors return 0 for metric values (no scrape failure on transient errors)"
  artifacts:
    - path: "pkg/observability/prometheus.go"
      provides: "SetRDSMonitoring method registering GaugeFunc collectors for both disk and hardware metrics"
      contains: "SetRDSMonitoring"
    - path: "pkg/observability/prometheus_test.go"
      provides: "Unit tests for RDS disk and hardware metric registration and GaugeFunc behavior"
      contains: "TestRDSMetrics"
    - path: "pkg/driver/driver.go"
      provides: "Wiring of RDS monitoring (SSH + SNMP) into metrics during controller init"
      contains: "SetRDSMonitoring"
    - path: "docs/MONITORING_DESIGN.md"
      provides: "Documentation of dual-approach monitoring (SSH for performance, SNMP for health)"
      contains: "rds_disk"
  key_links:
    - from: "pkg/driver/driver.go"
      to: "pkg/observability/prometheus.go"
      via: "SetRDSMonitoring call after RDS client connection"
      pattern: "SetRDSMonitoring"
    - from: "pkg/observability/prometheus.go"
      to: "pkg/rds/client.go"
      via: "GaugeFunc callbacks call GetDiskMetrics (SSH) and GetHardwareHealth (SNMP)"
      pattern: "GetDiskMetrics|GetHardwareHealth"
---

<objective>
Register RDS monitoring metrics (disk performance + hardware health) as Prometheus GaugeFunc collectors.

Purpose: Expose comprehensive RDS monitoring through the controller's /metrics endpoint via dual approach:
- SSH polling: Disk performance (IOPS, throughput, latency, queue depth) from /disk monitor-traffic
- SNMP polling: Hardware health (temperatures, fan speeds, PSU status, disk capacity) from MIKROTIK-MIB

Uses lightweight SNMP for hardware metrics while reserving SSH for performance data unavailable via SNMP.
Follows the established GaugeFunc pattern from Phase 28.1 (SetAttachmentManager).

Output: SetRDSMonitoring method on Metrics, 19 GaugeFunc metric registrations (9 disk + 10 hardware),
driver.go wiring with SNMP host configuration, unit tests, and MONITORING_DESIGN.md documentation.
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-RESEARCH.md
@.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-01-SUMMARY.md

@pkg/observability/prometheus.go
@pkg/observability/prometheus_test.go
@pkg/driver/driver.go
@pkg/rds/client.go
@pkg/rds/types.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SetRDSMonitoring to Prometheus metrics with GaugeFunc collectors for disk and hardware metrics</name>
  <files>pkg/observability/prometheus.go, pkg/observability/prometheus_test.go</files>
  <action>
1. **Add `SetRDSMonitoring` method** to `pkg/observability/prometheus.go` after the existing `SetAttachmentManager` method. This follows the exact same late-registration GaugeFunc pattern but handles both disk performance (SSH) and hardware health (SNMP) metrics.

The method accepts a callback function `func() (*rds.DiskMetrics, error)` rather than the RDSClient directly (avoids import cycle, matches Phase 28.1 pattern of using `func() int` callback).

IMPORTANT: Do NOT import `pkg/rds` in the observability package. Use a callback function type that returns the metrics values. Define a local struct or use individual callbacks. The cleanest approach: accept a single callback `func() (readIOPS, writeIOPS, readBytesPerSec, writeBytesPerSec, readTimeMs, writeTimeMs, waitTimeMs, inFlightOps, activeTimeMs float64, err error)` -- BUT that's unwieldy.

Better approach: Define simple snapshot structs in the observability package (no import needed):

```go
// DiskHealthSnapshot holds a point-in-time disk performance snapshot.
// Used as return type for the RDS disk monitoring callback to avoid
// importing pkg/rds in the observability package (prevents import cycles).
type DiskHealthSnapshot struct {
    ReadOpsPerSecond  float64
    WriteOpsPerSecond float64
    ReadBytesPerSec   float64
    WriteBytesPerSec  float64
    ReadTimeMs        float64
    WriteTimeMs       float64
    WaitTimeMs        float64
    InFlightOps       float64
    ActiveTimeMs      float64
}

// HardwareHealthSnapshot holds a point-in-time hardware health snapshot from SNMP.
// Used as return type for the RDS hardware monitoring callback to avoid
// importing pkg/rds in the observability package (prevents import cycles).
type HardwareHealthSnapshot struct {
    CPUTemperature    float64
    BoardTemperature  float64
    Fan1Speed         float64
    Fan2Speed         float64
    PSU1Power         float64
    PSU2Power         float64
    PSU1Temperature   float64
    PSU2Temperature   float64
    DiskPoolSizeBytes float64
    DiskPoolUsedBytes float64
}
```

Add fields to the Metrics struct:
```go
rdsDiskMetricsFunc     func() (*DiskHealthSnapshot, error)     // Callback for RDS disk performance metrics (SSH)
rdsHardwareMetricsFunc func() (*HardwareHealthSnapshot, error) // Callback for RDS hardware health metrics (SNMP)
```

Add the method:
```go
// SetRDSMonitoring registers GaugeFunc metrics for RDS monitoring (disk performance + hardware health).
//
// The diskMetricsFunc callback is invoked during Prometheus scrape to fetch disk performance
// data via SSH (/disk monitor-traffic). The hardwareMetricsFunc callback fetches hardware health
// via SNMP (temperature, fans, PSU, disk capacity).
//
// This must be called after the RDS client is connected. If not called (e.g., node plugin),
// RDS metrics are not registered.
//
// Metrics registered (all gauges, polled on scrape):
//   Disk Performance (9 metrics via SSH):
//     - rds_disk_read_ops_per_second{slot=<slot>}
//     - rds_disk_write_ops_per_second{slot=<slot>}
//     - rds_disk_read_bytes_per_second{slot=<slot>}
//     - rds_disk_write_bytes_per_second{slot=<slot>}
//     - rds_disk_read_latency_milliseconds{slot=<slot>}
//     - rds_disk_write_latency_milliseconds{slot=<slot>}
//     - rds_disk_wait_latency_milliseconds{slot=<slot>}
//     - rds_disk_in_flight_operations{slot=<slot>}
//     - rds_disk_active_time_milliseconds{slot=<slot>}
//   Hardware Health (10 metrics via SNMP):
//     - rds_hardware_cpu_temperature_celsius
//     - rds_hardware_board_temperature_celsius
//     - rds_hardware_fan1_speed_rpm
//     - rds_hardware_fan2_speed_rpm
//     - rds_hardware_psu1_power_watts
//     - rds_hardware_psu2_power_watts
//     - rds_hardware_psu1_temperature_celsius
//     - rds_hardware_psu2_temperature_celsius
//     - rds_hardware_disk_pool_size_bytes
//     - rds_hardware_disk_pool_used_bytes
func (m *Metrics) SetRDSMonitoring(slot string, snmpHost string, snmpCommunity string, diskMetricsFunc func() (*DiskHealthSnapshot, error), hardwareMetricsFunc func() (*HardwareHealthSnapshot, error)) {
    m.rdsDiskMetricsFunc = diskMetricsFunc
    m.rdsHardwareMetricsFunc = hardwareMetricsFunc

    // Helpers: fetch cached snapshots to avoid multiple SSH/SNMP calls per scrape.
    // Prometheus scrapes all metrics at once, so we cache results for 1 second.
    var (
        cachedDiskSnapshot     *DiskHealthSnapshot
        cachedHardwareSnapshot *HardwareHealthSnapshot
        diskCacheTime          time.Time
        hardwareCacheTime      time.Time
        cacheMu                sync.Mutex
    )

    getDiskSnapshot := func() *DiskHealthSnapshot {
        cacheMu.Lock()
        defer cacheMu.Unlock()

        // Cache for 1 second to avoid 9 SSH calls per scrape
        if cachedDiskSnapshot != nil && time.Since(diskCacheTime) < time.Second {
            return cachedDiskSnapshot
        }

        snapshot, err := diskMetricsFunc()
        if err != nil || snapshot == nil {
            // Return zero snapshot on error (metric reports 0, scrape succeeds)
            return &DiskHealthSnapshot{}
        }

        cachedDiskSnapshot = snapshot
        diskCacheTime = time.Now()
        return cachedDiskSnapshot
    }

    getHardwareSnapshot := func() *HardwareHealthSnapshot {
        cacheMu.Lock()
        defer cacheMu.Unlock()

        // Cache for 1 second to avoid 10 SNMP calls per scrape
        if cachedHardwareSnapshot != nil && time.Since(hardwareCacheTime) < time.Second {
            return cachedHardwareSnapshot
        }

        snapshot, err := hardwareMetricsFunc()
        if err != nil || snapshot == nil {
            // Return zero snapshot on error (metric reports 0, scrape succeeds)
            return &HardwareHealthSnapshot{}
        }

        cachedHardwareSnapshot = snapshot
        hardwareCacheTime = time.Now()
        return cachedHardwareSnapshot
    }

    // Disk metrics use slot label
    diskLabels := prometheus.Labels{"slot": slot}

    // Register all 19 metrics (9 disk + 10 hardware)
    m.registry.MustRegister(
        // === Disk Performance Metrics (9 metrics via SSH) ===
        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "read_ops_per_second",
            Help: "Current read IOPS from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().ReadOpsPerSecond }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "write_ops_per_second",
            Help: "Current write IOPS from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().WriteOpsPerSecond }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "read_bytes_per_second",
            Help: "Current read throughput in bytes per second from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().ReadBytesPerSec }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "write_bytes_per_second",
            Help: "Current write throughput in bytes per second from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().WriteBytesPerSec }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "read_latency_milliseconds",
            Help: "Current read latency in milliseconds from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().ReadTimeMs }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "write_latency_milliseconds",
            Help: "Current write latency in milliseconds from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().WriteTimeMs }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "wait_latency_milliseconds",
            Help: "Current wait/queue latency in milliseconds from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().WaitTimeMs }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "in_flight_operations",
            Help: "Current number of in-flight disk operations (queue depth) from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().InFlightOps }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "active_time_milliseconds",
            Help: "Disk active/busy time in milliseconds from /disk monitor-traffic (SSH)",
            ConstLabels: diskLabels,
        }, func() float64 { return getDiskSnapshot().ActiveTimeMs }),

        // === Hardware Health Metrics (10 metrics via SNMP) ===
        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "cpu_temperature_celsius",
            Help: "CPU temperature in Celsius from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().CPUTemperature }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "board_temperature_celsius",
            Help: "Board temperature in Celsius from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().BoardTemperature }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "fan1_speed_rpm",
            Help: "Fan 1 speed in RPM from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().Fan1Speed }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "fan2_speed_rpm",
            Help: "Fan 2 speed in RPM from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().Fan2Speed }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "psu1_power_watts",
            Help: "PSU 1 power draw in watts from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().PSU1Power }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "psu2_power_watts",
            Help: "PSU 2 power draw in watts from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().PSU2Power }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "psu1_temperature_celsius",
            Help: "PSU 1 temperature in Celsius from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().PSU1Temperature }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "psu2_temperature_celsius",
            Help: "PSU 2 temperature in Celsius from SNMP (MIKROTIK-MIB)",
        }, func() float64 { return getHardwareSnapshot().PSU2Temperature }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "disk_pool_size_bytes",
            Help: "RAID6 disk pool total size in bytes from SNMP (HOST-RESOURCES-MIB)",
        }, func() float64 { return getHardwareSnapshot().DiskPoolSizeBytes }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "hardware",
            Name: "disk_pool_used_bytes",
            Help: "RAID6 disk pool used space in bytes from SNMP (HOST-RESOURCES-MIB)",
        }, func() float64 { return getHardwareSnapshot().DiskPoolUsedBytes }),
    )
}
```

IMPORTANT: You will need to add `"sync"` and `"time"` to the imports in prometheus.go (time is already imported, sync is not).

2. **Add unit tests** in `pkg/observability/prometheus_test.go`:

```go
func TestRDSMetrics_Registration(t *testing.T) {
    m := NewMetrics()

    // Before SetRDSMonitoring, rds metrics should not appear
    body := scrapeMetrics(t, m)
    if strings.Contains(body, "rds_disk_") || strings.Contains(body, "rds_hardware_") {
        t.Error("rds metrics should not appear before SetRDSMonitoring")
    }

    // Register with test callbacks
    m.SetRDSMonitoring(
        "storage-pool",
        "10.42.68.1",
        "public",
        func() (*DiskHealthSnapshot, error) {
            return &DiskHealthSnapshot{
                ReadOpsPerSecond:  100,
                WriteOpsPerSecond: 50,
                ReadBytesPerSec:   1_000_000,
                WriteBytesPerSec:  500_000,
                ReadTimeMs:        2,
                WriteTimeMs:       5,
                WaitTimeMs:        1,
                InFlightOps:       3,
                ActiveTimeMs:      10,
            }, nil
        },
        func() (*HardwareHealthSnapshot, error) {
            return &HardwareHealthSnapshot{
                CPUTemperature:    45,
                BoardTemperature:  38,
                Fan1Speed:         7500,
                Fan2Speed:         7600,
                PSU1Power:         700,
                PSU2Power:         680,
                PSU1Temperature:   28,
                PSU2Temperature:   27,
                DiskPoolSizeBytes: 8_000_000_000_000,
                DiskPoolUsedBytes: 1_600_000_000_000,
            }, nil
        },
    )

    // Scrape and verify all 19 metrics present (9 disk + 10 hardware)
    body = scrapeMetrics(t, m)

    expectedDiskMetrics := []string{
        "rds_disk_read_ops_per_second",
        "rds_disk_write_ops_per_second",
        "rds_disk_read_bytes_per_second",
        "rds_disk_write_bytes_per_second",
        "rds_disk_read_latency_milliseconds",
        "rds_disk_write_latency_milliseconds",
        "rds_disk_wait_latency_milliseconds",
        "rds_disk_in_flight_operations",
        "rds_disk_active_time_milliseconds",
    }

    expectedHardwareMetrics := []string{
        "rds_hardware_cpu_temperature_celsius",
        "rds_hardware_board_temperature_celsius",
        "rds_hardware_fan1_speed_rpm",
        "rds_hardware_fan2_speed_rpm",
        "rds_hardware_psu1_power_watts",
        "rds_hardware_psu2_power_watts",
        "rds_hardware_psu1_temperature_celsius",
        "rds_hardware_psu2_temperature_celsius",
        "rds_hardware_disk_pool_size_bytes",
        "rds_hardware_disk_pool_used_bytes",
    }

    // Check disk metrics have slot label
    for _, name := range expectedDiskMetrics {
        expectedLine := fmt.Sprintf(`%s{slot="storage-pool"}`, name)
        if !strings.Contains(body, expectedLine) {
            t.Errorf("expected %s with slot label, not found", name)
        }
    }

    // Check hardware metrics (no labels)
    for _, name := range expectedHardwareMetrics {
        if !strings.Contains(body, name) {
            t.Errorf("expected %s, not found", name)
        }
    }
}

func TestRDSMetrics_ErrorReturnsZero(t *testing.T) {
    m := NewMetrics()

    // Register with error-returning callbacks
    m.SetRDSMonitoring(
        "storage-pool",
        "10.42.68.1",
        "public",
        func() (*DiskHealthSnapshot, error) {
            return nil, fmt.Errorf("SSH connection failed")
        },
        func() (*HardwareHealthSnapshot, error) {
            return nil, fmt.Errorf("SNMP timeout")
        },
    )

    // Scrape should succeed (not fail) - metrics should report 0
    body := scrapeMetrics(t, m)
    if !strings.Contains(body, "rds_disk_read_ops_per_second") {
        t.Error("disk metrics should still appear even with error callback")
    }
    if !strings.Contains(body, "rds_hardware_cpu_temperature_celsius") {
        t.Error("hardware metrics should still appear even with error callback")
    }
    if !strings.Contains(body, `rds_disk_read_ops_per_second{slot="storage-pool"} 0`) {
        t.Error("disk metrics should report 0 on error")
    }
}

func TestRDSMetrics_DynamicUpdates(t *testing.T) {
    m := NewMetrics()

    diskCallCount := 0
    hwCallCount := 0
    m.SetRDSMonitoring(
        "storage-pool",
        "10.42.68.1",
        "public",
        func() (*DiskHealthSnapshot, error) {
            diskCallCount++
            return &DiskHealthSnapshot{
                WriteOpsPerSecond: float64(diskCallCount * 100),
            }, nil
        },
        func() (*HardwareHealthSnapshot, error) {
            hwCallCount++
            return &HardwareHealthSnapshot{
                CPUTemperature: 40 + float64(hwCallCount),
            }, nil
        },
    )

    // First scrape
    body1 := scrapeMetrics(t, m)
    if !strings.Contains(body1, "rds_disk_write_ops_per_second") {
        t.Error("disk write IOPS metric not found in first scrape")
    }
    if !strings.Contains(body1, "rds_hardware_cpu_temperature_celsius") {
        t.Error("CPU temp metric not found in first scrape")
    }

    // Wait for cache to expire (>1 second)
    time.Sleep(1100 * time.Millisecond)

    // Second scrape should show updated values
    body2 := scrapeMetrics(t, m)
    if body1 == body2 {
        t.Log("Note: cache may still be valid if scrapes were fast")
    }
}

func TestRDSMetrics_NotRegisteredWithoutCall(t *testing.T) {
    m := NewMetrics()

    // Verify rds metrics do NOT appear when SetRDSMonitoring is never called
    body := scrapeMetrics(t, m)
    if strings.Contains(body, "rds_disk_") || strings.Contains(body, "rds_hardware_") {
        t.Error("rds metrics should not appear without SetRDSMonitoring call")
    }
}
```

Note: The `scrapeMetrics` helper already exists in the test file from Phase 28.1. Also add `"fmt"` and `"time"` to test imports if not already present.

3. Run `make test` to verify all tests pass.
  </action>
  <verify>
Run `make test` - all tests pass including new TestRDSMetrics_* tests.
Run `go build ./...` - compiles without errors.
Grep for `rds_disk_` in prometheus.go shows 9 disk metric registrations.
Grep for `rds_hardware_` in prometheus.go shows 10 hardware metric registrations.
Grep for `SetRDSMonitoring` shows method in prometheus.go and tests in prometheus_test.go.
  </verify>
  <done>
19 RDS monitoring GaugeFunc metrics registered via SetRDSMonitoring (9 disk via SSH + 10 hardware via SNMP). DiskHealthSnapshot and HardwareHealthSnapshot structs defined in observability package (no import cycle). Separate caches prevent multiple SSH/SNMP calls per scrape (1-second TTL each). Error callbacks return 0 (no scrape failure). Tests verify: registration, labels, error behavior, dynamic updates, absence without call.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire RDS monitoring (SSH + SNMP) into driver.go and create comprehensive monitoring design doc</name>
  <files>pkg/driver/driver.go, docs/MONITORING_DESIGN.md</files>
  <action>
1. **Wire SetRDSMonitoring into driver.go** following the existing SetAttachmentManager pattern.

In `pkg/driver/driver.go`, find the block where `config.Metrics.SetAttachmentManager` is called. After the attachment manager wiring block, add RDS monitoring wiring:

```go
    // Wire RDS monitoring (disk performance + hardware health) into Prometheus metrics.
    // GaugeFunc callbacks poll via SSH (/disk monitor-traffic) and SNMP (MIKROTIK-MIB)
    // during Prometheus scrape. Only registers in controller mode (node plugin has no RDS client).
    if config.EnableController && config.Metrics != nil && driver.rdsClient != nil {
        // Monitor the storage pool disk (configurable via basePath extraction)
        // Default: "storage-pool" - the primary Btrfs RAID6 pool
        storageSlot := "storage-pool"

        // SNMP target configuration (use storage VLAN IP, same as SSH operations)
        // CSI controller runs in environment with access to storage network
        snmpHost := "10.42.68.1"  // TODO: make configurable via helm values
        snmpCommunity := "public" // TODO: make configurable via helm values

        config.Metrics.SetRDSMonitoring(
            storageSlot,
            snmpHost,
            snmpCommunity,
            // Disk performance metrics callback (SSH)
            func() (*observability.DiskHealthSnapshot, error) {
                metrics, err := driver.rdsClient.GetDiskMetrics(storageSlot)
                if err != nil {
                    return nil, err
                }
                return &observability.DiskHealthSnapshot{
                    ReadOpsPerSecond:  metrics.ReadOpsPerSecond,
                    WriteOpsPerSecond: metrics.WriteOpsPerSecond,
                    ReadBytesPerSec:   metrics.ReadBytesPerSec,
                    WriteBytesPerSec:  metrics.WriteBytesPerSec,
                    ReadTimeMs:        metrics.ReadTimeMs,
                    WriteTimeMs:       metrics.WriteTimeMs,
                    WaitTimeMs:        metrics.WaitTimeMs,
                    InFlightOps:       metrics.InFlightOps,
                    ActiveTimeMs:      metrics.ActiveTimeMs,
                }, nil
            },
            // Hardware health metrics callback (SNMP)
            func() (*observability.HardwareHealthSnapshot, error) {
                metrics, err := driver.rdsClient.GetHardwareHealth(snmpHost, snmpCommunity)
                if err != nil {
                    return nil, err
                }
                return &observability.HardwareHealthSnapshot{
                    CPUTemperature:    metrics.CPUTemperature,
                    BoardTemperature:  metrics.BoardTemperature,
                    Fan1Speed:         metrics.Fan1Speed,
                    Fan2Speed:         metrics.Fan2Speed,
                    PSU1Power:         metrics.PSU1Power,
                    PSU2Power:         metrics.PSU2Power,
                    PSU1Temperature:   metrics.PSU1Temperature,
                    PSU2Temperature:   metrics.PSU2Temperature,
                    DiskPoolSizeBytes: metrics.DiskPoolSizeBytes,
                    DiskPoolUsedBytes: metrics.DiskPoolUsedBytes,
                }, nil
            },
        )
        klog.Infof("RDS monitoring enabled (disk slot=%s, snmp=%s)", storageSlot, snmpHost)
    }
```

Make sure to import the observability package if not already imported. Check existing imports in driver.go -- the observability package is likely already imported since SetAttachmentManager is called.

2. **Create `docs/MONITORING_DESIGN.md`** capturing the dual-approach monitoring (SSH + SNMP) as an actionable specification:

```markdown
# RDS Monitoring Design

**Phase:** 28.2 (RDS Health & Performance Monitoring Research)
**Status:** Implemented
**Date:** 2026-02-06

## Overview

The RDS CSI driver exposes comprehensive monitoring metrics from MikroTik RouterOS via Prometheus
using a dual-approach strategy:

- **SSH Polling**: Disk performance metrics (IOPS, throughput, latency, queue depth) from `/disk monitor-traffic`
- **SNMP Polling**: Hardware health metrics (temperatures, fan speeds, PSU status, disk capacity) from MIKROTIK-MIB

Both data sources are polled on-demand during Prometheus scrapes using GaugeFunc collectors.

## Architecture

```
Prometheus --> /metrics endpoint (controller pod)
                    |
        +-----------+------------+
        |                        |
   GaugeFunc (9 disk)     GaugeFunc (10 hardware)
        |                        |
   SSH: /disk                SNMP: gosnmp
   monitor-traffic           (MIKROTIK-MIB,
   storage-pool once          HOST-RESOURCES-MIB)
        |                        |
        +------------------------+
                    |
            MikroTik RDS
          10.42.68.1 (storage VLAN)
```

**Key design decisions:**
- **Dual approach:** SSH for performance metrics (unavailable via SNMP), SNMP for hardware health (lighter weight).
  Combines best-of-both: comprehensive coverage without excessive SSH overhead.
- **GaugeFunc over background polling:** Metrics fetched only during Prometheus scrape (30-60s),
  no background goroutines consuming resources when metrics aren't collected.
- **Separate 1-second caches:** SSH and SNMP results cached independently to avoid multiple calls per scrape
  (9 disk metrics share one SSH call, 10 hardware metrics share one SNMP call).
- **Storage pool aggregate:** Monitors the aggregate storage pool, not individual PVCs. Low cardinality
  (19 total time series). Per-PVC monitoring deferred to future phase if needed.
- **Error tolerance:** SSH/SNMP failures return 0 for all metrics (no scrape failure). Transient network
  issues don't break Prometheus alerting rules.
- **Same network as storage:** Uses 10.42.68.1 (storage VLAN) for both SSH and SNMP. CSI controller
  runs in environment with access to storage network, same IP used for volume operations.

## Metrics Catalog

### Disk Performance Metrics (9 metrics via SSH)

All disk metrics use namespace `rds`, subsystem `disk`, with `slot` label.

| Metric | Type | Unit | Description |
|--------|------|------|-------------|
| `rds_disk_read_ops_per_second` | Gauge | ops/s | Current read IOPS |
| `rds_disk_write_ops_per_second` | Gauge | ops/s | Current write IOPS |
| `rds_disk_read_bytes_per_second` | Gauge | bytes/s | Read throughput |
| `rds_disk_write_bytes_per_second` | Gauge | bytes/s | Write throughput |
| `rds_disk_read_latency_milliseconds` | Gauge | ms | Read latency |
| `rds_disk_write_latency_milliseconds` | Gauge | ms | Write latency |
| `rds_disk_wait_latency_milliseconds` | Gauge | ms | Queue wait time |
| `rds_disk_in_flight_operations` | Gauge | count | Queue depth |
| `rds_disk_active_time_milliseconds` | Gauge | ms | Disk busy time |

**Labels:**
- `slot`: Disk slot name (e.g., `storage-pool`)

**Example scrape output:**
```
rds_disk_read_ops_per_second{slot="storage-pool"} 0
rds_disk_write_ops_per_second{slot="storage-pool"} 76
rds_disk_write_bytes_per_second{slot="storage-pool"} 1600000
rds_disk_in_flight_operations{slot="storage-pool"} 0
```

### Hardware Health Metrics (10 metrics via SNMP)

All hardware metrics use namespace `rds`, subsystem `hardware`, with no additional labels.

| Metric | Type | Unit | Description | OID |
|--------|------|------|-------------|-----|
| `rds_hardware_cpu_temperature_celsius` | Gauge | °C | CPU temperature | 1.3.6.1.4.1.14988.1.1.3.100.1.3.17 |
| `rds_hardware_board_temperature_celsius` | Gauge | °C | Board temperature | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7101 |
| `rds_hardware_fan1_speed_rpm` | Gauge | RPM | Fan 1 speed | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7001 |
| `rds_hardware_fan2_speed_rpm` | Gauge | RPM | Fan 2 speed | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7002 |
| `rds_hardware_psu1_power_watts` | Gauge | W | PSU 1 power draw | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7501 |
| `rds_hardware_psu2_power_watts` | Gauge | W | PSU 2 power draw | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7502 |
| `rds_hardware_psu1_temperature_celsius` | Gauge | °C | PSU 1 temperature | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7601 |
| `rds_hardware_psu2_temperature_celsius` | Gauge | °C | PSU 2 temperature | 1.3.6.1.4.1.14988.1.1.3.100.1.3.7602 |
| `rds_hardware_disk_pool_size_bytes` | Gauge | bytes | RAID6 pool total size | 1.3.6.1.2.1.25.2.3.1.5.262170 |
| `rds_hardware_disk_pool_used_bytes` | Gauge | bytes | RAID6 pool used space | 1.3.6.1.2.1.25.2.3.1.6.262170 |

**Example scrape output:**
```
rds_hardware_cpu_temperature_celsius 45
rds_hardware_board_temperature_celsius 38
rds_hardware_fan1_speed_rpm 7500
rds_hardware_psu1_power_watts 700
rds_hardware_disk_pool_size_bytes 8e+12
rds_hardware_disk_pool_used_bytes 1.6e+12
```

## Prometheus Alert Examples

```yaml
# === Disk Performance Alerts (SSH) ===

# High write latency (>50ms average over 5 minutes)
- alert: RDSHighWriteLatency
  expr: avg_over_time(rds_disk_write_latency_milliseconds{slot="storage-pool"}[5m]) > 50
  for: 10m
  labels:
    severity: warning

# Queue depth saturation (>16 in-flight ops sustained)
- alert: RDSQueueSaturation
  expr: rds_disk_in_flight_operations{slot="storage-pool"} > 16
  for: 5m
  labels:
    severity: critical

# Zero I/O when volumes are attached (potential RDS hang)
- alert: RDSNoIOActivity
  expr: rds_disk_write_ops_per_second{slot="storage-pool"} == 0
    and rds_csi_nvme_connections_active > 0
  for: 15m
  labels:
    severity: warning

# === Hardware Health Alerts (SNMP) ===

# High CPU temperature
- alert: RDSHighCPUTemperature
  expr: rds_hardware_cpu_temperature_celsius > 80
  for: 5m
  labels:
    severity: warning

# Fan failure or low speed
- alert: RDSFanIssue
  expr: rds_hardware_fan1_speed_rpm < 3000 or rds_hardware_fan2_speed_rpm < 3000
  for: 5m
  labels:
    severity: critical

# PSU failure (zero power draw on either PSU)
- alert: RDSPSUFailure
  expr: rds_hardware_psu1_power_watts == 0 or rds_hardware_psu2_power_watts == 0
  for: 1m
  labels:
    severity: critical

# Disk pool near capacity (>90% full)
- alert: RDSDiskPoolNearFull
  expr: (rds_hardware_disk_pool_used_bytes / rds_hardware_disk_pool_size_bytes) > 0.9
  for: 30m
  labels:
    severity: warning
```

## Configuration

Currently hardcoded to monitor `storage-pool` slot via SSH and SNMP at 10.42.68.1 (storage VLAN).
SSH uses mgmt VRF 10.42.241.3 for connection. Future Helm chart values:

```yaml
monitoring:
  enabled: true
  ssh:
    diskSlot: "storage-pool"  # Disk slot to monitor via SSH
  snmp:
    host: "10.42.68.1"        # SNMP target (storage VLAN, same as volume operations)
    community: "public"        # SNMP community string (should be secret in production)
  # Per-PVC monitoring (future, opt-in)
  perPVCEnabled: false
```

## Limitations

1. **"once" modifier verified:** The `/disk monitor-traffic <slot> once` command works correctly
   on RouterOS 7.1+ (exits cleanly after one snapshot). No need for output parsing.
2. **Single disk only:** Currently monitors one storage pool. Multiple disk support requires
   repeated SSH commands and increases cardinality.
3. **SNMP provides hardware only:** SNMP does NOT expose disk performance metrics (IOPS, latency).
   MIKROTIK-MIB provides hardware health, HOST-RESOURCES-MIB provides disk capacity. SSH required
   for performance data.
4. **SSH overhead:** Each scrape triggers one SSH command (~50-100ms). At 30s scrape interval,
   this is negligible but should be monitored if polling frequency increases.
5. **SNMP overhead:** Each scrape triggers one SNMP query (~10-20ms). Negligible at standard intervals.
6. **Network access required:** Both SSH and SNMP use storage VLAN IP (10.42.68.1). CSI controller
   must run in environment with access to storage network (same requirement as volume operations).

## Implementation Details

### SSH Command Format
```bash
# Correct format (no user prefix, use mgmt IP)
ssh 10.42.241.3 '/disk monitor-traffic storage-pool once'

# Incorrect (user prefix causes issues)
ssh admin@10.42.68.1 '/disk monitor-traffic storage-pool once'
```

### SNMP Configuration
```go
// gosnmp v1.37.0 dependency
import "github.com/gosnmp/gosnmp"

// Connection parameters (use storage VLAN IP, same as SSH operations)
snmp := &gosnmp.GoSNMP{
    Target:    "10.42.68.1",
    Port:      161,
    Community: "public",
    Version:   gosnmp.Version2c,
    Timeout:   time.Second * 2,
}
```

## Open Questions for Hardware Validation

1. ✅ Does `/disk monitor-traffic <slot> once` work on RouterOS 7.1+? **CONFIRMED: YES**
2. What is the SSH command overhead on RDS CPU? Measure before/after enabling metrics.
3. What are realistic alert thresholds for RAID6 Btrfs on RDS? Baseline during normal operation.
4. What are normal temperature/fan/PSU ranges for this hardware? Baseline during normal operation.

## Related

- Phase 28.1: Fixed `rds_csi_nvme_connections_active` metric accuracy (GaugeFunc pattern established)
- Phase 28.2-01: SSH client GetDiskMetrics and SNMP client GetHardwareHealth implementation
- Phase 28: Helm chart will expose monitoring configuration as chart values
- Research: `.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-RESEARCH.md`
```

3. Run `make test` and `go build ./...` to verify everything compiles and tests pass.
  </action>
  <verify>
Run `make test` - all tests pass.
Run `go build ./...` - compiles without errors.
Grep for `SetRDSMonitoring` in driver.go confirms wiring present with both SSH and SNMP callbacks.
`docs/MONITORING_DESIGN.md` exists with dual-approach architecture, 19 metrics catalog, and alert examples.
  </verify>
  <done>
SetRDSMonitoring wired in driver.go (controller-only, after RDS client connection) with both SSH and SNMP callbacks. DiskHealthSnapshot and HardwareHealthSnapshot bridge observability and rds packages without import cycle. 19 GaugeFunc metrics registered (9 rds_disk with slot label + 10 rds_hardware). Separate 1-second caches prevent multiple SSH/SNMP calls per scrape. docs/MONITORING_DESIGN.md documents: dual-approach architecture (SSH for performance, SNMP for health), complete metrics catalog, Prometheus alert examples for both data sources, configuration guidance, limitations, and hardware validation questions. All existing and new tests pass.
  </done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles without errors
2. `make test` passes all tests (existing + new RDS metric tests for both disk and hardware)
3. `grep -r "SetRDSMonitoring" pkg/` shows prometheus.go, prometheus_test.go, driver.go
4. `grep -r "rds_disk_" pkg/observability/prometheus.go` shows 9 disk metric definitions
5. `grep -r "rds_hardware_" pkg/observability/prometheus.go` shows 10 hardware metric definitions
6. `grep -r "DiskHealthSnapshot\|HardwareHealthSnapshot" pkg/observability/` shows both structs and usage
7. `docs/MONITORING_DESIGN.md` exists with dual-approach architecture, 19 metrics catalog, and alert examples
8. No import of `pkg/rds` in `pkg/observability/` (verified by checking imports)
</verification>

<success_criteria>
1. 19 GaugeFunc metrics registered in controller mode (9 rds_disk + 10 rds_hardware)
2. Metrics polled on-demand via GaugeFunc (no background goroutines)
3. SSH/SNMP errors gracefully return 0 (no scrape failure)
4. DiskHealthSnapshot and HardwareHealthSnapshot bridge observability and rds packages without import cycle
5. Driver.go wires monitoring after RDS client connection (controller-only) with both SSH and SNMP callbacks
6. Separate 1-second caches for disk (SSH) and hardware (SNMP) snapshots prevent multiple calls per scrape
7. Monitoring design document captures dual-approach (SSH + SNMP) as actionable specification
8. All tests pass including new metric registration, error handling, and dynamic update tests for both data sources
</success_criteria>

<output>
After completion, create `.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-02-SUMMARY.md`
</output>
