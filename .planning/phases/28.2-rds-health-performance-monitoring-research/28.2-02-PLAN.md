---
phase: 28.2-rds-health-performance-monitoring-research
plan: 02
type: execute
wave: 2
depends_on: ["28.2-01"]
files_modified:
  - pkg/observability/prometheus.go
  - pkg/observability/prometheus_test.go
  - pkg/driver/driver.go
autonomous: true

must_haves:
  truths:
    - "RDS disk health metrics appear in /metrics endpoint when controller has RDS client configured"
    - "Metrics use rds_disk namespace with slot label (e.g., rds_disk_read_ops_per_second{slot=storage-pool})"
    - "GaugeFunc polls RDS via SSH only when Prometheus scrapes (no background goroutines)"
    - "Metrics do NOT appear in node plugin /metrics (only controller has RDS SSH client)"
    - "SSH errors return 0 for metric value (no scrape failure on transient errors)"
  artifacts:
    - path: "pkg/observability/prometheus.go"
      provides: "SetRDSHealthMonitoring method registering GaugeFunc collectors"
      contains: "SetRDSHealthMonitoring"
    - path: "pkg/observability/prometheus_test.go"
      provides: "Unit tests for RDS health metric registration and GaugeFunc behavior"
      contains: "TestRDSHealthMetrics"
    - path: "pkg/driver/driver.go"
      provides: "Wiring of RDS health monitoring into metrics during controller init"
      contains: "SetRDSHealthMonitoring"
  key_links:
    - from: "pkg/driver/driver.go"
      to: "pkg/observability/prometheus.go"
      via: "SetRDSHealthMonitoring call after RDS client connection"
      pattern: "SetRDSHealthMonitoring"
    - from: "pkg/observability/prometheus.go"
      to: "pkg/rds/client.go"
      via: "GaugeFunc callback calls GetDiskMetrics on RDSClient"
      pattern: "GetDiskMetrics"
---

<objective>
Register RDS disk health metrics as Prometheus GaugeFunc collectors and wire into the driver.

Purpose: Expose storage pool performance data (IOPS, throughput, latency, queue depth)
through the controller's /metrics endpoint, enabling Prometheus-based monitoring and alerting
for RDS storage health. Follows the established GaugeFunc pattern from Phase 28.1 (SetAttachmentManager).

Output: SetRDSHealthMonitoring method on Metrics, GaugeFunc-based metric registration,
driver.go wiring, and unit tests verifying metric behavior.
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-RESEARCH.md
@.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-01-SUMMARY.md

@pkg/observability/prometheus.go
@pkg/observability/prometheus_test.go
@pkg/driver/driver.go
@pkg/rds/client.go
@pkg/rds/types.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SetRDSHealthMonitoring to Prometheus metrics with GaugeFunc collectors</name>
  <files>pkg/observability/prometheus.go, pkg/observability/prometheus_test.go</files>
  <action>
1. **Add `SetRDSHealthMonitoring` method** to `pkg/observability/prometheus.go` after the existing `SetAttachmentManager` method. This follows the exact same late-registration GaugeFunc pattern.

The method accepts a callback function `func() (*rds.DiskMetrics, error)` rather than the RDSClient directly (avoids import cycle, matches Phase 28.1 pattern of using `func() int` callback).

IMPORTANT: Do NOT import `pkg/rds` in the observability package. Use a callback function type that returns the metrics values. Define a local struct or use individual callbacks. The cleanest approach: accept a single callback `func() (readIOPS, writeIOPS, readBytesPerSec, writeBytesPerSec, readTimeMs, writeTimeMs, waitTimeMs, inFlightOps, activeTimeMs float64, err error)` -- BUT that's unwieldy.

Better approach: Define a simple `DiskHealthSnapshot` struct in the observability package (no import needed):

```go
// DiskHealthSnapshot holds a point-in-time disk performance snapshot.
// Used as return type for the RDS health monitoring callback to avoid
// importing pkg/rds in the observability package (prevents import cycles).
type DiskHealthSnapshot struct {
    ReadOpsPerSecond  float64
    WriteOpsPerSecond float64
    ReadBytesPerSec   float64
    WriteBytesPerSec  float64
    ReadTimeMs        float64
    WriteTimeMs       float64
    WaitTimeMs        float64
    InFlightOps       float64
    ActiveTimeMs      float64
}
```

Add a field to the Metrics struct:
```go
rdsDiskMetricsFunc func() (*DiskHealthSnapshot, error) // Callback for RDS disk health metrics (GaugeFunc)
```

Add the method:
```go
// SetRDSHealthMonitoring registers GaugeFunc metrics for RDS disk health monitoring.
// The metricsFunc callback is invoked during Prometheus scrape to fetch current disk
// performance data via SSH. This must be called after the RDS client is connected.
// If not called (e.g., node plugin), RDS health metrics are not registered.
//
// Metrics registered (all gauges, polled on scrape):
//   - rds_disk_read_ops_per_second{slot=<slot>}
//   - rds_disk_write_ops_per_second{slot=<slot>}
//   - rds_disk_read_bytes_per_second{slot=<slot>}
//   - rds_disk_write_bytes_per_second{slot=<slot>}
//   - rds_disk_read_latency_milliseconds{slot=<slot>}
//   - rds_disk_write_latency_milliseconds{slot=<slot>}
//   - rds_disk_wait_latency_milliseconds{slot=<slot>}
//   - rds_disk_in_flight_operations{slot=<slot>}
//   - rds_disk_active_time_milliseconds{slot=<slot>}
func (m *Metrics) SetRDSHealthMonitoring(slot string, metricsFunc func() (*DiskHealthSnapshot, error)) {
    m.rdsDiskMetricsFunc = metricsFunc

    // Helper: fetch cached snapshot and return specific field.
    // All GaugeFunc callbacks share the same metricsFunc, which means each
    // Prometheus scrape could invoke SSH multiple times (once per metric).
    // To avoid this, cache the result for the duration of a single scrape.
    // Use a simple time-based cache: if last fetch was <1s ago, reuse result.
    var (
        cachedSnapshot *DiskHealthSnapshot
        cacheTime      time.Time
        cacheMu        sync.Mutex
    )

    getSnapshot := func() *DiskHealthSnapshot {
        cacheMu.Lock()
        defer cacheMu.Unlock()

        // Cache for 1 second to avoid multiple SSH calls per scrape
        if cachedSnapshot != nil && time.Since(cacheTime) < time.Second {
            return cachedSnapshot
        }

        snapshot, err := metricsFunc()
        if err != nil || snapshot == nil {
            // Return zero snapshot on error (metric reports 0, scrape succeeds)
            return &DiskHealthSnapshot{}
        }

        cachedSnapshot = snapshot
        cacheTime = time.Now()
        return cachedSnapshot
    }

    constLabels := prometheus.Labels{"slot": slot}

    m.registry.MustRegister(
        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "read_ops_per_second",
            Help: "Current read IOPS from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().ReadOpsPerSecond }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "write_ops_per_second",
            Help: "Current write IOPS from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().WriteOpsPerSecond }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "read_bytes_per_second",
            Help: "Current read throughput in bytes per second from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().ReadBytesPerSec }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "write_bytes_per_second",
            Help: "Current write throughput in bytes per second from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().WriteBytesPerSec }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "read_latency_milliseconds",
            Help: "Current read latency in milliseconds from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().ReadTimeMs }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "write_latency_milliseconds",
            Help: "Current write latency in milliseconds from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().WriteTimeMs }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "wait_latency_milliseconds",
            Help: "Current wait/queue latency in milliseconds from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().WaitTimeMs }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "in_flight_operations",
            Help: "Current number of in-flight disk operations (queue depth) from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().InFlightOps }),

        prometheus.NewGaugeFunc(prometheus.GaugeOpts{
            Namespace: "rds", Subsystem: "disk",
            Name: "active_time_milliseconds",
            Help: "Disk active/busy time in milliseconds from /disk monitor-traffic",
            ConstLabels: constLabels,
        }, func() float64 { return getSnapshot().ActiveTimeMs }),
    )
}
```

IMPORTANT: You will need to add `"sync"` and `"time"` to the imports in prometheus.go (time is already imported, sync is not).

2. **Add unit tests** in `pkg/observability/prometheus_test.go`:

```go
func TestRDSHealthMetrics_Registration(t *testing.T) {
    m := NewMetrics()

    // Before SetRDSHealthMonitoring, rds_disk metrics should not appear
    body := scrapeMetrics(t, m)
    if strings.Contains(body, "rds_disk_") {
        t.Error("rds_disk metrics should not appear before SetRDSHealthMonitoring")
    }

    // Register with test callback
    m.SetRDSHealthMonitoring("storage-pool", func() (*DiskHealthSnapshot, error) {
        return &DiskHealthSnapshot{
            ReadOpsPerSecond:  100,
            WriteOpsPerSecond: 50,
            ReadBytesPerSec:   1_000_000,
            WriteBytesPerSec:  500_000,
            ReadTimeMs:        2,
            WriteTimeMs:       5,
            WaitTimeMs:        1,
            InFlightOps:       3,
            ActiveTimeMs:      10,
        }, nil
    })

    // Scrape and verify all 9 metrics present with correct values
    body = scrapeMetrics(t, m)

    expectedMetrics := []struct {
        name  string
        value string
    }{
        {"rds_disk_read_ops_per_second", "100"},
        {"rds_disk_write_ops_per_second", "50"},
        {"rds_disk_read_bytes_per_second", "1e+06"},
        {"rds_disk_write_bytes_per_second", "500000"},
        {"rds_disk_read_latency_milliseconds", "2"},
        {"rds_disk_write_latency_milliseconds", "5"},
        {"rds_disk_wait_latency_milliseconds", "1"},
        {"rds_disk_in_flight_operations", "3"},
        {"rds_disk_active_time_milliseconds", "10"},
    }

    for _, em := range expectedMetrics {
        if !strings.Contains(body, em.name) {
            t.Errorf("metric %s not found in scrape output", em.name)
        }
        // Check slot label
        expectedLine := fmt.Sprintf(`%s{slot="storage-pool"}`, em.name)
        if !strings.Contains(body, expectedLine) {
            t.Errorf("expected %s with slot label, got:\n%s", em.name, body)
        }
    }
}

func TestRDSHealthMetrics_ErrorReturnsZero(t *testing.T) {
    m := NewMetrics()

    // Register with error-returning callback
    m.SetRDSHealthMonitoring("storage-pool", func() (*DiskHealthSnapshot, error) {
        return nil, fmt.Errorf("SSH connection failed")
    })

    // Scrape should succeed (not fail) - metrics should report 0
    body := scrapeMetrics(t, m)
    if !strings.Contains(body, "rds_disk_read_ops_per_second") {
        t.Error("metrics should still appear even with error callback")
    }
    if !strings.Contains(body, `rds_disk_read_ops_per_second{slot="storage-pool"} 0`) {
        t.Error("metrics should report 0 on error")
    }
}

func TestRDSHealthMetrics_DynamicUpdates(t *testing.T) {
    m := NewMetrics()

    callCount := 0
    m.SetRDSHealthMonitoring("storage-pool", func() (*DiskHealthSnapshot, error) {
        callCount++
        return &DiskHealthSnapshot{
            WriteOpsPerSecond: float64(callCount * 100),
        }, nil
    })

    // First scrape
    body1 := scrapeMetrics(t, m)
    if !strings.Contains(body1, "rds_disk_write_ops_per_second") {
        t.Error("write IOPS metric not found in first scrape")
    }

    // Wait for cache to expire (>1 second)
    time.Sleep(1100 * time.Millisecond)

    // Second scrape should show updated values
    body2 := scrapeMetrics(t, m)
    if body1 == body2 {
        t.Log("Note: cache may still be valid if scrapes were fast")
    }
}

func TestRDSHealthMetrics_NotRegisteredWithoutCall(t *testing.T) {
    m := NewMetrics()

    // Verify rds_disk metrics do NOT appear when SetRDSHealthMonitoring is never called
    body := scrapeMetrics(t, m)
    if strings.Contains(body, "rds_disk_") {
        t.Error("rds_disk metrics should not appear without SetRDSHealthMonitoring call")
    }
}
```

Note: The `scrapeMetrics` helper already exists in the test file from Phase 28.1. Also add `"fmt"` and `"time"` to test imports if not already present.

3. Run `make test` to verify all tests pass.
  </action>
  <verify>
Run `make test` - all tests pass including new TestRDSHealthMetrics_* tests.
Run `go build ./...` - compiles without errors.
Grep for `rds_disk_` in prometheus.go shows 9 metric registrations.
Grep for `SetRDSHealthMonitoring` shows method in prometheus.go and tests in prometheus_test.go.
  </verify>
  <done>
9 RDS disk health GaugeFunc metrics registered via SetRDSHealthMonitoring. DiskHealthSnapshot struct defined in observability package (no import cycle). Cache prevents multiple SSH calls per scrape (1-second TTL). Error callback returns 0 (no scrape failure). Tests verify: registration, slot label, error behavior, dynamic updates, absence without call.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire RDS health monitoring into driver.go and create monitoring design doc</name>
  <files>pkg/driver/driver.go, docs/MONITORING_DESIGN.md</files>
  <action>
1. **Wire SetRDSHealthMonitoring into driver.go** following the existing SetAttachmentManager pattern.

In `pkg/driver/driver.go`, find the block where `config.Metrics.SetAttachmentManager` is called (around line 218-220). AFTER the attachment manager wiring block (after the closing `}` of the `if config.EnableController && config.K8sClient != nil` block around line 223), add RDS health monitoring wiring in the existing RDS client initialization section.

The best location is after the RDS client connection succeeds (around line 202, after `klog.Infof("Connected to RDS at %s:%d"...)`), BUT inside a check for both metrics and rds client. Look for where `driver.rdsClient = rdsClient` is set. Add the wiring right after the controller-specific initialization block.

Actually, the cleanest location: After the attachment manager wiring block, add a new block:

```go
    // Wire RDS disk health monitoring into Prometheus metrics.
    // GaugeFunc callbacks poll /disk monitor-traffic via SSH during scrape.
    // Only registers in controller mode (node plugin has no RDS SSH client).
    if config.EnableController && config.Metrics != nil && driver.rdsClient != nil {
        // Monitor the storage pool disk (configurable via basePath extraction)
        // Default: "storage-pool" - the primary Btrfs RAID6 pool
        storageSlot := "storage-pool"
        config.Metrics.SetRDSHealthMonitoring(storageSlot, func() (*observability.DiskHealthSnapshot, error) {
            metrics, err := driver.rdsClient.GetDiskMetrics(storageSlot)
            if err != nil {
                return nil, err
            }
            return &observability.DiskHealthSnapshot{
                ReadOpsPerSecond:  metrics.ReadOpsPerSecond,
                WriteOpsPerSecond: metrics.WriteOpsPerSecond,
                ReadBytesPerSec:   metrics.ReadBytesPerSec,
                WriteBytesPerSec:  metrics.WriteBytesPerSec,
                ReadTimeMs:        metrics.ReadTimeMs,
                WriteTimeMs:       metrics.WriteTimeMs,
                WaitTimeMs:        metrics.WaitTimeMs,
                InFlightOps:       metrics.InFlightOps,
                ActiveTimeMs:      metrics.ActiveTimeMs,
            }, nil
        })
        klog.Infof("RDS disk health monitoring enabled (slot=%s)", storageSlot)
    }
```

Make sure to import the observability package if not already imported. Check existing imports in driver.go -- the observability package is likely already imported since SetAttachmentManager is called.

2. **Create `docs/MONITORING_DESIGN.md`** capturing the research conclusions as an actionable specification:

```markdown
# RDS Storage Health Monitoring Design

**Phase:** 28.2 (RDS Health & Performance Monitoring Research)
**Status:** Implemented
**Date:** 2026-02-06

## Overview

The RDS CSI driver exposes storage pool health metrics from MikroTik RouterOS via Prometheus.
The controller queries `/disk monitor-traffic <slot> once` via SSH and exposes performance
data as Prometheus GaugeFunc metrics, polled on-demand during each scrape.

## Architecture

```
Prometheus --> /metrics endpoint (controller pod)
                    |
              GaugeFunc (9 metrics)
                    |
              SSH: /disk monitor-traffic storage-pool once
                    |
              MikroTik RDS (10.42.68.1)
```

**Key design decisions:**
- **SSH polling over SNMP/API:** Reuses existing SSH infrastructure, no additional agents needed.
  RouterOS REST API explicitly does not support continuous monitoring commands.
- **GaugeFunc over background polling:** Metrics fetched only during Prometheus scrape (30-60s),
  no background goroutines consuming resources when metrics aren't collected.
- **1-second snapshot cache:** Multiple GaugeFunc callbacks share a cached SSH result to avoid
  9 separate SSH commands per scrape.
- **Storage pool only:** Monitors the aggregate storage pool, not individual PVCs. Low cardinality
  (1 disk = 9 time series). Per-PVC monitoring deferred to future phase if needed.
- **Error tolerance:** SSH failures return 0 for all metrics (no scrape failure). Transient network
  issues don't break Prometheus alerting rules.

## Metrics Catalog

All metrics use namespace `rds`, subsystem `disk`, with `slot` label.

| Metric | Type | Unit | Description |
|--------|------|------|-------------|
| `rds_disk_read_ops_per_second` | Gauge | ops/s | Current read IOPS |
| `rds_disk_write_ops_per_second` | Gauge | ops/s | Current write IOPS |
| `rds_disk_read_bytes_per_second` | Gauge | bytes/s | Read throughput |
| `rds_disk_write_bytes_per_second` | Gauge | bytes/s | Write throughput |
| `rds_disk_read_latency_milliseconds` | Gauge | ms | Read latency |
| `rds_disk_write_latency_milliseconds` | Gauge | ms | Write latency |
| `rds_disk_wait_latency_milliseconds` | Gauge | ms | Queue wait time |
| `rds_disk_in_flight_operations` | Gauge | count | Queue depth |
| `rds_disk_active_time_milliseconds` | Gauge | ms | Disk busy time |

**Labels:**
- `slot`: Disk slot name (e.g., `storage-pool`)

**Example scrape output:**
```
rds_disk_read_ops_per_second{slot="storage-pool"} 0
rds_disk_write_ops_per_second{slot="storage-pool"} 76
rds_disk_write_bytes_per_second{slot="storage-pool"} 1600000
rds_disk_in_flight_operations{slot="storage-pool"} 0
```

## Prometheus Alert Examples

```yaml
# High write latency (>50ms average over 5 minutes)
- alert: RDSHighWriteLatency
  expr: avg_over_time(rds_disk_write_latency_milliseconds{slot="storage-pool"}[5m]) > 50
  for: 10m
  labels:
    severity: warning

# Queue depth saturation (>16 in-flight ops sustained)
- alert: RDSQueueSaturation
  expr: rds_disk_in_flight_operations{slot="storage-pool"} > 16
  for: 5m
  labels:
    severity: critical

# Zero I/O when volumes are attached (potential RDS hang)
- alert: RDSNoIOActivity
  expr: rds_disk_write_ops_per_second{slot="storage-pool"} == 0
    and rds_csi_nvme_connections_active > 0
  for: 15m
  labels:
    severity: warning
```

## Configuration

Currently hardcoded to monitor `storage-pool` slot. Future Helm chart values:

```yaml
monitoring:
  enabled: true
  diskSlot: "storage-pool"  # Disk slot to monitor
  # Per-PVC monitoring (future, opt-in)
  perPVCEnabled: false
```

## Limitations

1. **"once" modifier unverified:** The `/disk monitor-traffic <slot> once` command needs
   hardware validation. If unsupported, fall back to parsing first snapshot from continuous output.
2. **Single disk only:** Currently monitors one storage pool. Multiple disk support requires
   repeated SSH commands and increases cardinality.
3. **No SNMP verification:** Whether SNMP exposes disk performance OIDs is unconfirmed.
   SSH polling is the only verified approach.
4. **SSH overhead:** Each scrape triggers one SSH command (~50-100ms). At 30s scrape interval,
   this is negligible but should be monitored if polling frequency increases.

## Open Questions for Hardware Validation

1. Does `/disk monitor-traffic <slot> once` work on RouterOS 7.1+? Test: `ssh admin@rds '/disk monitor-traffic 61 once'`
2. What is the SSH command overhead on RDS CPU? Measure before/after enabling metrics.
3. What are realistic alert thresholds for RAID6 Btrfs on RDS? Baseline during normal operation.

## Related

- Phase 28.1: Fixed `rds_csi_nvme_connections_active` metric accuracy (GaugeFunc pattern established)
- Phase 28: Helm chart will expose monitoring configuration as chart values
- Research: `.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-RESEARCH.md`
```

3. Run `make test` and `go build ./...` to verify everything compiles and tests pass.
  </action>
  <verify>
Run `make test` - all tests pass.
Run `go build ./...` - compiles without errors.
Grep for `SetRDSHealthMonitoring` in driver.go confirms wiring present.
`docs/MONITORING_DESIGN.md` exists with metrics catalog, architecture, and alert examples.
  </verify>
  <done>
SetRDSHealthMonitoring wired in driver.go (controller-only, after RDS client connection). DiskHealthSnapshot bridges observability and rds packages without import cycle. 9 GaugeFunc metrics registered with rds_disk namespace and slot label. 1-second cache prevents multiple SSH calls per scrape. docs/MONITORING_DESIGN.md documents: architecture, metrics catalog, Prometheus alert examples, configuration guidance, limitations, and open hardware validation questions. All existing and new tests pass.
  </done>
</task>

</tasks>

<verification>
1. `go build ./...` compiles without errors
2. `make test` passes all tests (existing + new RDS health metric tests)
3. `grep -r "SetRDSHealthMonitoring" pkg/` shows prometheus.go, prometheus_test.go, driver.go
4. `grep -r "rds_disk_" pkg/observability/prometheus.go` shows 9 metric definitions
5. `grep -r "DiskHealthSnapshot" pkg/observability/` shows struct and usage
6. `docs/MONITORING_DESIGN.md` exists with metrics catalog and alert examples
7. No import of `pkg/rds` in `pkg/observability/` (verified by checking imports)
</verification>

<success_criteria>
1. 9 GaugeFunc metrics registered in controller mode with rds_disk namespace
2. Metrics polled on-demand via GaugeFunc (no background goroutines)
3. SSH errors gracefully return 0 (no scrape failure)
4. DiskHealthSnapshot bridges observability and rds packages without import cycle
5. Driver.go wires monitoring after RDS client connection (controller-only)
6. Monitoring design document captures research conclusions as actionable specification
7. All tests pass including new metric registration and error handling tests
</success_criteria>

<output>
After completion, create `.planning/phases/28.2-rds-health-performance-monitoring-research/28.2-02-SUMMARY.md`
</output>
