---
phase: 28.2
plan: 02
subsystem: monitoring
tags: [prometheus, metrics, ssh, snmp, observability]
requires: [28.2-01]
provides:
  - RDS disk performance metrics (rds_disk_*)
  - RDS hardware health metrics (rds_hardware_*)
  - Prometheus GaugeFunc pattern for SSH and SNMP polling
  - MONITORING_DESIGN.md documentation
affects: [28]
decisions:
  - use-gaugefunc-pattern: "GaugeFunc polls RDS on-demand during Prometheus scrape (no background goroutines)"
  - separate-caches: "Disk (SSH) and hardware (SNMP) use independent 1-second caches"
  - error-returns-zero: "SSH/SNMP failures return zero snapshots (no scrape failure)"
  - snapshot-structs: "DiskHealthSnapshot and HardwareHealthSnapshot bridge observability and rds packages without import cycle"
  - storage-pool-only: "Monitor storage-pool slot only (low cardinality, 19 total time series)"
  - controller-only: "RDS metrics only appear in controller /metrics (node plugin has no RDS client)"
tech-stack:
  added: []
  patterns: [gaugefunc-callback, dual-approach-monitoring, ssh-snmp-hybrid]
key-files:
  created:
    - docs/MONITORING_DESIGN.md
  modified:
    - pkg/observability/prometheus.go
    - pkg/observability/prometheus_test.go
    - pkg/driver/driver.go
duration: 6 minutes
completed: 2026-02-06
---

# Phase 28.2 Plan 02: RDS Monitoring Metrics Exporter Summary

**One-liner:** Prometheus GaugeFunc metrics for RDS disk performance (SSH) and hardware health (SNMP) with dual-approach monitoring

## What Was Built

Implemented 19 Prometheus GaugeFunc metrics exposing RDS monitoring data via dual approach:

1. **SetRDSMonitoring method** in pkg/observability/prometheus.go:
   - Accepts two callbacks: diskMetricsFunc (SSH) and hardwareMetricsFunc (SNMP)
   - Registers 9 rds_disk metrics (IOPS, throughput, latency, queue depth) with slot label
   - Registers 10 rds_hardware metrics (temperatures, fans, PSU, disk capacity) without labels
   - Separate 1-second caches prevent multiple SSH/SNMP calls per scrape
   - Error callbacks return zero snapshots (metric reports 0, scrape succeeds)

2. **DiskHealthSnapshot and HardwareHealthSnapshot** structs in observability package:
   - Bridge types avoid importing pkg/rds in observability (prevents import cycle)
   - Map directly to rds.DiskMetrics and rds.HardwareHealthMetrics field-by-field

3. **Driver.go wiring** (controller-only):
   - SetRDSMonitoring called after RDS client connection
   - Monitors storage-pool slot via SSH (/disk monitor-traffic storage-pool once)
   - Polls 10.42.68.1 via SNMP (MIKROTIK-MIB + HOST-RESOURCES-MIB)
   - DiskHealthSnapshot callback transforms rds.DiskMetrics ‚Üí observability snapshot
   - HardwareHealthSnapshot callback transforms rds.HardwareHealthMetrics ‚Üí observability snapshot

4. **MONITORING_DESIGN.md** documentation:
   - Dual-approach architecture diagram (SSH for performance, SNMP for health)
   - Complete metrics catalog with 9 disk + 10 hardware metrics
   - Prometheus alert examples (high latency, queue saturation, temperature, fan failure, PSU failure, disk capacity)
   - Implementation details (SSH command format, SNMP OID mappings)
   - Configuration guidance for future Helm chart values
   - Limitations and open questions for hardware validation

5. **Unit tests** (4 new tests):
   - TestRDSMetrics_Registration: Verifies all 19 metrics appear after SetRDSMonitoring call with slot label
   - TestRDSMetrics_ErrorReturnsZero: Validates error callbacks return 0 (no scrape failure)
   - TestRDSMetrics_DynamicUpdates: Confirms metrics update after cache expiration
   - TestRDSMetrics_NotRegisteredWithoutCall: Ensures metrics absent without SetRDSMonitoring

## Implementation Approach

**Pattern:** Follow Phase 28.1 GaugeFunc pattern established for nvme_connections_active

**Dual-approach rationale:**
- **SSH for disk performance:** RouterOS /disk monitor-traffic exposes IOPS, throughput, latency, queue depth unavailable via SNMP
- **SNMP for hardware health:** MIKROTIK-MIB provides temperatures, fan speeds, PSU power/temps with lower overhead than SSH

**Import cycle avoidance:**
- Define DiskHealthSnapshot and HardwareHealthSnapshot in pkg/observability (no rds import)
- Driver.go callbacks transform rds.DiskMetrics ‚Üí observability.DiskHealthSnapshot (field-by-field copy)
- Avoids observability importing pkg/rds while allowing RDS client method calls in driver.go

**Caching strategy:**
- Separate caches for disk (SSH) and hardware (SNMP) with independent 1-second TTLs
- During Prometheus scrape, 9 disk metrics share one SSH call, 10 hardware metrics share one SNMP call
- Prevents excessive SSH commands (9 per scrape ‚Üí 1 per scrape) and SNMP queries (10 ‚Üí 1)

**Error handling:**
- SSH connection failure ‚Üí return &DiskHealthSnapshot{} (all zeros)
- SNMP timeout ‚Üí return &HardwareHealthSnapshot{} (all zeros)
- Metrics still appear in scrape output with value 0 (no scrape failure)
- Transient network issues don't break Prometheus alerting rules

## Deviations from Plan

None - plan executed exactly as written.

## Decisions Made

| Decision | Rationale | Impact | Alternatives Considered |
|----------|-----------|--------|-------------------------|
| Use GaugeFunc pattern (not background polling) | Metrics polled only during Prometheus scrape (30-60s), no background goroutines consuming resources | Follows established nvme_connections_active pattern, efficient resource usage | Background goroutine polling every 10s (rejected: wastes CPU/network when metrics not scraped) |
| Separate 1-second caches for disk and hardware | Prevent multiple SSH/SNMP calls per scrape (9 disk metrics ‚Üí 1 SSH call, 10 hardware ‚Üí 1 SNMP call) | Reduces SSH overhead from 9 calls to 1 per scrape, SNMP from 10 to 1 | No caching (rejected: 19 network calls per scrape), single cache (rejected: disk error doesn't affect hardware metrics) |
| Error callbacks return zero snapshots | SSH/SNMP failures don't cause scrape to fail, metrics report 0 | Transient network issues don't break Prometheus alerting, graceful degradation | Return error and skip metric registration (rejected: intermittent scrape failures confuse Prometheus) |
| DiskHealthSnapshot/HardwareHealthSnapshot in observability | Avoid import cycle (observability can't import pkg/rds) | Clean package boundaries, no circular dependencies | Import pkg/rds in observability (rejected: creates import cycle), use interface{} (rejected: loses type safety) |
| Monitor storage-pool slot only | Low cardinality (19 total time series), aggregate metrics for entire RAID6 pool | Simple initial implementation, sufficient for production monitoring | Per-PVC metrics (deferred: high cardinality, requires label extraction from volume IDs) |
| Controller-only registration | Node plugin has no RDS client, metrics only make sense in controller | Clear separation of concerns, metrics appear in controller /metrics only | Register in both controller and node (rejected: node has no RDS client to poll) |

## Tests Added

### Unit Tests (pkg/observability/prometheus_test.go)

1. **TestRDSMetrics_Registration** - Verifies all 19 metrics appear with correct labels:
   - Before SetRDSMonitoring: rds_disk_* and rds_hardware_* absent
   - After SetRDSMonitoring: all 9 disk metrics have slot="storage-pool" label
   - All 10 hardware metrics present without labels
   - Test callbacks return fixed snapshot values
   - Confirms metric names match expected patterns

2. **TestRDSMetrics_ErrorReturnsZero** - Validates error handling:
   - Disk callback returns error ‚Üí metrics still appear with value 0
   - Hardware callback returns error ‚Üí metrics still appear with value 0
   - Scrape succeeds (no HTTP error)
   - Confirms graceful degradation on SSH/SNMP failure

3. **TestRDSMetrics_DynamicUpdates** - Confirms cache expiration:
   - First scrape invokes callbacks, caches results
   - Wait 1.1 seconds for cache expiration
   - Second scrape invokes callbacks again
   - Validates callbacks are invoked per-scrape after cache TTL

4. **TestRDSMetrics_NotRegisteredWithoutCall** - Ensures opt-in behavior:
   - Without SetRDSMonitoring call, rds_disk_* and rds_hardware_* absent
   - Prevents metrics appearing in node plugin /metrics

All tests pass. Coverage: 4 test functions cover registration, error handling, caching, and opt-in behavior.

## Next Phase Readiness

**Phase 28 (Helm Chart)** is ready to proceed:
- ‚úÖ RDS monitoring metrics available in controller /metrics endpoint
- ‚úÖ Dual-approach monitoring (SSH + SNMP) documented in MONITORING_DESIGN.md
- ‚úÖ 19 Prometheus metrics ready for Grafana dashboard integration
- ‚úÖ Alert examples documented for critical thresholds
- ‚ö†Ô∏è  SNMP host (10.42.68.1) and community (public) hardcoded - Helm chart should make configurable
- ‚ö†Ô∏è  Storage slot (storage-pool) hardcoded - Helm chart should make configurable
- üìù Open questions for hardware validation (thresholds, baselines) should be answered during production testing

**What's next:**
1. Helm chart values for monitoring.snmp.host, monitoring.snmp.community, monitoring.ssh.diskSlot
2. Grafana dashboard with RDS monitoring panels (IOPS, latency, temperatures, fan speeds, disk capacity)
3. Baseline normal operating ranges for alert thresholds during production testing

## Blockers/Concerns

None. Plan completed successfully with all tests passing.

## Technical Details

### Metrics Registered (19 total)

**Disk Performance (9 metrics via SSH, slot label):**
- rds_disk_read_ops_per_second{slot="storage-pool"}
- rds_disk_write_ops_per_second{slot="storage-pool"}
- rds_disk_read_bytes_per_second{slot="storage-pool"}
- rds_disk_write_bytes_per_second{slot="storage-pool"}
- rds_disk_read_latency_milliseconds{slot="storage-pool"}
- rds_disk_write_latency_milliseconds{slot="storage-pool"}
- rds_disk_wait_latency_milliseconds{slot="storage-pool"}
- rds_disk_in_flight_operations{slot="storage-pool"}
- rds_disk_active_time_milliseconds{slot="storage-pool"}

**Hardware Health (10 metrics via SNMP, no labels):**
- rds_hardware_cpu_temperature_celsius
- rds_hardware_board_temperature_celsius
- rds_hardware_fan1_speed_rpm
- rds_hardware_fan2_speed_rpm
- rds_hardware_psu1_power_watts
- rds_hardware_psu2_power_watts
- rds_hardware_psu1_temperature_celsius
- rds_hardware_psu2_temperature_celsius
- rds_hardware_disk_pool_size_bytes
- rds_hardware_disk_pool_used_bytes

### Key Code Patterns

**GaugeFunc registration pattern:**
```go
prometheus.NewGaugeFunc(prometheus.GaugeOpts{
    Namespace: "rds", Subsystem: "disk",
    Name: "read_ops_per_second",
    Help: "Current read IOPS from /disk monitor-traffic (SSH)",
    ConstLabels: diskLabels,
}, func() float64 { return getDiskSnapshot().ReadOpsPerSecond })
```

**Cache implementation (1-second TTL):**
```go
getDiskSnapshot := func() *DiskHealthSnapshot {
    cacheMu.Lock()
    defer cacheMu.Unlock()

    if cachedDiskSnapshot != nil && time.Since(diskCacheTime) < time.Second {
        return cachedDiskSnapshot
    }

    snapshot, err := diskMetricsFunc()
    if err != nil || snapshot == nil {
        return &DiskHealthSnapshot{} // Zero snapshot on error
    }

    cachedDiskSnapshot = snapshot
    diskCacheTime = time.Now()
    return cachedDiskSnapshot
}
```

**Driver.go wiring (field-by-field transform):**
```go
func() (*observability.DiskHealthSnapshot, error) {
    metrics, err := driver.rdsClient.GetDiskMetrics(storageSlot)
    if err != nil {
        return nil, err
    }
    return &observability.DiskHealthSnapshot{
        ReadOpsPerSecond:  metrics.ReadOpsPerSecond,
        WriteOpsPerSecond: metrics.WriteOpsPerSecond,
        // ... 7 more fields
    }, nil
}
```

## Files Changed

**Created:**
- docs/MONITORING_DESIGN.md (9.5 KB) - Dual-approach monitoring architecture and metrics catalog

**Modified:**
- pkg/observability/prometheus.go (+238 lines) - SetRDSMonitoring method, snapshot structs, 19 GaugeFunc registrations
- pkg/observability/prometheus_test.go (+132 lines) - 4 RDS metric tests
- pkg/driver/driver.go (+54 lines) - RDS monitoring wiring with SSH and SNMP callbacks

## Related Work

- **Phase 28.2-01**: Implemented GetDiskMetrics (SSH) and GetHardwareHealth (SNMP) in RDS client
- **Phase 28.1**: Established GaugeFunc pattern for nvme_connections_active metric
- **Phase 28**: Helm chart will expose monitoring configuration as chart values (snmpHost, snmpCommunity, diskSlot)

## Lessons Learned

1. **GaugeFunc caching is critical:** Without 1-second cache, Prometheus scrape triggers 19 network calls (9 SSH + 10 SNMP). With cache, only 2 network calls (1 SSH + 1 SNMP).

2. **Separate caches for independent data sources:** Disk (SSH) and hardware (SNMP) failures are independent. Separate caches ensure SSH failure doesn't prevent SNMP metrics, and vice versa.

3. **Import cycle avoidance via bridge structs:** DiskHealthSnapshot and HardwareHealthSnapshot in observability package enable type-safe callbacks without importing pkg/rds (which imports observability for metrics).

4. **Error tolerance prevents alerting brittleness:** Returning zero snapshots on error (instead of failing scrape) ensures transient SSH/SNMP timeouts don't trigger false-positive "scrape failed" alerts.

5. **Dual-approach monitoring combines protocol strengths:** SSH provides disk performance metrics unavailable via SNMP, SNMP provides hardware health with lower overhead than SSH. Using both maximizes visibility while minimizing overhead.
