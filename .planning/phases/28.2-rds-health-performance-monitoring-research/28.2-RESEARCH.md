# Phase 28.2: RDS Health & Performance Monitoring Research - Research

**Researched:** 2026-02-06
**Domain:** MikroTik RouterOS monitoring (SNMP, SSH, API), Prometheus metrics for storage performance
**Confidence:** MEDIUM

## Summary

This research investigates monitoring approaches for exposing RDS storage health metrics from MikroTik RouterOS to Prometheus. The user discovered the `/disk monitor-traffic <slot>` command provides comprehensive storage metrics (IOPS, throughput, latency, queue depth) previously thought unavailable.

Three monitoring approaches exist: SSH polling (existing pattern in codebase), SNMP (agent-based with MIBs), and RouterOS API (REST or socket-based). SSH polling is the recommended approach for the RDS CSI driver because it aligns with the existing SSH-based architecture, requires no additional infrastructure, and provides direct access to `/disk monitor-traffic` output. SNMP is designed for low-overhead polling but requires agent configuration and may not expose disk-level metrics. RouterOS API is unsuitable because continuous monitoring commands (like monitor-traffic) are explicitly not supported—only "monitor once" is available, and API connections have 60-second timeouts.

The metrics should follow Prometheus conventions with namespace `rds_disk_*`, include labels for disk slot and operation type (read/write), and use GaugeFunc collectors for on-demand polling during scrape intervals to minimize overhead. Per-disk cardinality is low (typically 1-5 disks in homelab RDS), making label-based identification safe.

**Primary recommendation:** Implement SSH polling of `/disk monitor-traffic` with GaugeFunc collectors, expose metrics with `rds_disk_*` namespace, poll frequency matched to Prometheus scrape interval (30-60s), and focus on key metrics: IOPS, throughput, latency, queue depth, and active time.

## Standard Stack

The established libraries/tools for this domain:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| prometheus/client_golang | v1.19+ | Prometheus metrics client | Official Go client, GaugeFunc support, used in existing codebase |
| golang.org/x/crypto/ssh | stdlib | SSH client library | Already used for RDS control plane (CreateVolume, DeleteVolume) |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| go-routeros/routeros | v2.x | RouterOS API client | If future phases need API-based monitoring (not recommended for Phase 28.2) |
| prometheus/snmp_exporter | v0.26+ | SNMP metric collection | If SNMP approach is chosen instead of SSH (lower priority) |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| SSH polling | SNMP agent | SNMP is lower overhead but requires RDS agent config, may lack disk metrics, adds complexity |
| SSH polling | RouterOS REST API | REST API doesn't support continuous monitoring (only "monitor once"), 60s timeouts, not suitable |
| SSH polling | RouterOS socket API | Socket API requires persistent TCP connection (8728/8729), adds connection management complexity |

**Installation:**
```bash
# Already in go.mod
go get github.com/prometheus/client_golang/prometheus@v1.19.1
# golang.org/x/crypto/ssh is already imported
```

## Architecture Patterns

### Recommended Project Structure
```
pkg/
├── observability/
│   ├── prometheus.go       # Core metrics (existing)
│   ├── rds_metrics.go      # NEW: RDS disk health metrics
│   └── rds_metrics_test.go # Unit tests
├── rds/
│   ├── commands.go         # Add ParseMonitorTrafficOutput()
│   └── commands_test.go    # Test parsing logic
```

### Pattern 1: GaugeFunc-Based Polling (On-Demand)
**What:** Use Prometheus GaugeFunc to poll RDS metrics only when Prometheus scrapes the /metrics endpoint
**When to use:** For external system metrics that are expensive to poll continuously
**Example:**
```go
// Source: Existing pattern in pkg/observability/prometheus.go (SetAttachmentManager)
// GaugeFunc polls on-demand during Prometheus scrape, not continuously

diskIOPSRead := prometheus.NewGaugeFunc(
    prometheus.GaugeOpts{
        Namespace: "rds_disk",
        Name:      "read_ops_per_second",
        Help:      "Current read IOPS from /disk monitor-traffic",
    },
    func() float64 {
        // SSH to RDS, parse output, return value
        metrics, err := rdsClient.GetDiskMetrics(diskSlot)
        if err != nil {
            return 0
        }
        return metrics.ReadOpsPerSecond
    },
)
```

**Why this pattern:**
- Polling happens at Prometheus scrape interval (typically 30-60s), not more frequently
- No background goroutines consuming resources when metrics aren't being collected
- Scrape timeout enforces upper bound on SSH command duration
- Matches existing `nvme_connections_active` metric pattern from Phase 28.1

### Pattern 2: SSH Command Execution with Parsing
**What:** Execute `/disk monitor-traffic <slot>` via SSH, parse key=value output
**When to use:** For metrics derived from RouterOS CLI commands
**Example:**
```go
// Source: Existing pattern in pkg/rds/commands.go (GetVolumeInfo, GetCapacity)

func (c *Client) GetDiskMetrics(slot string) (*DiskMetrics, error) {
    // Use monitor-traffic with "once" to get snapshot, not continuous stream
    // The continuous output requires terminal control sequences
    cmd := fmt.Sprintf(`/disk monitor-traffic %s once`, slot)

    output, err := c.runCommand(cmd)
    if err != nil {
        return nil, fmt.Errorf("failed to get disk metrics: %w", err)
    }

    return parseDiskMetrics(output), nil
}

func parseDiskMetrics(output string) *DiskMetrics {
    // Parse key: value format from monitor-traffic output
    // read-ops-per-second:               0
    // write-ops-per-second:             76
    metrics := &DiskMetrics{}

    reReadOps := regexp.MustCompile(`read-ops-per-second:\s+(\d+)`)
    if matches := reReadOps.FindStringSubmatch(output); len(matches) > 1 {
        metrics.ReadOpsPerSecond, _ = strconv.ParseFloat(matches[1], 64)
    }
    // ... parse other fields ...

    return metrics
}
```

**Why this pattern:**
- Consistent with existing RDS command patterns (CreateVolume, GetVolumeInfo)
- SSH connection pooling already implemented
- Error handling and retry logic already in place
- Can use "once" modifier to get snapshot rather than continuous stream

### Pattern 3: Per-Disk Label Strategy (Low Cardinality)
**What:** Use disk slot as label, not metric name suffix
**When to use:** When monitoring multiple disks with same metric types
**Example:**
```go
// BAD: High metric count, rigid structure
rds_disk_storage_pool_read_iops
rds_disk_storage_pool_write_iops
rds_disk_pvc_abc123_read_iops
rds_disk_pvc_abc123_write_iops

// GOOD: Single metric with labels, flexible querying
rds_disk_ops_per_second{slot="storage-pool", operation="read"}
rds_disk_ops_per_second{slot="storage-pool", operation="write"}
rds_disk_ops_per_second{slot="pvc-abc123", operation="read"}
rds_disk_ops_per_second{slot="pvc-abc123", operation="write"}
```

**Why this pattern:**
- Prometheus best practice: labels enable filtering and aggregation
- Cardinality is low: typically 1-5 disks in homelab RDS (storage pool + PVCs)
- operation="read"|"write" label avoids metric name explosion
- Aligns with existing metric patterns (e.g., `status` label in volumeOpsTotal)

### Anti-Patterns to Avoid
- **Continuous SSH Streaming:** `/disk monitor-traffic` without "once" produces continuous output requiring terminal emulation. Use "once" modifier for snapshot polling.
- **High-Frequency Polling:** Polling faster than Prometheus scrape interval (30-60s) wastes resources. GaugeFunc ensures polling matches scrape cadence.
- **Unbounded Labels:** Using volume ID as label for every PVC would cause cardinality explosion. Monitor storage pool and representative PVCs only.
- **REST API for Monitoring:** RouterOS REST API explicitly does not support continuous monitoring ("There is no option to run a 'continuous' command"). Don't attempt to use REST for real-time metrics.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Prometheus metric registration | Custom metric exporter | prometheus/client_golang | Official library, handles HTTP endpoint, metric serialization, type safety |
| SSH connection pooling | Manual goroutine management | Existing pkg/rds/ssh_client.go | Already implements connection reuse, retries, error handling |
| Metric parsing | Custom string parsing | stdlib regexp + existing patterns | Consistent with GetVolumeInfo(), testable, handles malformed output |
| SNMP polling | Custom SNMP client | prometheus/snmp_exporter | Complex protocol, MIB parsing, OID mapping—use official tool if SNMP chosen |
| RouterOS API client | Custom API wrapper | go-routeros/routeros | Handles API protocol quirks, maintained by community |

**Key insight:** The RDS CSI driver already has SSH client infrastructure, Prometheus metrics infrastructure, and parsing patterns. Reuse these components rather than introducing new dependencies (SNMP, API clients) that add operational complexity.

## Common Pitfalls

### Pitfall 1: RouterOS API "monitor" Command Limitations
**What goes wrong:** Attempting to use RouterOS REST API or socket API for continuous monitoring fails because continuous monitoring is explicitly unsupported.
**Why it happens:** Documentation states "There is no option to run a 'continuous' command, like 'monitor' via REST API. Use 'monitor once' parameter instead."
**How to avoid:** Use SSH with `/disk monitor-traffic <slot> once` for snapshot polling. If future needs require real-time streaming, use SSH with PTY (pseudo-terminal) allocation.
**Warning signs:** REST API timeouts after 60 seconds, API returning errors for monitor commands without "once" parameter.

**Source:** [MikroTik REST API Documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/47579162/REST+API) (verified MEDIUM confidence)

### Pitfall 2: SNMP MIB Disk Metric Availability
**What goes wrong:** Assuming SNMP will expose `/disk monitor-traffic` metrics. SNMP may only expose basic disk capacity (HOST-RESOURCES-MIB) but not performance counters.
**Why it happens:** SNMP MIBs are predefined schema. RouterOS SNMP implementation may not include per-disk IOPS/latency OIDs.
**How to avoid:** Before committing to SNMP approach, verify disk performance OIDs exist in MIKROTIK-MIB. Use `snmpwalk` against RDS to enumerate available disk OIDs.
**Warning signs:** SNMP queries only return disk size/free space, not performance metrics. MIB documentation doesn't list disk IOPS OIDs.

**Source:** [MikroTik SNMP Documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/8978519/SNMP) notes HOST-RESOURCES-MIB support but doesn't document disk performance OIDs (LOW confidence on SNMP disk metrics availability)

### Pitfall 3: Cardinality Explosion with Per-PVC Metrics
**What goes wrong:** Creating metrics with labels for every PVC (e.g., `rds_disk_ops_per_second{slot="pvc-abc123"}`) causes cardinality explosion as PVC count grows to 100+.
**Why it happens:** Prometheus best practice limits unique label combinations. 100 PVCs × 2 operations (read/write) × 6 metric types = 1200 time series from one exporter.
**How to avoid:** Monitor storage pool health only, or limit per-PVC metrics to actively experiencing issues (on-demand via kubectl annotation). Consider aggregate metrics (pool-level IOPS) rather than per-volume.
**Warning signs:** Prometheus scrape timeouts, increasing memory usage, slow query performance, /metrics endpoint taking >5 seconds to respond.

**Source:** [Prometheus Labels Best Practices (CNCF)](https://www.cncf.io/blog/2025/07/22/prometheus-labels-understanding-and-best-practices/) warns against high cardinality from unbounded label values

### Pitfall 4: SSH Polling Overhead and Connection Limits
**What goes wrong:** Polling `/disk monitor-traffic` every 10 seconds from multiple metrics collectors causes excessive SSH connection overhead or hits RouterOS connection limits.
**Why it happens:** Each SSH command requires authentication handshake (~200-500ms overhead). RouterOS has default max-sessions limit.
**How to avoid:**
- Use GaugeFunc to poll only during Prometheus scrape (30-60s interval)
- Batch multiple disk metrics in single SSH command if monitoring multiple disks
- Reuse SSH connection pool (already implemented in pkg/rds/ssh_client.go)
- Single metrics collector per RDS (run in controller pod only, not per-node)
**Warning signs:** High SSH authentication latency, RouterOS logs showing "max-sessions limit reached", metrics collection taking >5 seconds.

**Source:** General SSH polling overhead [documented in comparison guides](https://locall.host/snmp-vs-ssh-monitoring/) (MEDIUM confidence)

### Pitfall 5: Parsing Continuous vs. Snapshot Output
**What goes wrong:** `/disk monitor-traffic <slot>` without "once" produces continuous real-time output with terminal control sequences, which breaks line-based parsing.
**Why it happens:** Interactive mode uses ANSI escape codes to update same line. SSH exec channel doesn't allocate PTY by default.
**How to avoid:** Always use "once" modifier: `/disk monitor-traffic <slot> once` to get single snapshot suitable for parsing.
**Warning signs:** Parse failures with "unexpected format", escape sequences in output like `\x1b[2J`, hanging SSH commands waiting for terminal input.

**Source:** User-provided discovery in README.md shows continuous output format. RouterOS [Interface monitoring documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/139526175/Interface+stats+and+monitor-traffic) describes similar behavior for interface monitor-traffic (MEDIUM confidence)

## Code Examples

Verified patterns from official sources:

### GaugeFunc Registration (On-Demand Polling)
```go
// Source: pkg/observability/prometheus.go (SetAttachmentManager)
// Pattern: Register GaugeFunc after dependency (RDS client) is initialized

func (m *Metrics) SetRDSHealthMonitoring(rdsClient *rds.Client, diskSlot string) {
    // Read IOPS gauge
    readIOPS := prometheus.NewGaugeFunc(
        prometheus.GaugeOpts{
            Namespace: "rds_disk",
            Name:      "read_ops_per_second",
            Help:      "Current read IOPS from /disk monitor-traffic",
            ConstLabels: prometheus.Labels{"slot": diskSlot},
        },
        func() float64 {
            metrics, err := rdsClient.GetDiskMetrics(diskSlot)
            if err != nil {
                // Log error but return 0 to avoid scrape failure
                return 0
            }
            return metrics.ReadOpsPerSecond
        },
    )

    // Write IOPS gauge
    writeIOPS := prometheus.NewGaugeFunc(
        prometheus.GaugeOpts{
            Namespace: "rds_disk",
            Name:      "write_ops_per_second",
            Help:      "Current write IOPS from /disk monitor-traffic",
            ConstLabels: prometheus.Labels{"slot": diskSlot},
        },
        func() float64 {
            metrics, err := rdsClient.GetDiskMetrics(diskSlot)
            if err != nil {
                return 0
            }
            return metrics.WriteOpsPerSecond
        },
    )

    m.registry.MustRegister(readIOPS, writeIOPS)
}
```

### SSH Command with Output Parsing
```go
// Source: pkg/rds/commands.go (GetVolumeInfo pattern)
// Pattern: Execute command, parse structured output, return typed struct

type DiskMetrics struct {
    Slot              string
    ReadOpsPerSecond  float64
    WriteOpsPerSecond float64
    ReadRate          float64 // bps
    WriteRate         float64 // bps
    ReadTime          float64 // ms
    WriteTime         float64 // ms
    WaitTime          float64 // ms
    InFlightOps       float64
    ActiveTime        float64 // ms
}

func (c *Client) GetDiskMetrics(slot string) (*DiskMetrics, error) {
    // Use "once" to get snapshot, not continuous stream
    cmd := fmt.Sprintf(`/disk monitor-traffic %s once`, slot)

    output, err := c.runCommand(cmd)
    if err != nil {
        return nil, fmt.Errorf("failed to get disk metrics: %w", err)
    }

    return parseDiskMetrics(output)
}

func parseDiskMetrics(output string) (*DiskMetrics, error) {
    metrics := &DiskMetrics{}

    // Parse "key: value" format from monitor-traffic output
    // Example: "   read-ops-per-second:               0"

    patterns := map[string]*float64{
        `read-ops-per-second:\s+(\d+)`:  &metrics.ReadOpsPerSecond,
        `write-ops-per-second:\s+(\d+)`: &metrics.WriteOpsPerSecond,
        `in-flight-ops:\s+(\d+)`:        &metrics.InFlightOps,
    }

    for pattern, field := range patterns {
        re := regexp.MustCompile(pattern)
        if matches := re.FindStringSubmatch(output); len(matches) > 1 {
            value, err := strconv.ParseFloat(matches[1], 64)
            if err == nil {
                *field = value
            }
        }
    }

    // Parse rate fields with units (e.g., "12.8Mbps")
    reReadRate := regexp.MustCompile(`read-rate:\s+([\d.]+)(\w+)`)
    if matches := reReadRate.FindStringSubmatch(output); len(matches) > 2 {
        value, _ := strconv.ParseFloat(matches[1], 64)
        unit := matches[2]
        metrics.ReadRate = convertRateToBytes(value, unit)
    }

    // Validate required fields present
    if metrics.ReadOpsPerSecond == 0 && metrics.WriteOpsPerSecond == 0 {
        return nil, fmt.Errorf("failed to parse disk metrics from output")
    }

    return metrics, nil
}

func convertRateToBytes(value float64, unit string) float64 {
    // Convert "Mbps", "Gbps", "kbps" to bytes per second
    switch unit {
    case "bps":
        return value / 8
    case "kbps", "Kbps":
        return (value * 1000) / 8
    case "Mbps":
        return (value * 1000000) / 8
    case "Gbps":
        return (value * 1000000000) / 8
    default:
        return value
    }
}
```

### Metric Naming Convention
```go
// Source: Prometheus naming best practices + existing codebase patterns
// Pattern: <namespace>_<subsystem>_<metric>_<unit>

const (
    metricNamespace = "rds"
    metricSubsystem = "disk"
)

// Metric examples following conventions:
// rds_disk_read_ops_per_second{slot="storage-pool"}
// rds_disk_write_ops_per_second{slot="storage-pool"}
// rds_disk_read_bytes_per_second{slot="storage-pool"}
// rds_disk_write_bytes_per_second{slot="storage-pool"}
// rds_disk_read_latency_milliseconds{slot="storage-pool"}
// rds_disk_write_latency_milliseconds{slot="storage-pool"}
// rds_disk_wait_latency_milliseconds{slot="storage-pool"}
// rds_disk_in_flight_operations{slot="storage-pool"}
// rds_disk_active_time_milliseconds{slot="storage-pool"}

// Label conventions:
// - slot: disk slot name (e.g., "storage-pool", "pvc-abc123")
// - Operation-specific metrics use operation="read"|"write" label if combined
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| No RDS health monitoring | Monitor storage pool via SSH | Phase 28.2 discovery | Enables proactive disk failure detection, capacity planning, performance troubleshooting |
| SNMP assumed best for monitoring | SSH polling with GaugeFunc | 2026 (current research) | Simpler integration, reuses existing infrastructure, no agent config |
| Continuous metric collection | On-demand polling at scrape | Phase 28.1 (GaugeFunc pattern) | Lower overhead, no background goroutines, scrape timeout enforces bounds |
| Per-metric REST API calls | Batch SSH commands | Existing RDS client pattern | Fewer network round-trips, connection reuse, better performance |

**Deprecated/outdated:**
- **RouterOS REST API for monitoring:** Explicitly doesn't support continuous monitoring commands. Use SSH for monitor-traffic.
- **Manual counter arithmetic for gauges:** Phase 28.1 replaced counter-derived gauges with GaugeFunc. Use same pattern for RDS metrics.
- **High-frequency polling loops:** Don't poll faster than Prometheus scrape interval. GaugeFunc ensures sync with scrape cadence.

## Open Questions

Things that couldn't be fully resolved:

1. **Does `/disk monitor-traffic` support "once" modifier?**
   - What we know: RouterOS interface monitor-traffic supports "once". User example shows continuous output without "once".
   - What's unclear: Whether disk monitor-traffic command specifically supports "once" parameter. May need hardware validation.
   - Recommendation: Test on real RDS: `ssh admin@rds '/disk monitor-traffic 61 once'`. If unsupported, fall back to parsing first snapshot from continuous output or use `/disk print detail` for static metrics only.

2. **What are RouterOS SSH connection limits for production?**
   - What we know: General guidance suggests avoiding excessive polling. Default max-sessions exists but specific limit not documented.
   - What's unclear: Exact max-sessions default, whether it's configurable, impact of connection pooling.
   - Recommendation: Test with actual RDS under load. Start with conservative 60s polling interval. Monitor SSH authentication latency.

3. **Are disk performance metrics available via SNMP?**
   - What we know: RouterOS supports HOST-RESOURCES-MIB (basic capacity). MIKROTIK-MIB exists but specific disk OIDs not documented in search results.
   - What's unclear: Whether SNMP exposes IOPS, latency, throughput at disk level or only interface level.
   - Recommendation: Run `snmpwalk -v2c -c public <rds-ip> 1.3.6.1.4.1.14988` to enumerate MIKROTIK-MIB. Check for disk-related OIDs. If absent, SSH polling is only option.

4. **Should metrics monitor storage pool only or individual PVCs?**
   - What we know: Storage pool metrics (aggregate) are low cardinality (1 disk). Per-PVC metrics scale linearly with PVC count.
   - What's unclear: Production use case—do users need per-PVC granularity or is pool-level sufficient for alerting?
   - Recommendation: Start with storage pool monitoring only (Phase 28.2). Add per-PVC as opt-in feature in future phase if operators request it. Use annotation-based targeting: `rds-csi.srvlab.io/monitor-disk: "true"`.

5. **What is performance overhead of SSH polling on RDS?**
   - What we know: SSH command latency is 50-100ms baseline. Authentication adds 200-500ms if not pooled.
   - What's unclear: CPU/memory impact on RDS when executing monitor-traffic command every 60s. Does it block disk I/O?
   - Recommendation: Measure RDS CPU usage before/after enabling metrics. Run load test with concurrent disk I/O. If overhead >5% CPU, increase polling interval to 120s or disable by default (opt-in via helm value).

6. **Rate vs. absolute throughput units?**
   - What we know: `/disk monitor-traffic` reports rate in "bps" (bits per second) with unit suffixes (Mbps, Gbps).
   - What's unclear: Whether Prometheus convention prefers bytes_per_second (rate) or bytes_total (counter) for throughput.
   - Recommendation: Use `_bytes_per_second` (gauge) for consistency with `_ops_per_second`. Avoid counter if monitor-traffic doesn't provide cumulative totals. Prometheus rate() function works with gauges.

## Sources

### Primary (HIGH confidence)
- Prometheus client_golang documentation: https://pkg.go.dev/github.com/prometheus/client_golang/prometheus
  - GaugeFunc API and usage patterns verified
- Existing codebase patterns: pkg/observability/prometheus.go, pkg/rds/commands.go
  - SetAttachmentManager GaugeFunc pattern (Phase 28.1)
  - SSH command execution and parsing patterns

### Secondary (MEDIUM confidence)
- [MikroTik REST API Documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/47579162/REST+API)
  - Verified: "There is no option to run a 'continuous' command, like 'monitor' via REST API"
  - Verified: 60-second timeout for REST API commands
- [MikroTik RouterOS API Documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/47579160/API)
  - Socket API listen command for real-time updates
  - Default ports 8728 (TCP) and 8729 (TLS)
- [Prometheus Metric Naming Best Practices (Official)](https://prometheus.io/docs/practices/naming/)
  - Metric naming conventions: namespace_subsystem_metric_unit
  - Use underscores, single-word prefix, descriptive names
- [Prometheus Labels Best Practices (CNCF Blog)](https://www.cncf.io/blog/2025/07/22/prometheus-labels-understanding-and-best-practices/)
  - Cardinality warnings, avoid unbounded label values
  - Label keys should be descriptive
- [SSH vs SNMP Monitoring Comparison](https://locall.host/snmp-vs-ssh-monitoring/)
  - SNMP designed for low-overhead polling, SSH higher overhead
  - Performance comparison of monitoring approaches
- [SNMP vs API Monitoring (InfraOn)](https://infraon.io/blog/snmp-vs-api-network-monitoring-tools-comparison/)
  - REST API performance superior to SSH for monitoring

### Tertiary (LOW confidence - needs hardware validation)
- [MikroTik SNMP Documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/8978519/SNMP)
  - HOST-RESOURCES-MIB support confirmed
  - Disk-specific performance OIDs not documented (may not exist)
- [MikroTik Interface monitor-traffic Documentation](https://help.mikrotik.com/docs/spaces/ROS/pages/139526175/Interface+stats+and+monitor-traffic)
  - Interface monitoring patterns, but disk monitoring may differ
- Go RouterOS API libraries (github.com/go-routeros/routeros)
  - Community-maintained, not evaluated for stability
- User-provided /disk monitor-traffic output (README.md)
  - Shows continuous format, unclear if "once" modifier supported

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - prometheus/client_golang and golang.org/x/crypto/ssh already in use, proven patterns
- Architecture: MEDIUM - GaugeFunc pattern proven in Phase 28.1, but RDS-specific parsing needs hardware validation
- Pitfalls: MEDIUM - RouterOS API limitations verified in official docs, SNMP disk metrics unconfirmed, cardinality best practices from Prometheus docs

**Research date:** 2026-02-06
**Valid until:** 60 days (2026-04-06) - RouterOS API stable, Prometheus conventions unlikely to change, but disk command specifics may need RDS hardware validation sooner
