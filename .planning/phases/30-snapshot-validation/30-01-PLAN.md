---
phase: 30-snapshot-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - test/mock/rds_server.go
  - test/mock/rds_server_test.go
autonomous: true

must_haves:
  truths:
    - "Mock RDS server handles /disk add with copy-from parameter to create snapshot disk entries"
    - "Mock RDS server does NOT handle /disk/btrfs/subvolume/* commands (removed)"
    - "Snapshot disks created via copy-from have no NVMe export flags in mock state"
    - "Mock snapshot disk entries are independent copies (deleting source does not delete snapshot)"
    - "/disk print detail returns snapshot entries with slot, file-path, file-size, type=file, no nvme-tcp-export"
    - "/disk remove works on snapshot slots for delete"
    - "/file remove works on snapshot backing files for belt-and-suspenders cleanup"
  artifacts:
    - path: "test/mock/rds_server.go"
      provides: "Updated mock RDS server with copy-from snapshot semantics"
      contains: "copy-from"
    - path: "test/mock/rds_server_test.go"
      provides: "Tests for copy-from snapshot handlers in mock server"
      contains: "TestMockRDS_SnapshotCopyFrom"
  key_links:
    - from: "test/mock/rds_server.go"
      to: "pkg/rds/commands.go"
      via: "SSH command format matching"
      pattern: "/disk add.*copy-from"
    - from: "test/mock/rds_server.go"
      to: "test/sanity/sanity_test.go"
      via: "CSI sanity test uses mock server for snapshot operations"
      pattern: "mock.NewMockRDSServer"
---

<objective>
Update mock RDS server to replace broken Btrfs subvolume snapshot handlers with copy-from disk semantics matching Phase 29's rewritten SSH commands.

Purpose: The mock RDS server (test/mock/rds_server.go) still handles /disk/btrfs/subvolume/* commands that no longer exist in the driver. The real snapshot flow now uses `/disk add copy-from=[find slot=<name>]` which creates an independent disk entry (no NVMe export). The mock must match this so CSI sanity tests and e2e tests produce realistic behavior.

Output: Updated mock RDS server with copy-from snapshot support; old Btrfs handlers removed; snapshot-specific unit tests added.
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-snapshot-implementation-fix/29-01-SUMMARY.md
@test/mock/rds_server.go
@pkg/rds/commands.go
@pkg/rds/types.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace Btrfs subvolume handlers with copy-from snapshot semantics in mock RDS server</name>
  <files>test/mock/rds_server.go</files>
  <action>
Update the mock RDS server to handle the new snapshot SSH command format from Phase 29.

**Remove entirely:**
- `handleBtrfsSubvolumeAdd()` function
- `handleBtrfsSubvolumeRemove()` function
- `handleBtrfsSubvolumePrintDetail()` function
- `handleBtrfsSubvolumePrint()` function
- All four routing entries in `executeCommand()` for `/disk/btrfs/subvolume/*`

**Update MockSnapshot struct** to match the new copy-from model (remove Btrfs-specific fields):
```go
type MockSnapshot struct {
    Slot          string    // Snapshot slot name (snap-<uuid>-at-<hash>)
    SourceVolume  string    // Source volume slot (pvc-<uuid>)
    FilePath      string    // Backing file path (e.g., /storage-pool/metal-csi/snap-xxx.img)
    FileSizeBytes int64     // Size copied from source
    CreatedAt     time.Time // Creation timestamp
    // Removed: Parent, FSLabel, ReadOnly, Name (use Slot instead)
}
```

**Update `handleDiskAdd()`** to detect `copy-from` parameter:
- If command contains `copy-from=`, route to snapshot creation logic
- Parse `copy-from=[find slot=<name>]` to extract source volume slot name using regex `copy-from=\[find slot=([^\]]+)\]`
- Look up source volume in `s.volumes` by slot name
- If source not found, return `"failure: no such item\n", 1`
- Create snapshot entry in `s.snapshots` map (key = slot value)
- Snapshot gets: Slot from `slot=` param, FilePath from `file-path=` param, FileSizeBytes copied from source volume, SourceVolume = source slot, CreatedAt = time.Now()
- Snapshot disk is NOT added to `s.volumes` (snapshots are NOT NVMe-exported volumes)
- Also create backing file entry in `s.files` with the snapshot file path and size
- If `copy-from` source is a snapshot (found in `s.snapshots` instead of `s.volumes`), support that too (for RestoreSnapshot which copies from snapshot to create a new NVMe-exported volume — but note RestoreSnapshot creates a volume with NVMe flags, so the `copy-from` with NVMe flags present means it goes through the normal volume creation path with data from snapshot)
- For RestoreSnapshot path: if `copy-from` is present AND `nvme-tcp-export=yes` is also present, this is a restore operation — create the entry in `s.volumes` (not `s.snapshots`), copy size from source snapshot

**Update `handleDiskRemove()`** to also check `s.snapshots` map:
- Current code only checks `s.volumes`. After removing from `s.volumes`, also check and remove from `s.snapshots`
- Also remove the backing file from `s.files` when removing a snapshot

**Update `handleDiskPrintDetail()`** to also search snapshots:
- When querying by slot and volume not found, check `s.snapshots` map
- Format snapshot output as: `slot="<slot>" type="file" file-path="<path>" file-size=<bytes> status="ready"` (NO nvme-tcp-export fields, matching what real RouterOS returns for copy-from entries)
- When listing all (no slot filter with slot~"snap-" prefix), include snapshot entries from `s.snapshots` map in the output

**Update `GetSnapshot()` and `ListSnapshots()` accessor methods** to use `Slot` instead of `Name`.

**Do NOT change:** Volume creation, deletion, capacity queries, file operations, error injection, timing simulation, or any non-snapshot functionality.
  </action>
  <verify>
Run `go build ./test/mock/...` to verify compilation.
Run `go vet ./test/mock/...` to check for issues.
Grep for `/disk/btrfs/subvolume` in test/mock/rds_server.go — must return 0 matches.
Grep for `copy-from` in test/mock/rds_server.go — must return at least 1 match.
  </verify>
  <done>
Mock RDS server compiles, handles `/disk add copy-from=` for snapshot creation, handles `/disk remove` for snapshot deletion, handles `/disk print detail` for snapshot queries. No Btrfs subvolume handler code remains.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add snapshot copy-from unit tests for mock RDS server</name>
  <files>test/mock/rds_server_test.go</files>
  <action>
Add focused unit tests for the new copy-from snapshot handlers in the mock RDS server. Tests should start the mock server, connect via SSH, execute commands, and verify state.

**Add test function `TestMockRDS_SnapshotCopyFrom(t *testing.T)` with subtests:**

1. **"create snapshot via copy-from"**: Create a volume first (via `/disk add`), then create a snapshot (via `/disk add type=file copy-from=[find slot=pvc-test-1] file-path=/storage-pool/metal-csi/snap-test-1.img slot=snap-test-1`). Verify: snapshot exists in server state via `GetSnapshot()`, has correct source volume, file path, and size copied from source. Verify snapshot is NOT in `GetVolume()` (not an NVMe volume).

2. **"snapshot independent of source"**: Create volume, create snapshot from it, delete source volume. Verify snapshot still exists (independent copy semantics).

3. **"query snapshot via disk print detail"**: Create snapshot, execute `/disk print detail where slot=snap-test-1`. Verify output contains slot, file-path, file-size, type=file. Verify output does NOT contain nvme-tcp-export.

4. **"delete snapshot via disk remove"**: Create snapshot, execute `/disk remove [find slot=snap-test-1]`. Verify snapshot removed from state. Verify backing file also removed from files state.

5. **"copy-from nonexistent source fails"**: Execute `/disk add type=file copy-from=[find slot=nonexistent] file-path=/storage-pool/metal-csi/snap-fail.img slot=snap-fail`. Verify exit code 1 and error message.

6. **"restore from snapshot creates NVMe volume"**: Create volume, create snapshot, then restore via `/disk add type=file copy-from=[find slot=snap-test-1] file-path=/storage-pool/metal-csi/pvc-restored.img file-size=10G slot=pvc-restored nvme-tcp-export=yes nvme-tcp-server-port=4420 nvme-tcp-server-nqn=nqn.2000-02.com.mikrotik:pvc-restored`. Verify new volume exists in `GetVolume()` with NVMe export enabled.

For SSH command execution, use the same pattern as existing tests — create server, start it, connect via Go's `golang.org/x/crypto/ssh` client, execute command, check output and exit status. Use port 0 for random port assignment.

**Note:** Keep tests concise. Each subtest should be independent (create fresh state or use unique slot names).
  </action>
  <verify>
Run `go test ./test/mock/... -v -run TestMockRDS_SnapshotCopyFrom` — all subtests pass.
Run `go test ./test/mock/...` — all existing tests still pass (no regressions).
  </verify>
  <done>
6 subtests verify snapshot copy-from creation, independence from source, query via disk print, deletion, error on nonexistent source, and restore to NVMe volume. All pass alongside existing mock server tests.
  </done>
</task>

</tasks>

<verification>
1. `go build ./test/mock/...` compiles without errors
2. `go test ./test/mock/...` — all tests pass (existing + new snapshot tests)
3. `grep -c "btrfs/subvolume" test/mock/rds_server.go` returns 0
4. `grep -c "copy-from" test/mock/rds_server.go` returns > 0
5. Mock server snapshot state uses Slot (not Name), has no FSLabel/ReadOnly fields
</verification>

<success_criteria>
- Mock RDS server handles copy-from snapshot semantics matching real RouterOS behavior
- Old Btrfs subvolume handlers completely removed
- Snapshot disk entries are independent copies (not linked to source)
- Snapshot entries have no NVMe export in their disk print output
- All existing mock server tests continue to pass
- New snapshot-specific tests validate the copy-from flow end-to-end
</success_criteria>

<output>
After completion, create `.planning/phases/30-snapshot-validation/30-01-SUMMARY.md`
</output>
