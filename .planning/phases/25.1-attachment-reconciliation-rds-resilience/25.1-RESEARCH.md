# Phase 25.1: Attachment Reconciliation & RDS Resilience - Research

**Researched:** 2026-02-05
**Domain:** CSI attachment state management, connection resilience, health monitoring
**Confidence:** HIGH

## Summary

This phase addresses production-critical resilience gaps exposed by the 2026-02-05 RDS storage crash incident where stale VolumeAttachment objects prevented volume reattachment, extending a 3-hour outage to 5+ hours due to manual intervention requirements.

The standard approach combines four key mechanisms: (1) startup reconciliation to detect stale attachments by querying actual node state, (2) continuous node watcher to proactively detect node failures, (3) automatic RDS connection recovery with exponential backoff, and (4) CSI Probe health check integration reflecting true backend connectivity.

The driver already has strong foundations: AttachmentManager tracks in-memory state, AttachmentReconciler runs periodic cleanup (5min default), informer-based caching prevents API throttling, and Prometheus metrics expose attachment operations. This phase extends these with startup reconciliation, node event watching, connection resilience, and health check integration.

**Primary recommendation:** Implement attachment state reconciliation on controller startup to detect and clear stale attachments before serving requests, add Kubernetes node informer event handlers to trigger immediate reconciliation when nodes become NotReady, enhance RDS SSH client with exponential backoff reconnection (1s→2s→4s→8s→16s max), and integrate connection state into CSI Probe health check to prevent misleading readiness during backend failures.

## Standard Stack

The established libraries/tools for this domain:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| k8s.io/client-go | v0.28.0 | Kubernetes client with informers | Official Kubernetes client library, provides SharedInformerFactory for cached API access preventing throttling |
| k8s.io/apimachinery | v0.28.0 | Kubernetes API types | Core types for Node, PV, VolumeAttachment objects |
| prometheus/client_golang | v1.23.2 | Prometheus metrics | Industry standard for Kubernetes observability |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| cenkalti/backoff/v4 | v4.x | Exponential backoff | Standard Go backoff library with configurable MaxElapsedTime and randomization |
| golang.org/x/time/rate | v0.3.0 | Rate limiting | Already in go.mod, useful for reconnection throttling if needed |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| cenkalti/backoff | avast/retry-go | avast/retry-go is more opinionated (function wrapper), cenkalti/backoff is lower-level and more flexible for connection management |
| Informer-based node watcher | Direct API polling | Informers provide cached access and event-driven updates, polling causes API throttling and higher latency |
| In-memory attachment state | VolumeAttachment API as source of truth | VolumeAttachment is declarative (desired state), actual attachment state requires querying nodes; hybrid approach is correct |

**Installation:**
```bash
# cenkalti/backoff not currently in go.mod - will need to add
go get github.com/cenkalti/backoff/v4
```

## Architecture Patterns

### Recommended Project Structure
```
pkg/
├── driver/
│   ├── identity.go          # Probe() health check updated
│   ├── driver.go             # Driver.Run() startup reconciliation
├── attachment/
│   ├── reconciler.go         # Existing reconciliation loop
│   ├── node_watcher.go       # NEW: Node event handler
│   ├── startup_reconcile.go  # NEW: One-time startup reconciliation
├── rds/
│   ├── ssh_client.go         # Enhanced with reconnection
│   ├── connection_manager.go # NEW: Reconnection logic with backoff
```

### Pattern 1: Informer-Based Node Watcher
**What:** Use SharedInformerFactory to watch Node events and trigger reconciliation on NotReady transitions
**When to use:** Need real-time notification of node failures without API polling
**Example:**
```go
// Source: Kubernetes controller patterns from client-go examples
// https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go

// In driver.go initialization
nodeInformer := d.informerFactory.Core().V1().Nodes().Informer()
nodeInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
    UpdateFunc: func(oldObj, newObj interface{}) {
        oldNode := oldObj.(*corev1.Node)
        newNode := newObj.(*corev1.Node)

        // Check if node transitioned to NotReady
        if isNodeReady(oldNode) && !isNodeReady(newNode) {
            klog.Infof("Node %s became NotReady, triggering attachment reconciliation", newNode.Name)
            d.attachmentReconciler.TriggerReconcile()
        }
    },
    DeleteFunc: func(obj interface{}) {
        node := obj.(*corev1.Node)
        klog.Infof("Node %s deleted, triggering attachment reconciliation", node.Name)
        d.attachmentReconciler.TriggerReconcile()
    },
})

func isNodeReady(node *corev1.Node) bool {
    for _, condition := range node.Status.Conditions {
        if condition.Type == corev1.NodeReady {
            return condition.Status == corev1.ConditionTrue
        }
    }
    return false
}
```

### Pattern 2: Exponential Backoff Reconnection
**What:** Automatic SSH reconnection with exponential backoff on connection loss
**When to use:** Transient network failures or backend service restarts
**Example:**
```go
// Source: https://pkg.go.dev/github.com/cenkalti/backoff/v4#ExponentialBackOff

import "github.com/cenkalti/backoff/v4"

func (c *sshClient) ConnectWithRetry() error {
    b := backoff.NewExponentialBackOff()
    b.InitialInterval = 1 * time.Second
    b.MaxInterval = 16 * time.Second
    b.MaxElapsedTime = 0 // Never stop trying (for background reconnection)
    b.Multiplier = 2.0
    b.RandomizationFactor = 0.1

    operation := func() error {
        return c.Connect()
    }

    notify := func(err error, duration time.Duration) {
        klog.Warningf("RDS connection failed (retry in %v): %v", duration, err)
    }

    return backoff.RetryNotify(operation, b, notify)
}
```

### Pattern 3: Startup Reconciliation
**What:** One-time reconciliation on controller startup to detect stale attachments from previous crashes
**When to use:** Controller restart, pod eviction, or operator upgrade
**Example:**
```go
// Source: Controller initialization patterns from kubernetes-csi/external-attacher
// https://github.com/kubernetes-csi/external-attacher

func (d *Driver) Run(endpoint string) error {
    // ... existing initialization ...

    // Perform startup reconciliation if attachment manager enabled
    if d.attachmentReconciler != nil {
        klog.Info("Performing startup attachment reconciliation...")
        ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
        defer cancel()

        if err := d.performStartupReconciliation(ctx); err != nil {
            // Log warning but don't fail startup - periodic reconciler will fix
            klog.Warningf("Startup reconciliation failed (periodic reconciler will retry): %v", err)
        }
    }

    // ... start gRPC server ...
}

func (d *Driver) performStartupReconciliation(ctx context.Context) error {
    attachments := d.attachmentManager.ListAttachments()

    for volumeID, state := range attachments {
        // Query actual attachment state from node
        nodeExists, err := d.checkNodeExists(ctx, state.NodeID)
        if err != nil {
            klog.Warningf("Failed to check node %s for volume %s: %v", state.NodeID, volumeID, err)
            continue
        }

        if !nodeExists {
            klog.Infof("Clearing stale attachment on startup: volume=%s node=%s (node deleted)", volumeID, state.NodeID)
            if err := d.attachmentManager.UntrackAttachment(ctx, volumeID); err != nil {
                return fmt.Errorf("failed to clear stale attachment: %w", err)
            }
        }
    }

    return nil
}
```

### Pattern 4: Connection-Aware Health Check
**What:** CSI Probe RPC returns not-ready when backend connection is lost
**When to use:** Prevent misleading health status during infrastructure failures
**Example:**
```go
// Source: CSI spec Identity service requirements
// https://github.com/container-storage-interface/spec/blob/master/spec.md#probe

func (ids *IdentityServer) Probe(ctx context.Context, req *csi.ProbeRequest) (*csi.ProbeResponse, error) {
    klog.V(5).Info("Probe called")

    ready := true

    // Check if RDS client is connected (if controller mode)
    if ids.driver.rdsClient != nil {
        if !ids.driver.rdsClient.IsConnected() {
            klog.Warning("RDS client is not connected - reporting not ready")
            ready = false

            // Record metric for connection state
            if ids.driver.metrics != nil {
                ids.driver.metrics.RecordConnectionState("disconnected")
            }
        }
    }

    return &csi.ProbeResponse{
        Ready: wrapperspb.Bool(ready),
    }, nil
}
```

### Anti-Patterns to Avoid
- **Synchronous reconnection in CSI RPC calls:** Don't block CreateVolume/DeleteVolume waiting for reconnection - fail fast with codes.Unavailable and let Kubernetes retry
- **Unbounded retry loops:** Always use MaxElapsedTime or retry count limits to prevent infinite loops during persistent failures
- **Direct API calls for node checking:** Use informer cache (nodeLister.Get) to avoid API throttling during reconciliation
- **Clearing attachments immediately on node NotReady:** Use grace period to handle transient kubelet restarts (30s default matches existing AttachmentReconciler)

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Exponential backoff | Custom retry loop with time.Sleep | cenkalti/backoff/v4 | Handles jitter, max elapsed time, reset logic correctly; well-tested in production |
| Node state watching | time.Ticker polling API | SharedInformerFactory with event handlers | Informers provide caching, event-driven updates, handle API throttling |
| Connection health checking | Periodic ping goroutine | CSI Probe RPC with IsConnected() | CSI spec-compliant, integrates with Kubernetes liveness probe |
| Leader election | Custom lock file or database | client-go/tools/leaderelection | Kubernetes-native, handles split-brain, required for HA controller |

**Key insight:** The Kubernetes ecosystem provides robust primitives for watching cluster state (informers), managing connection reliability (backoff libraries), and coordinating distributed processes (leader election). Custom solutions miss edge cases like cache consistency, API throttling, and network partition handling.

## Common Pitfalls

### Pitfall 1: Stale Cache During Startup Reconciliation
**What goes wrong:** AttachmentManager.Initialize() loads VolumeAttachments from API, but node informer cache may not be populated yet, causing false "node deleted" detections
**Why it happens:** SharedInformerFactory.Start() is asynchronous and WaitForCacheSync() may not have completed before startup reconciliation runs
**How to avoid:**
1. Ensure informers are started and synced BEFORE running startup reconciliation
2. Add explicit cache sync check: `d.informerFactory.WaitForCacheSync(ctx.Done())`
3. Document startup ordering in code comments
**Warning signs:** Logs showing "node does not exist" for nodes that actually exist during startup

### Pitfall 2: Reconnection Storm During RDS Maintenance
**What goes wrong:** Multiple controller replicas all detect RDS connection loss simultaneously and hammer RDS with reconnection attempts during planned maintenance
**Why it happens:** No coordination between replicas, exponential backoff resets independently
**How to avoid:**
1. Use jitter in backoff (RandomizationFactor: 0.1-0.3) to desynchronize reconnection attempts
2. Consider circuit breaker pattern (already have sony/gobreaker in go.mod) for sustained failures
3. Document RDS maintenance procedures: scale controller to 0 replicas during upgrades
**Warning signs:** RDS SSH logs showing hundreds of connection attempts within seconds

### Pitfall 3: Race Between Reconciler and Node Watcher
**What goes wrong:** Node watcher triggers reconciliation while periodic reconciler is running, causing duplicate cleanup attempts and attachment state corruption
**Why it happens:** No synchronization between event-driven and periodic reconciliation
**How to avoid:**
1. AttachmentReconciler.reconcile() already uses manager's per-volume locks via UntrackAttachment()
2. TriggerReconcile() should skip if reconciliation already in progress (check with mutex)
3. Or: make TriggerReconcile() just reset the ticker (next periodic run happens sooner)
**Warning signs:** "Failed to clear stale attachment" errors in logs, duplicate event posting

### Pitfall 4: SSH Session Leaks Preventing Reconnection
**What goes wrong:** Old SSH connection isn't properly closed before reconnection attempt, leading to resource exhaustion
**Why it happens:** IsConnected() creates test session but might not close client if test fails
**How to avoid:**
1. Always defer session.Close() in IsConnected()
2. Call sshClient.Close() before attempting reconnection
3. Add SSH connection metrics (active connections gauge) to detect leaks
**Warning signs:** "too many open files" errors, gradual memory growth in controller pod

### Pitfall 5: VolumeAttachment vs Actual State Confusion
**What goes wrong:** Code treats VolumeAttachment.Status.Attached=true as authoritative for actual attachment state
**Why it happens:** Misunderstanding CSI architecture - VolumeAttachment is managed by external-attacher sidecar, not this driver
**How to avoid:**
1. Document clearly: driver maintains in-memory AttachmentManager state, VolumeAttachment is external
2. Startup reconciliation queries AttachmentManager (in-memory), not VolumeAttachment API
3. Node watcher checks actual node existence, not VolumeAttachment objects
4. Consider: This driver doesn't implement ControllerPublish/ControllerUnpublish (no external-attacher sidecar), so VolumeAttachment objects may not exist
**Warning signs:** Queries to VolumeAttachment API failing with 404, confusion about who owns attachment state

## Code Examples

Verified patterns from official sources:

### Node Watcher with Informer
```go
// Source: client-go informer examples
// https://github.com/kubernetes/client-go/tree/master/examples/workqueue

package attachment

import (
    corev1 "k8s.io/api/core/v1"
    "k8s.io/client-go/tools/cache"
    "k8s.io/klog/v2"
)

// NodeWatcher watches for node failures and triggers attachment reconciliation
type NodeWatcher struct {
    reconciler *AttachmentReconciler
}

// NewNodeWatcher creates a node watcher that integrates with the node informer
func NewNodeWatcher(reconciler *AttachmentReconciler) *NodeWatcher {
    return &NodeWatcher{reconciler: reconciler}
}

// GetEventHandlers returns ResourceEventHandlerFuncs for node informer
func (nw *NodeWatcher) GetEventHandlers() cache.ResourceEventHandlerFuncs {
    return cache.ResourceEventHandlerFuncs{
        UpdateFunc: func(oldObj, newObj interface{}) {
            oldNode := oldObj.(*corev1.Node)
            newNode := newObj.(*corev1.Node)

            // Check if node transitioned to NotReady
            oldReady := isNodeReady(oldNode)
            newReady := isNodeReady(newNode)

            if oldReady && !newReady {
                klog.Infof("Node %s became NotReady, triggering attachment reconciliation", newNode.Name)
                nw.reconciler.TriggerReconcile()
            }
        },
        DeleteFunc: func(obj interface{}) {
            node := obj.(*corev1.Node)
            klog.Infof("Node %s deleted, triggering attachment reconciliation", node.Name)
            nw.reconciler.TriggerReconcile()
        },
    }
}

func isNodeReady(node *corev1.Node) bool {
    for _, condition := range node.Status.Conditions {
        if condition.Type == corev1.NodeReady {
            return condition.Status == corev1.ConditionTrue
        }
    }
    return false
}
```

### Exponential Backoff Connection Manager
```go
// Source: cenkalti/backoff v4 documentation and examples
// https://pkg.go.dev/github.com/cenkalti/backoff/v4

package rds

import (
    "time"
    "github.com/cenkalti/backoff/v4"
    "k8s.io/klog/v2"
)

// ConnectionManager handles RDS connection with automatic reconnection
type ConnectionManager struct {
    client *sshClient
    backoff backoff.BackOff
    connected bool
    mu sync.RWMutex
}

// NewConnectionManager creates a connection manager with configured backoff
func NewConnectionManager(client *sshClient) *ConnectionManager {
    b := backoff.NewExponentialBackOff()
    b.InitialInterval = 1 * time.Second
    b.MaxInterval = 16 * time.Second
    b.MaxElapsedTime = 5 * time.Minute // Give up after 5 minutes
    b.Multiplier = 2.0
    b.RandomizationFactor = 0.1 // 10% jitter

    return &ConnectionManager{
        client: client,
        backoff: b,
    }
}

// ConnectWithRetry attempts connection with exponential backoff
func (cm *ConnectionManager) ConnectWithRetry() error {
    operation := func() error {
        err := cm.client.Connect()
        if err != nil {
            return err
        }

        cm.mu.Lock()
        cm.connected = true
        cm.mu.Unlock()

        return nil
    }

    notify := func(err error, duration time.Duration) {
        klog.Warningf("RDS connection failed, retrying in %v: %v", duration, err)
    }

    return backoff.RetryNotify(operation, cm.backoff, notify)
}

// IsConnected returns connection state
func (cm *ConnectionManager) IsConnected() bool {
    cm.mu.RLock()
    defer cm.mu.RUnlock()
    return cm.connected
}
```

### Startup Reconciliation
```go
// Source: Controller initialization patterns
// Inspired by kubernetes-csi/external-attacher startup logic

package driver

import (
    "context"
    "time"
    "k8s.io/klog/v2"
)

// performStartupReconciliation detects and clears stale attachments on startup
func (d *Driver) performStartupReconciliation(ctx context.Context) error {
    klog.Info("Starting attachment reconciliation on controller startup")
    startTime := time.Now()

    // Get all tracked attachments from in-memory state
    attachments := d.attachmentManager.ListAttachments()

    clearedCount := 0
    for volumeID, state := range attachments {
        // Check if context cancelled
        if ctx.Err() != nil {
            return ctx.Err()
        }

        // Query node existence from informer cache (no API call)
        nodeExists, err := d.checkNodeExistsFromCache(state.NodeID)
        if err != nil {
            // Cache not ready - skip this volume, periodic reconciler will fix
            klog.Warningf("Cannot verify node %s for volume %s (cache not ready): %v", state.NodeID, volumeID, err)
            continue
        }

        if !nodeExists {
            klog.Infof("Clearing stale attachment on startup: volume=%s node=%s (node deleted)", volumeID, state.NodeID)
            if err := d.attachmentManager.UntrackAttachment(ctx, volumeID); err != nil {
                klog.Errorf("Failed to clear stale attachment for volume %s: %v", volumeID, err)
                continue
            }
            clearedCount++

            // Record metrics
            if d.metrics != nil {
                d.metrics.RecordStaleAttachmentCleared()
            }
        }
    }

    duration := time.Since(startTime)
    if clearedCount > 0 {
        klog.Infof("Startup reconciliation complete: cleared %d stale attachments (duration=%v)", clearedCount, duration)
    } else {
        klog.Infof("Startup reconciliation complete: no stale attachments (duration=%v)", duration)
    }

    return nil
}

// checkNodeExistsFromCache checks node existence using informer cache
func (d *Driver) checkNodeExistsFromCache(nodeID string) (bool, error) {
    if d.informerFactory == nil {
        return false, fmt.Errorf("informer factory not initialized")
    }

    nodeLister := d.informerFactory.Core().V1().Nodes().Lister()
    _, err := nodeLister.Get(nodeID)
    if err != nil {
        if errors.IsNotFound(err) {
            return false, nil
        }
        return false, err
    }
    return true, nil
}
```

### Prometheus Metrics for Connection State
```go
// Source: Existing observability/prometheus.go patterns

package observability

// Add to Metrics struct
type Metrics struct {
    // ... existing fields ...

    // RDS connection metrics
    rdsConnectionState  *prometheus.GaugeVec    // 1=connected, 0=disconnected
    rdsReconnectTotal   *prometheus.CounterVec  // by status (success, failure)
    rdsReconnectDuration prometheus.Histogram
}

// In NewMetrics()
rdsConnectionState: prometheus.NewGaugeVec(
    prometheus.GaugeOpts{
        Namespace: namespace,
        Subsystem: "rds",
        Name:      "connection_state",
        Help:      "RDS connection state (1=connected, 0=disconnected)",
    },
    []string{"address"},
),

rdsReconnectTotal: prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Namespace: namespace,
        Subsystem: "rds",
        Name:      "reconnect_total",
        Help:      "Total RDS reconnection attempts by status",
    },
    []string{"status"}, // success, failure
),

rdsReconnectDuration: prometheus.NewHistogram(prometheus.HistogramOpts{
    Namespace: namespace,
    Subsystem: "rds",
    Name:      "reconnect_duration_seconds",
    Help:      "Duration of RDS reconnection attempts",
    Buckets:   []float64{0.1, 0.5, 1, 2, 5, 10, 30, 60},
}),

// Recording methods
func (m *Metrics) RecordConnectionState(address string, connected bool) {
    state := 0.0
    if connected {
        state = 1.0
    }
    m.rdsConnectionState.WithLabelValues(address).Set(state)
}

func (m *Metrics) RecordReconnectAttempt(err error, duration time.Duration) {
    status := "success"
    if err != nil {
        status = "failure"
    }
    m.rdsReconnectTotal.WithLabelValues(status).Inc()
    m.rdsReconnectDuration.Observe(duration.Seconds())
}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Periodic reconciliation only | Startup reconciliation + periodic + event-driven | 2024+ (industry trend) | Faster detection of stale state, reduced time-to-recovery from crashes |
| Fail on connection loss | Automatic reconnection with backoff | 2020+ (cloud-native patterns) | Resilient to transient network issues and backend restarts |
| Probe always returns ready | Connection-aware health checks | CSI spec 1.5+ (2021) | Prevents misleading health status during infrastructure failures |
| Direct API polling | Informer-based watching | Kubernetes 1.11+ (2018) | Eliminates API throttling, reduces latency, provides cached access |

**Deprecated/outdated:**
- **Manual VolumeAttachment cleanup via kubectl:** Modern CSI drivers reconcile automatically on startup
- **SSH connection without retry logic:** Cloud-native applications expect automatic reconnection
- **Health checks ignoring backend state:** CSI spec mandates Probe reflects actual readiness

## Open Questions

Things that couldn't be fully resolved:

1. **VolumeAttachment API Usage**
   - What we know: Driver doesn't implement ControllerPublish/ControllerUnpublish (no external-attacher sidecar in deployment)
   - What's unclear: Whether VolumeAttachment objects exist at all for this driver's volumes
   - Recommendation: Confirm driver architecture - if no external-attacher, VolumeAttachment reconciliation may not be relevant; focus on in-memory AttachmentManager state only

2. **Leader Election for Controller**
   - What we know: ROADMAP shows "Single Controller: One controller replica (no HA/leader election for homelab simplicity)"
   - What's unclear: Whether production incident justifies adding HA now
   - Recommendation: Phase 25.1 focuses on single-controller resilience; defer HA to future phase if needed

3. **Optimal Reconciliation Frequency**
   - What we know: Current AttachmentReconciler uses 5-minute interval
   - What's unclear: Whether node watcher makes periodic reconciliation redundant
   - Recommendation: Keep both - periodic catches any missed events, event-driven provides fast response

4. **Circuit Breaker vs Unbounded Retry**
   - What we know: RDS may be down for extended periods during maintenance
   - What's unclear: Should reconnection give up eventually or retry forever?
   - Recommendation: Use MaxElapsedTime for startup connection (fail fast if RDS down), but unbounded retry for background reconnection (assume RDS will eventually return)

## Sources

### Primary (HIGH confidence)
- [Kubernetes client-go informer examples](https://github.com/kubernetes/client-go/tree/master/examples/workqueue) - Official informer patterns
- [CSI Specification v1.12.0 Identity Service](https://github.com/container-storage-interface/spec/blob/master/spec.md#probe) - Probe RPC requirements
- [cenkalti/backoff v4 documentation](https://pkg.go.dev/github.com/cenkalti/backoff/v4) - Exponential backoff implementation
- [kubernetes-csi/external-attacher README](https://github.com/kubernetes-csi/external-attacher/blob/master/README.md) - VolumeAttachment reconciliation patterns
- Existing codebase: pkg/attachment/reconciler.go, pkg/attachment/manager.go, pkg/driver/identity.go, pkg/rds/ssh_client.go

### Secondary (MEDIUM confidence)
- [Introducing Node Readiness Controller | Kubernetes](https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/) - Recent node state management patterns (2026-02-03)
- [Recover CSI volumes from dangling attachments PR](https://github.com/kubernetes/kubernetes/pull/96617) - Kubernetes-level stale attachment handling
- [VolumeAttachment stale state issue](https://github.com/kubernetes-csi/external-attacher/issues/416) - Community discussions on attachment state problems
- [AWS EBS CSI Driver metrics documentation](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/metrics.md) - Attachment state metrics patterns

### Tertiary (LOW confidence)
- [How to Implement Retry Logic in Go with Exponential Backoff](https://oneuptime.com/blog/post/2026-01-07-go-retry-exponential-backoff/view) - General Go backoff patterns
- Various Kubernetes troubleshooting guides for node NotReady and attachment issues

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Libraries already in use or well-established in ecosystem
- Architecture: HIGH - Patterns verified against existing codebase and official Kubernetes examples
- Pitfalls: MEDIUM-HIGH - Based on known CSI issues and code review, but not production-tested in this specific driver

**Research date:** 2026-02-05
**Valid until:** 30 days (stable domain - Kubernetes CSI patterns evolve slowly)

**Key uncertainties requiring validation during planning:**
1. VolumeAttachment API relevance (driver architecture question, not research gap)
2. Optimal backoff parameters (requires production testing to tune)
3. Race condition handling between reconciler and node watcher (needs careful design review)
