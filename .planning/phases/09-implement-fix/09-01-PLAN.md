---
phase: 09-implement-fix
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pkg/virt-controller/watch/vmi/volume-hotplug.go
autonomous: true

must_haves:
  truths:
    - "Code path from volume add to pod deletion is documented"
    - "cleanupAttachmentPods checks VolumeStatus.Phase before deleting old pod"
    - "Old attachment pod remains alive until new pod volumes are VolumeReady"
  artifacts:
    - path: "pkg/virt-controller/watch/vmi/volume-hotplug.go"
      provides: "Modified cleanupAttachmentPods with readiness check"
      contains: "VolumeReady"
    - path: ".planning/phases/09-implement-fix/09-01-CODEPATH.md"
      provides: "Documented code path for bug analysis"
  key_links:
    - from: "cleanupAttachmentPods"
      to: "VolumeStatus.Phase"
      via: "readiness check before deletion"
      pattern: "VolumeReady|Phase"
---

<objective>
Document the hotplug code path that causes the bug, then implement the fix in `cleanupAttachmentPods` to wait for new pod volume readiness before deleting the old pod.

Purpose: This is the core fix for the KubeVirt hotplug bug. Understanding the code path is essential for a minimal, correct fix.

Output:
- Documented code path explaining the bug
- Modified `volume-hotplug.go` with the fix
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/milestones/v0.5-ROADMAP.md
@.planning/milestones/v0.5-REQUIREMENTS.md
@.planning/STATE.md

Key prior analysis from STATE.md:
- Fix location: pkg/virt-controller/watch/vmi/volume-hotplug.go
- Key functions: handleHotplugVolumes, cleanupAttachmentPods, getActiveAndOldAttachmentPods
- Bug: Line 46 podVolumesMatchesReadyVolumes returns false when new volume added
- This triggers new pod creation AND old pod deletion without coordination
- Fix approach: Modify cleanupAttachmentPods to check VolumeStatus.Phase == VolumeReady for all volumes in new pod before allowing old pod deletion

KubeVirt fork: https://github.com/whiskey-works/kubevirt
Working directory: /tmp/kubevirt-fork (or clone fresh)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Document the hotplug code path</name>
  <files>
    .planning/phases/09-implement-fix/09-01-CODEPATH.md
    pkg/virt-controller/watch/vmi/volume-hotplug.go (read-only for analysis)
  </files>
  <action>
1. Clone the KubeVirt fork if not already present:
   ```bash
   cd /tmp
   git clone https://github.com/whiskey-works/kubevirt kubevirt-fork || true
   cd kubevirt-fork
   git checkout main
   ```

2. Read and analyze the following files in the fork:
   - pkg/virt-controller/watch/vmi/volume-hotplug.go (main file)
   - Look for handleHotplugVolumes, cleanupAttachmentPods, getActiveAndOldAttachmentPods

3. Trace the code path that causes the bug:
   - Start: Volume add request comes in (DataVolume or PVC annotation change)
   - Middle: How handleHotplugVolumes decides to create new pod
   - Middle: How podVolumesMatchesReadyVolumes returns false for new volumes
   - End: How cleanupAttachmentPods deletes old pod prematurely

4. Create 09-01-CODEPATH.md documenting:
   - The triggering condition (what starts the hotplug)
   - The race condition (why old pod is deleted too early)
   - The specific line/function where the bug manifests
   - The proposed fix location and approach

Keep the document concise - focus on the specific bug, not general architecture.
  </action>
  <verify>
    - File .planning/phases/09-implement-fix/09-01-CODEPATH.md exists
    - grep -q "cleanupAttachmentPods" 09-01-CODEPATH.md (fix location identified)
    - grep -q "VolumeReady\|VolumeStatus" 09-01-CODEPATH.md (readiness check mentioned)
    - grep -qi "race\|premature\|before.*ready" 09-01-CODEPATH.md (race condition explained)
  </verify>
  <done>
    Code path documented with clear identification of bug location and fix approach
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement the fix in cleanupAttachmentPods</name>
  <files>
    pkg/virt-controller/watch/vmi/volume-hotplug.go
  </files>
  <action>
1. Navigate to the kubevirt fork:
   ```bash
   cd /tmp/kubevirt-fork
   git checkout -b hotplug-fix-v1 || git checkout hotplug-fix-v1
   ```

2. Open pkg/virt-controller/watch/vmi/volume-hotplug.go

3. Locate the cleanupAttachmentPods function (or wherever old pod deletion happens)

4. Implement the fix:
   - Before deleting the old attachment pod, check that ALL volumes in the new pod have VolumeStatus.Phase == VolumeReady
   - If any volume in the new pod is not ready, do NOT delete the old pod yet
   - Return early and let the next reconcile loop check again

5. The fix logic should be approximately:
   ```go
   // Before deleting old pod, verify new pod volumes are ready
   for _, vol := range vmi.Status.VolumeStatus {
       // Only check volumes that should be in the new pod
       if isHotplugVolume(vol) && vol.Phase != VolumeReady {
           // New pod volumes not ready yet, keep old pod alive
           return nil
       }
   }
   // All new pod volumes ready, safe to delete old pod
   ```

6. Ensure the fix:
   - Does NOT break single-volume hotplug (still works same as before)
   - Does NOT break volume removal (removal uses different path)
   - Uses existing VolumeStatus.Phase field (no new fields needed)

7. Handle edge cases:
   - Empty VolumeStatus: If no VolumeStatus entries, don't block deletion (no volumes to protect)
   - New pod not created yet: If new pod doesn't exist, don't delete old pod (wait for next reconcile)
   - Volume removal scenario: Only check readiness for volumes being ADDED, not removed
   - Mixed states: If ANY hotplug volume is not ready, keep old pod alive

7. Commit the change:
   ```bash
   git add pkg/virt-controller/watch/vmi/volume-hotplug.go
   git commit -m "fix(hotplug): wait for new pod volumes ready before deleting old pod

   When hotplugging a new volume, the virt-controller creates a new attachment
   pod with all volumes. Previously, the old attachment pod was deleted
   immediately, causing block devices for existing volumes to disappear.

   This change modifies cleanupAttachmentPods to check that all volumes in
   the new pod have reached VolumeReady phase before deleting the old pod.
   This ensures existing block devices remain available during the handoff.

   Fixes: kubevirt/kubevirt#6564
   Fixes: kubevirt/kubevirt#9708
   Ref: kubevirt/kubevirt#16520"
   ```
  </action>
  <verify>
    - git diff shows changes to volume-hotplug.go
    - Change includes VolumeReady check before old pod deletion
    - Change preserves existing single-volume and removal behavior
    - Commit exists on hotplug-fix-v1 branch
  </verify>
  <done>
    Fix implemented and committed: cleanupAttachmentPods now waits for new pod volume readiness before deleting old pod
  </done>
</task>

</tasks>

<verification>
1. Code path document exists and explains the bug clearly
2. Fix is committed to hotplug-fix-v1 branch
3. Fix logic checks VolumeStatus.Phase == VolumeReady
4. Existing behavior preserved for single-volume and removal scenarios
</verification>

<success_criteria>
- 09-01-CODEPATH.md documents the race condition
- volume-hotplug.go modified with readiness check
- Commit on hotplug-fix-v1 branch with proper message
- No syntax errors (go build succeeds for the package)
</success_criteria>

<output>
After completion, create `.planning/phases/09-implement-fix/09-01-SUMMARY.md`
</output>
