---
phase: 09-implement-fix
plan: 03
type: execute
wave: 3
depends_on: ["09-02"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "Custom KubeVirt images built and pushed to ghcr.io"
    - "Metal cluster running patched virt-controller"
    - "Multi-volume hotplug works without VM pause or I/O errors"
    - "Volume removal still works correctly"
  artifacts:
    - path: ".planning/phases/09-implement-fix/09-03-VALIDATION.md"
      provides: "Manual validation results"
  key_links:
    - from: "ghcr.io/whiskey-works/kubevirt images"
      to: "metal cluster KubeVirt deployment"
      via: "image override"
---

<objective>
Build custom KubeVirt images with the fix, deploy to the metal cluster, and manually validate multi-volume hotplug works without issues.

Purpose: Real-world validation on production-like environment confirms the fix works outside of unit test mocks.

Output:
- Built and pushed custom images
- Successful multi-volume hotplug test
- Documented validation results
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/milestones/v0.5-ROADMAP.md
@.planning/phases/09-implement-fix/09-01-SUMMARY.md
@.planning/phases/09-implement-fix/09-02-SUMMARY.md

KubeVirt fork: https://github.com/whiskey-works/kubevirt
Branch: hotplug-fix-v1
Working directory: /tmp/kubevirt-fork

Build commands from STATE.md:
```bash
export DOCKER_PREFIX=ghcr.io/whiskey-works/kubevirt
export DOCKER_TAG=hotplug-fix-v1
make bazel-build-images
make bazel-push-images
```

Metal cluster access required for validation.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build and push custom KubeVirt images</name>
  <files></files>
  <action>
1. Navigate to the fork and ensure on fix branch:
   ```bash
   cd /tmp/kubevirt-fork
   git checkout hotplug-fix-v1
   git pull origin hotplug-fix-v1
   ```

2. Build the images using Bazel:
   ```bash
   export DOCKER_PREFIX=ghcr.io/whiskey-works/kubevirt
   export DOCKER_TAG=hotplug-fix-v1

   # This builds virt-controller, virt-handler, virt-api, etc.
   make bazel-build-images 2>&1 | tail -50
   ```

3. Push images to registry:
   ```bash
   # May need: docker login ghcr.io -u whiskey-works
   make bazel-push-images 2>&1 | tail -50
   ```

4. Verify images are in registry:
   ```bash
   # Check virt-controller image exists
   docker pull ghcr.io/whiskey-works/kubevirt/virt-controller:hotplug-fix-v1 --quiet && echo "virt-controller image available"
   ```

5. If CI workflow is set up in Phase 8, the push to hotplug-fix-v1 branch may have already triggered a build. Check GitHub Actions.

Note: If local Bazel build is problematic, rely on GitHub Actions CI from Phase 8 workflow.
  </action>
  <verify>
    - Images built (bazel-build-images succeeds or CI builds)
    - Images pushed to ghcr.io/whiskey-works/kubevirt/*:hotplug-fix-v1
    - Can pull virt-controller image from registry
  </verify>
  <done>
    Custom KubeVirt images with fix are available in ghcr.io registry
  </done>
</task>

<task type="auto">
  <name>Task 2: Deploy patched KubeVirt to metal cluster</name>
  <files></files>
  <action>
1. Connect to metal cluster:
   ```bash
   # Ensure kubectl context is set to metal cluster
   kubectl config current-context
   kubectl get nodes
   ```

2. Identify current KubeVirt deployment:
   ```bash
   kubectl -n kubevirt get deployment virt-controller -o yaml | grep image:
   kubectl -n kubevirt get daemonset virt-handler -o yaml | grep image:
   ```

3. Patch virt-controller to use custom image:
   ```bash
   kubectl -n kubevirt set image deployment/virt-controller \
     virt-controller=ghcr.io/whiskey-works/kubevirt/virt-controller:hotplug-fix-v1
   ```

4. Wait for rollout:
   ```bash
   kubectl -n kubevirt rollout status deployment/virt-controller --timeout=120s
   ```

5. Verify new image is running:
   ```bash
   kubectl -n kubevirt get pods -l kubevirt.io=virt-controller -o wide
   kubectl -n kubevirt logs deployment/virt-controller --tail=20
   ```

Note: virt-handler may also need patching depending on where the fix applies. Check if the readiness check is in virt-controller only.
  </action>
  <verify>
    - kubectl context is metal cluster
    - virt-controller pod running with custom image
    - Pod logs show no errors
  </verify>
  <done>
    Metal cluster running patched virt-controller with the hotplug fix
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Custom KubeVirt images built and deployed with the hotplug fix. The patched virt-controller is running on the metal cluster.
  </what-built>
  <how-to-verify>
    **Test 1: Multi-volume hotplug (the main fix)**
    1. Create a test VM (or use existing nested K3s worker)
    2. Ensure VM is running and has 1 volume attached
    3. Hotplug a second volume:
       ```bash
       # Create a DataVolume or PVC
       # Add annotation to VMI to attach it
       virtctl addvolume <vmi-name> --volume-name=test-vol-2 --persist
       ```
    4. While vol2 is attaching, verify vol1 stays accessible:
       ```bash
       # Inside VM, check existing volume is still mounted and readable
       cat /mnt/existing-volume/testfile
       ```
    5. Hotplug a third volume while second is still attaching
    6. Verify NO VM pause, NO I/O errors, all volumes accessible

    **Test 2: Volume removal still works**
    1. Remove one of the hotplugged volumes:
       ```bash
       virtctl removevolume <vmi-name> --volume-name=test-vol-3
       ```
    2. Verify volume is detached cleanly
    3. Verify remaining volumes still work

    **Test 3: Single volume hotplug (regression check)**
    1. Start a fresh VM with no extra volumes
    2. Hotplug a single volume
    3. Verify it works as before the fix

    **Expected results:**
    - All hotplug operations succeed
    - VM never pauses during hotplug
    - No I/O errors in VM logs or dmesg
    - All volumes accessible after operations complete
  </how-to-verify>
  <resume-signal>
    Type "validated" if all tests pass, or describe any issues found
  </resume-signal>
</task>

<task type="auto">
  <name>Task 3: Document validation results</name>
  <files>
    .planning/phases/09-implement-fix/09-03-VALIDATION.md
  </files>
  <action>
Based on the checkpoint verification results, create a validation document:

1. Create .planning/phases/09-implement-fix/09-03-VALIDATION.md with:
   - Date and cluster details
   - Test results for each scenario
   - Any issues encountered
   - Screenshots or logs if relevant
   - Final verdict: PASS or FAIL

2. Format:
   ```markdown
   # Phase 9 Validation Results

   **Date:** [date]
   **Cluster:** [cluster name]
   **KubeVirt Version:** [base version + hotplug-fix-v1]

   ## Test Results

   ### Multi-volume Hotplug
   - Result: PASS/FAIL
   - Details: [what happened]

   ### Volume Removal
   - Result: PASS/FAIL
   - Details: [what happened]

   ### Single Volume Hotplug (Regression)
   - Result: PASS/FAIL
   - Details: [what happened]

   ## Issues Found
   [Any issues, or "None"]

   ## Verdict
   [PASS/FAIL with summary]
   ```
  </action>
  <verify>
    - 09-03-VALIDATION.md exists
    - All test scenarios documented
    - Clear verdict provided
  </verify>
  <done>
    Validation results documented with clear PASS/FAIL verdict
  </done>
</task>

</tasks>

<verification>
1. Custom images available in ghcr.io
2. Metal cluster running patched virt-controller
3. Multi-volume hotplug tested without VM pause/I/O errors
4. Volume removal verified working
5. Validation document created
</verification>

<success_criteria>
- Images built and pushed to registry
- Patched KubeVirt deployed to metal cluster
- Manual testing confirms fix works (no VM pauses during multi-volume hotplug)
- Volume removal still works correctly
- 09-03-VALIDATION.md documents results
</success_criteria>

<output>
After completion, create `.planning/phases/09-implement-fix/09-03-SUMMARY.md`
</output>
