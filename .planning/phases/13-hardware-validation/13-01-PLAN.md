---
phase: 13-hardware-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - test/e2e/HARDWARE_VALIDATION.md
autonomous: false

must_haves:
  truths:
    - "KubeVirt VM boots with RDS block volume"
    - "VM can read and write data to block volume"
    - "KubeVirt live migration moves VM between nodes"
    - "Migration metrics are emitted during migration"
    - "Data integrity preserved after migration"
  artifacts:
    - path: "test/e2e/HARDWARE_VALIDATION.md"
      provides: "Hardware validation results and runbook"
      min_lines: 50
  key_links:
    - from: "PVC with volumeMode: Block"
      to: "KubeVirt VirtualMachine"
      via: "dataVolumeTemplates or volumes.persistentVolumeClaim"
      pattern: "volumeMode.*Block"
---

<objective>
Validate that KubeVirt VMs boot and live migrate successfully with RDS block volumes on the metal cluster.

Purpose: Prove Phase 11-12 block volume implementation works in production environment with real KubeVirt workloads. This is the final validation gate for v0.6.0 Block Volume Support milestone.

Output:
- Hardware validation results documented
- Known issues captured (if any)
- v0.6.0 release readiness confirmed
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-hardware-validation/13-CONTEXT.md
@.planning/phases/11-block-volume-node-operations/11-03-SUMMARY.md
@.planning/phases/12-compatibility-and-quality/12-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deploy latest driver version</name>
  <files>N/A - kubectl operations</files>
  <action>
  Wait for any active CI/CD pipeline to complete (check GitHub/Gitea actions).

  Deploy the latest driver version to the metal cluster:

  1. Check current deployed version:
     ```bash
     kubectl -n rds-csi-system get pods -o wide
     kubectl -n rds-csi-system get ds rds-csi-node -o jsonpath='{.spec.template.spec.containers[0].image}'
     ```

  2. If not the latest version (post-Phase 12), trigger redeployment:
     ```bash
     # Option 1: If using kustomize
     kubectl apply -k deploy/kubernetes/

     # Option 2: If using Helm
     helm upgrade rds-csi deploy/helm/rds-csi/ -n rds-csi-system

     # Option 3: Trigger rollout restart if image uses :latest or :dev tag
     kubectl -n rds-csi-system rollout restart deployment rds-csi-controller
     kubectl -n rds-csi-system rollout restart daemonset rds-csi-node
     ```

  3. Wait for pods to be ready:
     ```bash
     kubectl -n rds-csi-system rollout status deployment rds-csi-controller
     kubectl -n rds-csi-system rollout status daemonset rds-csi-node
     ```

  4. Verify node plugin running on worker nodes:
     ```bash
     kubectl get nodes
     kubectl -n rds-csi-system get pods -o wide
     ```
  </action>
  <verify>
  - `kubectl -n rds-csi-system get pods` shows all pods Running
  - Controller pod ready (1/1)
  - Node pods running on each worker node
  - No pod restarts or CrashLoopBackOff
  </verify>
  <done>Driver deployed and healthy on metal cluster</done>
</task>

<task type="auto">
  <name>Task 2: Create test PVC and KubeVirt VM</name>
  <files>test/e2e/HARDWARE_VALIDATION.md (to document applied manifests)</files>
  <action>
  Create test resources for hardware validation.

  1. Create a block-mode PVC (5Gi for reasonable test size):
     ```yaml
     # test-block-pvc.yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: test-block-vm-disk
       namespace: default
     spec:
       accessModes:
         - ReadWriteMany  # RWX for live migration
       volumeMode: Block
       resources:
         requests:
           storage: 5Gi
       storageClassName: rds-csi  # or your StorageClass name
     ```
     Apply: `kubectl apply -f test-block-pvc.yaml`

  2. Verify PVC bound:
     ```bash
     kubectl get pvc test-block-vm-disk
     kubectl describe pvc test-block-vm-disk
     ```

  3. Create a simple KubeVirt VirtualMachine:
     ```yaml
     # test-block-vm.yaml
     apiVersion: kubevirt.io/v1
     kind: VirtualMachine
     metadata:
       name: test-block-vm
       namespace: default
     spec:
       running: true
       template:
         metadata:
           labels:
             kubevirt.io/vm: test-block-vm
         spec:
           domain:
             devices:
               disks:
                 - name: rootdisk
                   disk:
                     bus: virtio
                 - name: datadisk
                   disk:
                     bus: virtio
               interfaces:
                 - name: default
                   masquerade: {}
             resources:
               requests:
                 memory: 1Gi
                 cpu: 1
           networks:
             - name: default
               pod: {}
           volumes:
             - name: rootdisk
               containerDisk:
                 image: quay.io/kubevirt/cirros-container-disk-demo:latest
             - name: datadisk
               persistentVolumeClaim:
                 claimName: test-block-vm-disk
     ```
     Apply: `kubectl apply -f test-block-vm.yaml`

  4. Wait for VM to start:
     ```bash
     kubectl get vmi test-block-vm -w
     # Wait for "Running" phase
     ```

  5. Verify VM running and on which node:
     ```bash
     kubectl get vmi test-block-vm -o wide
     ```

  Document the applied manifests in test/e2e/HARDWARE_VALIDATION.md.
  </action>
  <verify>
  - PVC shows Bound status
  - VirtualMachineInstance shows Running phase
  - VM scheduled on a worker node
  - No errors in CSI driver logs: `kubectl -n rds-csi-system logs deployment/rds-csi-controller`
  </verify>
  <done>Block PVC bound and KubeVirt VM running with RDS block volume</done>
</task>

<task type="auto">
  <name>Task 3: Validate I/O operations on block volume</name>
  <files>test/e2e/HARDWARE_VALIDATION.md (to document results)</files>
  <action>
  Access VM console and test block device I/O.

  1. Access VM console:
     ```bash
     virtctl console test-block-vm
     # Login: cirros / gocubsgo (default cirros credentials)
     ```

  2. Find the block device (should be /dev/vdb for datadisk):
     ```bash
     lsblk
     # Look for vdb (5G)
     ```

  3. Write data pattern for integrity check:
     ```bash
     # Create a filesystem (if first time)
     sudo mkfs.ext4 /dev/vdb

     # Mount and write test data
     sudo mkdir -p /mnt/data
     sudo mount /dev/vdb /mnt/data

     # Create test file with known pattern
     sudo dd if=/dev/urandom of=/mnt/data/testfile bs=1M count=100
     sudo md5sum /mnt/data/testfile | sudo tee /mnt/data/checksum.txt

     # Verify checksum was saved
     cat /mnt/data/checksum.txt

     # Sync to ensure data is flushed
     sync
     ```

  4. Record checksum for post-migration validation.

  5. Exit console (Ctrl+] or type 'exit').

  Document I/O test results in test/e2e/HARDWARE_VALIDATION.md.
  </action>
  <verify>
  - Block device visible in VM (lsblk shows /dev/vdb)
  - Filesystem creation successful (mkfs.ext4)
  - Test data written (100MB file)
  - Checksum recorded for validation
  </verify>
  <done>I/O operations validated on block volume</done>
</task>

<task type="auto">
  <name>Task 4: Execute live migration and validate</name>
  <files>test/e2e/HARDWARE_VALIDATION.md (to document results)</files>
  <action>
  Perform KubeVirt live migration and validate data integrity.

  1. Record current node before migration:
     ```bash
     kubectl get vmi test-block-vm -o jsonpath='{.status.nodeName}'
     # Save this for comparison
     ```

  2. Initiate live migration:
     ```bash
     # Create migration request
     kubectl apply -f - <<EOF
     apiVersion: kubevirt.io/v1
     kind: VirtualMachineInstanceMigration
     metadata:
       name: test-block-vm-migration-1
     spec:
       vmiName: test-block-vm
     EOF
     ```

  3. Watch migration progress:
     ```bash
     kubectl get vmim test-block-vm-migration-1 -w
     # Wait for "Succeeded" phase
     ```

  4. Verify VM on new node:
     ```bash
     kubectl get vmi test-block-vm -o jsonpath='{.status.nodeName}'
     # Should be different from step 1
     ```

  5. Check migration metrics from controller:
     ```bash
     # Port-forward to metrics endpoint
     kubectl -n rds-csi-system port-forward deployment/rds-csi-controller 9809:9809 &
     PF_PID=$!
     sleep 2

     # Fetch metrics
     curl -s http://localhost:9809/metrics | grep -E 'migrations_total|migration_duration'

     # Clean up port-forward
     kill $PF_PID
     ```

  6. Access VM console and verify data integrity:
     ```bash
     virtctl console test-block-vm
     # Login

     # Mount and verify checksum
     sudo mount /dev/vdb /mnt/data
     md5sum /mnt/data/testfile
     cat /mnt/data/checksum.txt
     # Checksums must match!

     exit
     ```

  Document migration results, node change, metrics, and data integrity in test/e2e/HARDWARE_VALIDATION.md.
  </action>
  <verify>
  - Migration completed successfully (Succeeded phase)
  - VM running on different node than before migration
  - Migration metrics emitted (migrations_total, migration_duration_seconds)
  - Checksum matches after migration (no data corruption)
  </verify>
  <done>Live migration completed with data integrity preserved</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
  Hardware validation complete:
  - Driver deployed to metal cluster
  - KubeVirt VM booted with RDS block volume
  - I/O operations validated on block device
  - Live migration executed and metrics captured
  - Data integrity confirmed after migration
  </what-built>
  <how-to-verify>
  Review the documented results in test/e2e/HARDWARE_VALIDATION.md:

  1. Confirm PVC bound and VM running
  2. Verify I/O test passed (checksum recorded)
  3. Confirm migration succeeded (node changed)
  4. Verify migration metrics were emitted
  5. Confirm data integrity (checksums match)

  If all criteria pass, v0.6.0 Block Volume Support is validated.

  If any issues encountered:
  - Note specific failures
  - Capture relevant logs
  - Determine if blockers or documentation items
  </how-to-verify>
  <resume-signal>Type "validated" if all tests passed, or describe any issues found</resume-signal>
</task>

<task type="auto">
  <name>Task 6: Cleanup and document final results</name>
  <files>test/e2e/HARDWARE_VALIDATION.md</files>
  <action>
  Clean up test resources and finalize documentation.

  1. Delete test resources:
     ```bash
     kubectl delete vm test-block-vm
     kubectl delete pvc test-block-vm-disk
     kubectl delete vmim test-block-vm-migration-1
     ```

  2. Verify cleanup:
     ```bash
     kubectl get pvc,vm,vmi,vmim | grep test-block
     # Should return nothing
     ```

  3. Finalize test/e2e/HARDWARE_VALIDATION.md with:
     - Validation date
     - Driver version tested
     - All test results (pass/fail)
     - Any issues encountered and resolutions
     - Recommendations or notes for operators

  4. Create the documentation file structure:
     ```markdown
     # Hardware Validation Results

     ## Summary
     - Date: YYYY-MM-DD
     - Driver version: vX.Y.Z
     - Cluster: metal
     - Result: PASS/FAIL

     ## Tests Performed

     ### VAL-01: VM Boot with Block Volume
     - PVC: test-block-vm-disk (5Gi, RWX, Block)
     - VM: test-block-vm (cirros + data disk)
     - Result: PASS
     - Notes: ...

     ### VAL-02: Live Migration
     - Source node: nodeA
     - Target node: nodeB
     - Migration duration: Xs
     - Result: PASS
     - Notes: ...

     ### VAL-03: Migration Metrics
     - migrations_total: observed
     - migration_duration_seconds: observed
     - Result: PASS

     ### Data Integrity
     - Pre-migration checksum: abc123...
     - Post-migration checksum: abc123...
     - Result: MATCH/MISMATCH

     ## Issues Encountered
     (None or list)

     ## Recommendations
     (None or list)
     ```
  </action>
  <verify>
  - Test resources cleaned up
  - HARDWARE_VALIDATION.md created with complete results
  - All VAL-XX requirements verified
  </verify>
  <done>Cleanup complete and validation documented</done>
</task>

</tasks>

<verification>
All Phase 13 success criteria validated:
1. KubeVirt VM boots successfully with RDS block volume on metal cluster
2. VM can read and write data to block volume (I/O validation)
3. KubeVirt live migration completes end-to-end (VM moves between nodes)
4. Migration metrics (migrations_total, migration_duration_seconds) are emitted correctly
5. No data corruption detected after migration (checksum validation)
</verification>

<success_criteria>
- test/e2e/HARDWARE_VALIDATION.md exists with complete results
- All VAL-01, VAL-02, VAL-03 requirements satisfied
- v0.6.0 Block Volume Support milestone validated for release
</success_criteria>

<output>
After completion, create `.planning/phases/13-hardware-validation/13-01-SUMMARY.md`
</output>
