---
phase: 27-documentation-a-hardware-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/HARDWARE_VALIDATION.md
autonomous: true

must_haves:
  truths:
    - "Operator can follow step-by-step instructions to validate basic volume lifecycle on real RDS hardware"
    - "Operator can validate NVMe/TCP connection behavior including reconnection after disconnect"
    - "Operator can validate volume expansion on real hardware"
    - "Operator can validate attachment reconciliation recovery scenarios"
    - "Every test case has cleanup steps that work even if the test fails mid-way"
    - "Timing expectations are documented (volume creation 10-30s, NVMe connect 2-5s, etc.)"
  artifacts:
    - path: "docs/HARDWARE_VALIDATION.md"
      provides: "Complete hardware validation guide with test cases"
      min_lines: 400
      contains: "TC-01"
  key_links:
    - from: "docs/HARDWARE_VALIDATION.md"
      to: "deploy/kubernetes/"
      via: "kubectl commands referencing deployed driver"
      pattern: "kubectl.*rds-csi"
    - from: "docs/HARDWARE_VALIDATION.md"
      to: "README.md"
      via: "cross-reference link"
      pattern: "HARDWARE_VALIDATION"
---

<objective>
Create comprehensive hardware validation guide (HARDWARE_VALIDATION.md) with step-by-step test scenarios for validating the RDS CSI driver against real production hardware.

Purpose: The user has v0.9.0 deployed on their production cluster RIGHT NOW and needs documented procedures to validate driver behavior against real RDS hardware. This is time-sensitive -- cluster access is available now and hardware validation procedures need to be documented while they can be verified against reality.

Output: docs/HARDWARE_VALIDATION.md with 6+ test cases covering volume lifecycle, NVMe/TCP, expansion, failure recovery, and attachment reconciliation.
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@docs/TESTING.md
@docs/architecture.md
@test/integration/hardware_integration_test.go
@README.md
@CLAUDE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create HARDWARE_VALIDATION.md with complete test scenarios</name>
  <files>docs/HARDWARE_VALIDATION.md</files>
  <action>
Create docs/HARDWARE_VALIDATION.md following the structure from 27-RESEARCH.md Pattern 1 (Hardware Validation Guide Structure). The guide must be executable by an operator with cluster access.

**Document Structure:**

1. **Header and Purpose** - What this document is for, when to use it (initial deployment, after upgrades, troubleshooting)

2. **Prerequisites Section:**
   - RDS accessible (management IP: 10.42.241.3 for SSH, storage IP: 10.42.68.1 for NVMe/TCP)
   - Kubernetes cluster with RDS CSI driver v0.9.0+ deployed
   - SSH access to RDS (`ssh admin@10.42.241.3`)
   - SSH access to worker nodes for NVMe verification
   - At least 20GB free space on RDS for test volumes
   - `nvme-cli` installed on worker nodes
   - `kubectl` access with cluster-admin privileges

3. **Environment Validation (Pre-flight checks):**
   - Verify controller pod running: `kubectl get pods -n kube-system -l app=rds-csi-controller`
   - Verify node plugin pods: `kubectl get pods -n kube-system -l app=rds-csi-node`
   - Verify StorageClass exists: `kubectl get sc rds-nvme-tcp`
   - Verify SSH connectivity to RDS: `ssh admin@10.42.241.3 '/system/resource/print'`
   - Verify RDS storage capacity: `ssh admin@10.42.241.3 '/disk print brief'`

4. **Test Cases** (each with Objective, Prerequisites, Estimated Time, Steps with expected output, Cleanup, Success Criteria, Troubleshooting):

   **TC-01: Basic Volume Lifecycle** (~5 min)
   - Create PVC (5Gi, rds-nvme-tcp StorageClass)
   - Wait for binding (expect Bound within 30s)
   - Verify volume on RDS via SSH (`/disk print detail where slot~"pvc-"`)
   - Create pod using volume (nginx:alpine)
   - Verify mount inside pod (`df -h /data`)
   - Write test data, read it back
   - Delete pod, delete PVC
   - Verify volume deleted on RDS
   - Use inline YAML (kubectl apply -f - <<EOF)

   **TC-02: NVMe/TCP Connection Validation** (~5 min)
   - Create PVC and pod (same as TC-01 setup)
   - SSH to worker node, verify NVMe connection: `nvme list`, `nvme list-subsys`
   - Verify NQN matches expected format: `nqn.2000-02.com.mikrotik:pvc-<uuid>`
   - Check NVMe/TCP transport: `cat /sys/class/nvme/nvme*/transport`
   - Verify block device exists: `lsblk | grep nvme`
   - Check reconnection parameters: `cat /sys/class/nvme/nvme*/ctrl_loss_tmo`
   - Cleanup: delete pod, delete PVC

   **TC-03: Volume Expansion** (~5 min)
   - Create 5Gi PVC, create pod, verify mount
   - Patch PVC to 10Gi: `kubectl patch pvc ... -p '{"spec":{"resources":{"requests":{"storage":"10Gi"}}}}'`
   - Wait for expansion (check events on PVC)
   - Verify new size inside pod: `df -h /data`
   - Verify new size on RDS: `ssh admin@10.42.241.3 '/disk print detail where slot=<vol-id>'`
   - Cleanup

   **TC-04: Block Volume for KubeVirt** (~10 min, requires KubeVirt)
   - Create PVC with volumeMode: Block
   - Create pod mounting as block device (volumeDevices)
   - Verify block device in pod: `ls -la /dev/xvda` (or similar)
   - Write data via dd: `dd if=/dev/zero of=/dev/xvda bs=1M count=10`
   - Read back: `dd if=/dev/xvda of=/dev/null bs=1M count=10`
   - Cleanup
   - Note: Mark as OPTIONAL if KubeVirt not installed

   **TC-05: Failure Recovery - Pod Deletion and Reattachment** (~5 min)
   - Create PVC and pod, write "persistence-test" to file
   - Delete pod (NOT PVC)
   - Recreate pod using same PVC
   - Verify data persists: read the file, confirm "persistence-test"
   - Cleanup

   **TC-06: Failure Recovery - RDS Connection Resilience** (~10 min, CAUTION)
   - Create PVC and pod, write data
   - Note: This test validates connection manager behavior. If RDS restart would affect production, SKIP this test and document it as manual-only.
   - Check controller logs for connection manager status
   - Verify probe endpoint reports healthy
   - Document expected behavior if RDS becomes temporarily unreachable (exponential backoff, auto-reconnect)
   - Cleanup

   **TC-07: Multi-Volume Concurrent Operations** (~5 min)
   - Create 3 PVCs simultaneously (5Gi each)
   - Create 3 pods using them
   - Verify all 3 bound and mounted
   - Write unique data to each
   - Delete all pods and PVCs
   - Verify all cleaned up on RDS

5. **Performance Baselines Section:**
   - Expected volume creation time: 10-30s
   - Expected NVMe connect time: 2-5s
   - Expected volume deletion time: 5-15s
   - Expected expansion time: 5-20s
   - I/O benchmarking command (fio) for operator to run if desired (not automated)

6. **Troubleshooting Decision Tree:**
   - "PVC stuck Pending" -> check controller logs -> check SSH -> check capacity
   - "Pod stuck ContainerCreating" -> check node logs -> check NVMe/TCP -> check network
   - "Volume not deleted on RDS" -> check controller logs -> check SSH connectivity -> manual cleanup command
   - "Expansion not reflecting" -> check PVC events -> check controller logs -> check filesystem resize

7. **Results Template:**
   - Markdown table for operator to fill in results
   - Columns: Test Case | Status (Pass/Fail) | Duration | Notes
   - Include space for recording actual timings vs expected

**Style Notes:**
- Use exact kubectl/ssh commands, not pseudocode
- Include expected output samples for every step
- Provide "If stuck" guidance after each waiting step
- Use `test-hw-` prefix for all test resource names (easy cleanup with `kubectl delete all -l test=hardware-validation`)
- Add labels to all test resources: `test: hardware-validation`
- Every test case must have Cleanup section that works even if test failed mid-way
- Include version info: "Written for RDS CSI Driver v0.9.0, RouterOS 7.16+"
  </action>
  <verify>
Run: `wc -l docs/HARDWARE_VALIDATION.md` - should be 400+ lines.
Run: `grep -c "^### TC-" docs/HARDWARE_VALIDATION.md` - should be 7 (test cases).
Run: `grep -c "Cleanup" docs/HARDWARE_VALIDATION.md` - should be at least 7 (one per test case).
Run: `grep -c "kubectl" docs/HARDWARE_VALIDATION.md` - should be 30+ (real commands).
Run: `grep "Expected:" docs/HARDWARE_VALIDATION.md | head -5` - should show expected output guidance.
  </verify>
  <done>
HARDWARE_VALIDATION.md exists in docs/ with 7 test cases (TC-01 through TC-07), each having prerequisites, steps with expected output, cleanup procedures, and success criteria. Troubleshooting decision tree and performance baselines are included. All commands use real kubectl/ssh syntax targeting the production cluster IPs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add HARDWARE_VALIDATION.md to README documentation links</name>
  <files>README.md</files>
  <action>
In README.md, find the Documentation section (around line 246) and add a link to the new hardware validation guide.

Add this entry to the documentation list, positioned first (most relevant for operators):
```
- **[Hardware Validation Guide](docs/HARDWARE_VALIDATION.md)** - Step-by-step test procedures for production RDS hardware
```

Place it before the existing "Kubernetes Setup Guide" entry.

Do NOT change any other part of README.md.
  </action>
  <verify>
Run: `grep "HARDWARE_VALIDATION" README.md` - should show the link.
Run: `grep -A 8 "## Documentation" README.md` - should show hardware validation as first doc link.
  </verify>
  <done>
README.md Documentation section includes link to HARDWARE_VALIDATION.md as the first documentation entry.
  </done>
</task>

</tasks>

<verification>
- docs/HARDWARE_VALIDATION.md exists and is comprehensive (400+ lines, 7 test cases)
- Every test case has: Objective, Prerequisites, Estimated Time, Steps (with exact commands and expected output), Cleanup, Success Criteria, Troubleshooting
- All commands reference real cluster IPs (10.42.241.3 for SSH, 10.42.68.1 for NVMe/TCP)
- Cleanup procedures work even if test fails mid-way (idempotent delete commands)
- Performance baselines section with expected timings
- Troubleshooting decision tree for common failure modes
- README.md links to the new document
</verification>

<success_criteria>
An operator with kubectl access and SSH access to RDS can follow the guide from start to finish, executing every test case without needing to look up additional commands or ask questions. Every step has expected output so the operator knows if something went wrong.
</success_criteria>

<output>
After completion, create `.planning/phases/27-documentation-a-hardware-validation/27-01-SUMMARY.md`
</output>
