---
phase: 32-resilience-regression-tests
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/HARDWARE_VALIDATION.md
  - docs/TESTING.md
autonomous: false

must_haves:
  truths:
    - "A documented TC-09 test case provides exact steps to validate NVMe reconnect after network interruption on real hardware"
    - "A documented TC-10 test case provides exact steps to validate RDS restart volume preservation on real hardware"
    - "A documented TC-11 test case provides exact steps to validate node failure VolumeAttachment cleanup on real hardware"
    - "The testing guide references the new resilience regression tests"
  artifacts:
    - path: "docs/HARDWARE_VALIDATION.md"
      provides: "TC-09, TC-10, TC-11 resilience test cases"
      contains: "TC-09"
    - path: "docs/TESTING.md"
      provides: "Updated testing guide referencing resilience E2E tests"
      contains: "resilience"
  key_links:
    - from: "docs/HARDWARE_VALIDATION.md"
      to: "pkg/rds/connection_manager.go"
      via: "documents connection manager behavior during RDS restart"
      pattern: "connection.manager|exponential.backoff"
    - from: "docs/HARDWARE_VALIDATION.md"
      to: "pkg/attachment/reconciler.go"
      via: "documents stale attachment cleanup during node failure"
      pattern: "stale.*attachment|VolumeAttachment"
---

<objective>
Document hardware validation test cases for the three resilience scenarios (NVMe reconnect, RDS restart, node failure) and update the testing guide to reference the new automated resilience tests.

Purpose: The automated E2E tests (Plan 01) validate logic with mocks, but real resilience must be confirmed on hardware. These documented test cases provide step-by-step procedures for manual hardware validation. They also serve as a regression checklist for future changes to reconnection logic.

Output: Three new test cases (TC-09, TC-10, TC-11) in HARDWARE_VALIDATION.md and an updated TESTING.md referencing the resilience test suite.
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@docs/HARDWARE_VALIDATION.md
@docs/TESTING.md
@pkg/rds/connection_manager.go
@pkg/attachment/reconciler.go
@pkg/nvme/config.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add resilience test cases TC-09, TC-10, TC-11 to HARDWARE_VALIDATION.md</name>
  <files>docs/HARDWARE_VALIDATION.md</files>
  <action>
Add three new test cases to `docs/HARDWARE_VALIDATION.md` after TC-08, following the exact format and structure of existing test cases (Objective, Estimated Time, Prerequisites, Steps with numbered sub-steps, Expected output blocks, Cleanup, Success Criteria, Troubleshooting).

**TC-09: NVMe Reconnect After Network Interruption**
Objective: Verify that after a network interruption causes NVMe/TCP connection drop, pods with mounted volumes recover and continue I/O without manual intervention.
Estimated Time: 15 minutes.
Prerequisites: TC-01 passed, worker node SSH access, ability to temporarily block NVMe/TCP traffic (iptables or similar).

Steps:
1. Create PVC (5Gi) and pod with continuous I/O writer (use `sh -c 'while true; do date >> /data/io-test.log; sleep 1; done'`).
2. Wait for pod Running, verify I/O is happening (`kubectl exec ... -- tail -5 /data/io-test.log`).
3. Note the last written timestamp.
4. SSH to the worker node and block NVMe/TCP traffic to RDS: `iptables -A OUTPUT -d 10.42.68.1 -p tcp --dport 4420 -j DROP`.
5. Wait 30 seconds. Check `dmesg | grep nvme` on worker node for connection timeout/error messages.
6. Verify pod I/O pauses (the write loop will stall because the filesystem becomes read-only or hangs).
7. Restore NVMe/TCP traffic: `iptables -D OUTPUT -d 10.42.68.1 -p tcp --dport 4420 -j DROP`.
8. Wait up to 60 seconds for kernel NVMe/TCP reconnection (controlled by ctrl_loss_tmo and reconnect_delay parameters).
9. Verify I/O resumes: `kubectl exec ... -- tail -10 /data/io-test.log` should show new timestamps after the gap.
10. Verify data written before the interruption is still present: timestamps before step 4 should still be in the log.

Cleanup: Delete pod and PVC. Remove any lingering iptables rules.

Success Criteria:
- NVMe/TCP connection recovers automatically after network restoration
- I/O resumes without pod restart
- Data written before interruption is preserved
- No data corruption (log file readable, timestamps in order)

Troubleshooting:
- If I/O doesn't resume after 60s: Check ctrl_loss_tmo setting (`cat /sys/class/nvme/nvmeX/ctrl_loss_tmo`). If set to a short value (e.g., 600), the controller may have timed out. The driver sets ctrl_loss_tmo=-1 for infinite retry.
- If pod is evicted: Check if the node marked the volume as unhealthy, check kubelet logs for volume condition changes.
- If filesystem goes read-only: May need to remount (this indicates ctrl_loss_tmo fired before reconnection).

CAUTION box: "This test temporarily interrupts NVMe/TCP traffic on a worker node. If other pods use RDS volumes on the same node, they will also be affected. Run during a maintenance window."

**TC-10: RDS Restart Volume Preservation**
Objective: Verify that after an RDS restart, volumes remain mounted and data written before the restart is readable after reconnection.
Estimated Time: 15-20 minutes.
Prerequisites: TC-01 passed, SSH access to RDS management IP (10.42.241.3), understanding that RDS restart will affect ALL NVMe/TCP connections on ALL nodes.

Steps:
1. Create PVC (5Gi) and pod, wait for Running.
2. Write test data with timestamp: `kubectl exec ... -- sh -c 'echo "pre-restart-$(date +%s)" > /data/restart-test.txt'`.
3. Verify data: `kubectl exec ... -- cat /data/restart-test.txt`. Save the output.
4. Check NVMe connection status on worker node: `ssh <node-ip> 'nvme list-subsys'`. Note the NQN and connection state (should be "live").
5. **Restart RDS:** `ssh admin@10.42.241.3 '/system/reboot'`. Confirm with `y` when prompted.
6. Wait for RDS to come back online (typically 60-120 seconds). Poll with `ssh admin@10.42.241.3 '/system/resource/print'` until it responds.
7. Monitor NVMe reconnection on worker node: `ssh <node-ip> 'dmesg | tail -30 | grep nvme'`. Look for reconnection messages.
8. Wait for NVMe/TCP to reconnect (up to 2-3 minutes with ctrl_loss_tmo=-1). Check: `ssh <node-ip> 'nvme list-subsys'` — connection state should return to "live".
9. Check controller logs for connection manager reconnection: `kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin --tail=50 | grep -i "reconnect"`.
10. Verify data integrity: `kubectl exec ... -- cat /data/restart-test.txt`. Compare with step 3 output — must match exactly.
11. Verify continued I/O: `kubectl exec ... -- sh -c 'echo "post-restart-$(date +%s)" >> /data/restart-test.txt && cat /data/restart-test.txt'`.

Cleanup: Delete pod and PVC.

Success Criteria:
- RDS restarts and comes back online within 2 minutes
- NVMe/TCP connections reconnect automatically (ctrl_loss_tmo=-1 infinite retry)
- Data written before restart is fully preserved and readable
- New I/O succeeds after reconnection
- Controller SSH connection manager reconnects via exponential backoff

Troubleshooting:
- If RDS doesn't come back: Check physical hardware, verify RDS management IP is still reachable.
- If NVMe doesn't reconnect: Check ctrl_loss_tmo. If the kernel gave up, a pod restart may be needed to trigger NodeStageVolume again.
- If data is lost: This would indicate a serious bug — file-backed volumes should survive RDS restart since they're on persistent Btrfs storage.
- If controller can't reconnect: Check SSH key is still valid, check if RDS changed IP after restart.

CAUTION box: "DANGER: Restarting RDS affects ALL NVMe/TCP connections on ALL cluster nodes. Only run this test during a maintenance window when no production workloads are running. Ensure you have physical access to the RDS hardware in case it doesn't come back online."

**TC-11: Node Failure Stale VolumeAttachment Cleanup**
Objective: Verify that after a node failure, stale VolumeAttachment objects are detected and removed, allowing volumes to be reattached on another node.
Estimated Time: 10-15 minutes.
Prerequisites: TC-01 passed, cluster with at least 2 worker nodes, kubectl admin access.

Steps:
1. Create PVC (5Gi) and pod, wait for Running.
2. Note which node the pod is scheduled on: `kubectl get pod <name> -o jsonpath='{.spec.nodeName}'`.
3. Write test data: `kubectl exec ... -- sh -c 'echo "node-failure-test-$(date +%s)" > /data/node-test.txt'`.
4. Verify VolumeAttachment exists: `kubectl get volumeattachment | grep <pv-name>`.
5. **Simulate node failure:** Cordon and drain the node, then delete the node object.
   - `kubectl cordon <node-name>`
   - `kubectl drain <node-name> --force --ignore-daemonsets --delete-emptydir-data --grace-period=0`
   - Wait for pod to be evicted.
   - `kubectl delete node <node-name>` (this simulates the node going away entirely).
6. Observe attachment reconciler behavior: `kubectl logs -n kube-system -l app=rds-csi-controller -c rds-csi-plugin --tail=50 | grep -i "stale\|reconcil\|attachment"`.
7. Wait for attachment reconciler to clear stale attachment (default: 5-minute interval with 30s grace period). Or trigger faster by restarting the controller pod.
8. Verify VolumeAttachment is cleared: `kubectl get volumeattachment | grep <pv-name>` should show no results or the attachment should be gone.
9. Re-join the node to the cluster (or verify another node can pick up the workload):
   - If using a Deployment: The pod will be rescheduled to another node automatically.
   - If using a bare Pod: Recreate the pod without nodeSelector to land on another node.
10. Verify the rescheduled pod starts and data is accessible: `kubectl exec <new-pod> -- cat /data/node-test.txt`.

Cleanup: Ensure the drained node is re-joined to the cluster. Delete test pod and PVC.

Success Criteria:
- VolumeAttachment for the deleted node is cleared by the reconciler
- Pod reschedules to another node successfully
- Volume reattaches to the new node without errors
- Data written on the original node is accessible on the new node

Troubleshooting:
- If VolumeAttachment persists: Check reconciler logs, verify reconciler interval and grace period configuration.
- If pod can't reschedule: Check if PV still has stale nodeAffinity, check VolumeAttachment status.
- If volume fails to attach on new node: The NVMe/TCP connection from the old node may not have been cleanly disconnected. Check for "already connected" errors and verify `nvme disconnect` ran on the old node (it may not have if the node crashed).

Also update the Results Template table in HARDWARE_VALIDATION.md to include TC-09, TC-10, TC-11 rows.

Update the "Last Updated" date at the bottom of the file to 2026-02-18.
  </action>
  <verify>Verify the file is well-formed markdown: `wc -l docs/HARDWARE_VALIDATION.md` shows increase in line count. Search for TC-09, TC-10, TC-11 headers to confirm they exist.</verify>
  <done>TC-09 (NVMe reconnect), TC-10 (RDS restart), TC-11 (node failure) are documented in HARDWARE_VALIDATION.md with complete step-by-step procedures, expected outputs, success criteria, and troubleshooting guidance.</done>
</task>

<task type="auto">
  <name>Task 2: Update TESTING.md with resilience test references</name>
  <files>docs/TESTING.md</files>
  <action>
Update `docs/TESTING.md` to reference the new resilience regression tests:

1. In the E2E Tests section ("What E2E tests cover"), add a bullet:
   - `resilience_test.go` - NVMe reconnect, RDS connection recovery, and node failure stale attachment cleanup

2. In the "Key test suites" list under E2E, add:
   - `resilience_test.go` - Resilience regression (RESIL-01, RESIL-02, RESIL-03) with mock error injection

3. Add a new subsection "### Resilience Regression Tests" after the "Hardware Integration Tests" section with:
   - Brief explanation that these tests validate driver resilience using mock infrastructure
   - Command to run: `go test -v ./test/e2e/... -ginkgo.v -ginkgo.focus="Resilience"`
   - What they cover: SSH connection recovery, RDS unavailability handling, stale attachment cleanup
   - Note that hardware-level validation (actual NVMe disconnect, RDS restart) requires manual testing per HARDWARE_VALIDATION.md TC-09/TC-10/TC-11

4. In the "CSI Capability Matrix" or a suitable location, note that resilience behavior is now regression-tested.
  </action>
  <verify>`grep -c "resilience" docs/TESTING.md` returns at least 3 matches.</verify>
  <done>TESTING.md references resilience_test.go in E2E section, has new Resilience Regression Tests subsection, and cross-references HARDWARE_VALIDATION.md for manual testing.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Review resilience test documentation</name>
  <what-built>Resilience regression test documentation (TC-09, TC-10, TC-11) in HARDWARE_VALIDATION.md and updated TESTING.md with resilience test references</what-built>
  <how-to-verify>
    1. Review TC-09 (NVMe reconnect) steps in docs/HARDWARE_VALIDATION.md — verify the iptables approach is appropriate for your network setup
    2. Review TC-10 (RDS restart) CAUTION box — confirm you're comfortable with the documented risk assessment
    3. Review TC-11 (node failure) steps — verify the drain/delete approach matches your cluster's node management
    4. Check that the Results Template table includes TC-09, TC-10, TC-11
    5. Check docs/TESTING.md mentions resilience regression tests
  </how-to-verify>
  <resume-signal>Type "approved" or describe changes needed</resume-signal>
</task>

</tasks>

<verification>
- docs/HARDWARE_VALIDATION.md contains TC-09, TC-10, TC-11 with complete procedures
- docs/TESTING.md references resilience tests
- All test case steps are actionable and include expected outputs
</verification>

<success_criteria>
- Three hardware validation test cases (TC-09, TC-10, TC-11) documented with step-by-step procedures
- Testing guide updated to reference automated resilience E2E tests
- Results template updated with new test case rows
- User has reviewed and approved the documentation
</success_criteria>

<output>
After completion, create `.planning/phases/32-resilience-regression-tests/32-02-SUMMARY.md`
</output>
