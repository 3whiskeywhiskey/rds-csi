---
phase: 04-observability
plan: 05
type: execute
wave: 3
depends_on: ["04-01", "04-02", "04-03", "04-04"]
files_modified:
  - pkg/observability/prometheus_test.go
  - pkg/driver/node_test.go
autonomous: true

must_haves:
  truths:
    - "Prometheus metrics package has unit tests verifying metric recording"
    - "NodeGetVolumeStats always returns VolumeCondition (verified by tests)"
    - "Tests verify correct metric labels and values"
  artifacts:
    - path: "pkg/observability/prometheus_test.go"
      provides: "Unit tests for Prometheus metrics"
      min_lines: 100
    - path: "pkg/driver/node_test.go"
      provides: "Tests for NodeGetVolumeStats VolumeCondition"
      contains: "VolumeCondition"
  key_links:
    - from: "pkg/observability/prometheus_test.go"
      to: "NewMetrics"
      via: "test setup"
      pattern: "NewMetrics\\(\\)"
---

<objective>
Add comprehensive unit tests for Prometheus metrics and verify NodeGetVolumeStats VolumeCondition behavior.

Purpose: Ensure observability features work correctly and regressions are caught.
Output: New prometheus_test.go with metric recording tests, updated node_test.go with VolumeCondition tests.
</objective>

<execution_context>
@/Users/whiskey/.claude/get-shit-done/workflows/execute-plan.md
@/Users/whiskey/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-observability/04-RESEARCH.md
@pkg/observability/prometheus.go
@pkg/driver/node.go
@pkg/driver/node_test.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Prometheus metrics unit tests</name>
  <files>pkg/observability/prometheus_test.go</files>
  <action>
Create comprehensive tests for the Prometheus metrics package:

```go
package observability

import (
    "errors"
    "io"
    "net/http"
    "net/http/httptest"
    "strings"
    "testing"
    "time"
)

func TestNewMetrics(t *testing.T) {
    m := NewMetrics()
    if m == nil {
        t.Fatal("NewMetrics returned nil")
    }
    if m.registry == nil {
        t.Error("registry is nil")
    }
}

func TestHandler(t *testing.T) {
    m := NewMetrics()
    handler := m.Handler()
    if handler == nil {
        t.Fatal("Handler returned nil")
    }

    // Test that handler serves metrics
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    if rec.Code != http.StatusOK {
        t.Errorf("expected status 200, got %d", rec.Code)
    }

    body, _ := io.ReadAll(rec.Body)
    if !strings.Contains(string(body), "rds_csi_") {
        t.Error("metrics response should contain rds_csi_ namespace")
    }
}

func TestRecordVolumeOp(t *testing.T) {
    m := NewMetrics()

    // Test success
    m.RecordVolumeOp("stage", nil, 100*time.Millisecond)

    // Test failure
    m.RecordVolumeOp("stage", errors.New("test error"), 50*time.Millisecond)

    // Verify via handler output
    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_volume_operations_total") {
        t.Error("expected volume_operations_total metric")
    }
}

func TestRecordNVMeConnect(t *testing.T) {
    m := NewMetrics()

    // Test success
    m.RecordNVMeConnect(nil, 500*time.Millisecond)

    // Test failure
    m.RecordNVMeConnect(errors.New("connection failed"), 0)

    // Verify active connections gauge increased on success
    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_nvme_connects_total") {
        t.Error("expected nvme_connects_total metric")
    }
    if !strings.Contains(body, "rds_csi_nvme_connections_active") {
        t.Error("expected nvme_connections_active metric")
    }
}

func TestRecordNVMeDisconnect(t *testing.T) {
    m := NewMetrics()

    // Connect first
    m.RecordNVMeConnect(nil, 100*time.Millisecond)

    // Disconnect
    m.RecordNVMeDisconnect()

    // Active connections should be 0
    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_nvme_connections_active 0") {
        t.Error("expected nvme_connections_active to be 0")
    }
}

func TestRecordMountOp(t *testing.T) {
    m := NewMetrics()

    m.RecordMountOp("mount", nil)
    m.RecordMountOp("unmount", errors.New("unmount failed"))

    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_mount_operations_total") {
        t.Error("expected mount_operations_total metric")
    }
}

func TestRecordStaleMountDetected(t *testing.T) {
    m := NewMetrics()

    m.RecordStaleMountDetected()
    m.RecordStaleMountDetected()

    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_stale_mounts_detected_total 2") {
        t.Error("expected stale_mounts_detected_total to be 2")
    }
}

func TestRecordStaleRecovery(t *testing.T) {
    m := NewMetrics()

    m.RecordStaleRecovery(nil)
    m.RecordStaleRecovery(errors.New("recovery failed"))

    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_stale_recoveries_total") {
        t.Error("expected stale_recoveries_total metric")
    }
}

func TestRecordOrphanCleaned(t *testing.T) {
    m := NewMetrics()

    m.RecordOrphanCleaned()

    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_orphans_cleaned_total 1") {
        t.Error("expected orphans_cleaned_total to be 1")
    }
}

func TestRecordEventPosted(t *testing.T) {
    m := NewMetrics()

    m.RecordEventPosted("MountFailure")
    m.RecordEventPosted("MountFailure")
    m.RecordEventPosted("ConnectionFailure")

    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()
    if !strings.Contains(body, "rds_csi_events_posted_total") {
        t.Error("expected events_posted_total metric")
    }
}

func TestMetricsNamespace(t *testing.T) {
    m := NewMetrics()

    // Record something
    m.RecordStaleMountDetected()

    handler := m.Handler()
    req := httptest.NewRequest("GET", "/metrics", nil)
    rec := httptest.NewRecorder()
    handler.ServeHTTP(rec, req)

    body := rec.Body.String()

    // All metrics should use rds_csi_ namespace
    lines := strings.Split(body, "\n")
    for _, line := range lines {
        // Skip comments and empty lines
        if strings.HasPrefix(line, "#") || line == "" {
            continue
        }
        // Skip standard Go runtime metrics
        if strings.HasPrefix(line, "go_") || strings.HasPrefix(line, "process_") {
            continue
        }
        if !strings.HasPrefix(line, "rds_csi_") {
            t.Errorf("metric line should start with rds_csi_: %s", line)
        }
    }
}
```
  </action>
  <verify>Run `go test ./pkg/observability/... -v` to verify all tests pass.</verify>
  <done>pkg/observability/prometheus_test.go exists with comprehensive tests for all recording methods.</done>
</task>

<task type="auto">
  <name>Task 2: Add VolumeCondition tests to node_test.go</name>
  <files>pkg/driver/node_test.go</files>
  <action>
Add or update tests in node_test.go to verify NodeGetVolumeStats always returns VolumeCondition.

If node_test.go doesn't exist, create it. If it exists, add tests for VolumeCondition behavior.

Test cases:
1. Healthy volume - VolumeCondition{Abnormal: false, Message: "Volume is healthy"}
2. Stale mount - VolumeCondition{Abnormal: true, Message: "Stale mount detected: ..."}
3. Stats retrieval failure still returns VolumeCondition

```go
func TestNodeGetVolumeStats_AlwaysReturnsVolumeCondition(t *testing.T) {
    tests := []struct {
        name            string
        setupMocks      func(*MockMounter, *MockStaleChecker)
        wantAbnormal    bool
        wantMsgContains string
    }{
        {
            name: "healthy volume returns healthy condition",
            setupMocks: func(m *MockMounter, sc *MockStaleChecker) {
                sc.stale = false
                m.stats = &mount.DeviceStats{
                    TotalBytes:     100 * 1024 * 1024,
                    UsedBytes:      50 * 1024 * 1024,
                    AvailableBytes: 50 * 1024 * 1024,
                    TotalInodes:    1000,
                    UsedInodes:     100,
                    AvailableInodes: 900,
                }
            },
            wantAbnormal:    false,
            wantMsgContains: "healthy",
        },
        {
            name: "stale mount returns abnormal condition",
            setupMocks: func(m *MockMounter, sc *MockStaleChecker) {
                sc.stale = true
                sc.reason = "device path mismatch"
            },
            wantAbnormal:    true,
            wantMsgContains: "Stale mount detected",
        },
    }

    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            // Setup mocks and node server
            // ...

            resp, err := ns.NodeGetVolumeStats(ctx, req)
            if err != nil {
                t.Fatalf("unexpected error: %v", err)
            }

            if resp.VolumeCondition == nil {
                t.Fatal("VolumeCondition should never be nil")
            }

            if resp.VolumeCondition.Abnormal != tt.wantAbnormal {
                t.Errorf("Abnormal = %v, want %v", resp.VolumeCondition.Abnormal, tt.wantAbnormal)
            }

            if !strings.Contains(resp.VolumeCondition.Message, tt.wantMsgContains) {
                t.Errorf("Message = %q, want to contain %q", resp.VolumeCondition.Message, tt.wantMsgContains)
            }
        })
    }
}
```

If mocks don't exist, create minimal mocks or use the existing mock patterns from the codebase.
  </action>
  <verify>Run `go test ./pkg/driver/... -v -run VolumeCondition` to verify VolumeCondition tests pass.</verify>
  <done>node_test.go has tests verifying NodeGetVolumeStats always returns VolumeCondition.</done>
</task>

</tasks>

<verification>
1. `go test ./pkg/observability/... -v` - all Prometheus tests pass
2. `go test ./pkg/driver/... -v` - all driver tests pass
3. `go test ./... -v` - full test suite passes
4. `go test -cover ./pkg/observability/...` - reasonable coverage (>70%)
</verification>

<success_criteria>
- prometheus_test.go has tests for NewMetrics, Handler, and all recording methods
- node_test.go has tests verifying VolumeCondition is always present
- All tests pass
- Tests verify both success and error paths
</success_criteria>

<output>
After completion, create `.planning/phases/04-observability/04-05-SUMMARY.md`
</output>
