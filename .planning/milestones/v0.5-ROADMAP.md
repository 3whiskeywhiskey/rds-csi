# Milestone v0.5: KubeVirt Hotplug Fix

**Status:** In Progress
**Phases:** 8-10
**Estimated Plans:** 2-3 per phase

## Overview

This milestone addresses a longstanding KubeVirt bug where concurrent volume hotplug operations cause VM pauses and I/O errors. The fix requires forking KubeVirt, setting up CI/CD for custom builds, implementing the fix in virt-controller, and contributing back upstream.

**Root Cause:** When hotplugging a new volume, KubeVirt recreates the attachment pod (hp-volume-*) with all volumes. The old pod is deleted before the new pod is ready, causing existing block devices to disappear mid-use.

**Fix Location:** `pkg/virt-controller/watch/vmi/volume-hotplug.go`

## Phases

### Phase 8: Fork and CI/CD Setup
**Goal:** Have automated builds of custom KubeVirt images we can deploy for testing
**Depends on:** None (can start immediately)
**Requirements:** KVFIX-01

**Success Criteria:**
1. KubeVirt fork exists in our GitHub organization
2. GitHub Actions workflow triggers on push to fix branch
3. Workflow builds virt-controller and virt-handler images
4. Images pushed to ghcr.io with predictable tags
5. Can deploy to test cluster using custom images

Plans:
- [ ] 08-01-PLAN.md — Fork repo, configure GitHub Actions workflow for image builds
- [ ] 08-02-PLAN.md — Test deployment with custom images

**Details:**
- Fork from kubevirt/kubevirt
- Use existing Bazel build system via `make bazel-build-images`
- Push to ghcr.io/<org>/virt-controller and ghcr.io/<org>/virt-handler
- Create simple deployment script to patch existing KubeVirt install

### Phase 9: Implement and Test Fix
**Goal:** Working fix with unit tests, validated in our environment
**Depends on:** Phase 8
**Requirements:** KVFIX-02, KVFIX-03, KVFIX-04
**Plans:** 3 plans

**Success Criteria:**
1. Documented code path from volume add request to pod deletion
2. Modified `cleanupAttachmentPods` waits for new pod readiness
3. All existing hotplug unit tests pass
4. New unit tests cover the fix logic
5. Manual validation on metal cluster confirms fix works
6. Volume removal still works correctly

Plans:
- [ ] 09-01-PLAN.md — Document code path and implement fix
- [ ] 09-02-PLAN.md — Unit tests for fix
- [ ] 09-03-PLAN.md — Manual validation on metal cluster

**Details:**
- Key files: `pkg/virt-controller/watch/vmi/volume-hotplug.go`
- Key functions: `handleHotplugVolumes`, `cleanupAttachmentPods`, `getActiveAndOldAttachmentPods`
- The bug is in line 46: `podVolumesMatchesReadyVolumes` returns false when new volume added
- This triggers new pod creation and old pod deletion without coordination
- Modify `cleanupAttachmentPods` to check VolumeStatus.Phase == VolumeReady for all volumes in new pod
- Unit tests verify state machine logic; manual testing confirms real-world fix

### Phase 10: Upstream Contribution
**Goal:** Fix contributed back to kubevirt/kubevirt
**Depends on:** Phase 9
**Requirements:** KVFIX-05

**Success Criteria:**
1. PR opened against kubevirt/kubevirt main branch
2. PR includes unit tests
3. PR passes upstream CI
4. Maintainer feedback addressed
5. (Stretch) PR merged

Plans:
- [ ] 10-01-PLAN.md — Prepare PR with tests and documentation
- [ ] 10-02-PLAN.md — Address review feedback

**Details:**
- Follow KubeVirt contribution guidelines
- Reference issues #6564, #9708, #16520
- Include detailed commit message explaining the fix
- Be prepared for multiple review cycles

---

## Milestone Summary

**Key Decisions:**
- TBD (will be captured during execution)

**Issues Targeted:**
- kubevirt/kubevirt#6564 — Add hotplug volume more than one will cause io error
- kubevirt/kubevirt#9708 — domain Paused(I/O Error) after hotplug the second volume
- kubevirt/kubevirt#16520 — hotplug mount resolution picks wrong parent mount
- rds-csi#12 — concurrent volume operations causing VM pauses

**Technical Debt:**
- Running custom KubeVirt build until upstream merges fix

---

*Milestone created: 2026-01-31*
*Motivation: Nested K3s clusters cannot safely use multi-volume VMs*
