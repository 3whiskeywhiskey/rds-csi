# v0.10.0 Feature Enhancements - Roadmap Archive

**Shipped:** 2026-02-06
**Duration:** 3 days (Feb 4-6, 2026)
**Commits:** 206 commits
**Plans:** 15 plans across 5 phases
**LOC:** 45,310 Go

## Milestone Goal

Add volume snapshots, comprehensive documentation, and Helm chart for easier deployment.

## Key Accomplishments

1. **Volume Snapshots**: Btrfs-based snapshots with restore workflow, external-snapshotter v8.2.0 integration, and full CSI spec compliance
2. **Hardware Validation Suite**: 7 executable test cases with step-by-step instructions for validating driver against production RDS
3. **Comprehensive Documentation**: Testing guide, capability analysis vs peer drivers, CI/CD integration, and troubleshooting decision trees
4. **Helm Chart Deployment**: One-command installation with configurable RDS parameters, multiple StorageClasses, and Prometheus monitoring
5. **RDS Health Monitoring**: Disk metrics via SSH polling with Prometheus GaugeFunc collectors (Phase 28.2)
6. **Production Observability**: Fixed rds_csi_nvme_connections_active metric to accurately reflect cluster state (Phase 28.1)

## Phases

### Phase 26: Volume Snapshots

**Goal**: Btrfs-based volume snapshots enable backup and restore workflows
**Depends on**: Phase 25.2 (all v0.9.0 work complete)
**Requirements**: SNAP-01, SNAP-02, SNAP-03, SNAP-04, SNAP-05, SNAP-06, SNAP-07, SNAP-08, SNAP-09, SNAP-10
**Success Criteria** (what must be TRUE):
  1. CreateSnapshot capability is advertised in controller service GetControllerCapabilities
  2. CreateSnapshot creates Btrfs snapshot via SSH RouterOS command and returns snapshot ID to Kubernetes
  3. Snapshot metadata (size, creation timestamp, source volume ID) is stored in VolumeSnapshotContent annotations
  4. DeleteSnapshot removes Btrfs snapshot from RDS and cleans up Kubernetes metadata
  5. ListSnapshots returns existing snapshots via RouterOS query for snapshot enumeration
  6. CreateVolume from snapshot (restore) creates new volume from Btrfs snapshot enabling backup restore workflow
  7. Snapshot operations pass CSI sanity snapshot tests validating spec compliance
  8. external-snapshotter sidecar v8.0+ is integrated in controller deployment manifest
  9. VolumeSnapshotClass supports StorageClass parameter for snapshot configuration
**Plans**: 6 plans
**Status**: ✅ Complete (2026-02-06)
**Verification**: 9/9 must-haves verified

Plans:
- [x] 26-01-PLAN.md -- Snapshot foundation (types, ID utils, interface extension, mock support)
- [x] 26-02-PLAN.md -- RDS SSH snapshot commands (Btrfs subvolume operations + unit tests)
- [x] 26-03-PLAN.md -- Controller RPCs: CreateSnapshot, DeleteSnapshot + capability registration
- [x] 26-04-PLAN.md -- Controller RPCs: ListSnapshots pagination + CreateVolume from snapshot restore
- [x] 26-05-PLAN.md -- Deployment manifests (RBAC, csi-snapshotter sidecar, VolumeSnapshotClass)
- [x] 26-06-PLAN.md -- Testing (CSI sanity snapshot config + controller unit tests)

**Key Deliverables:**
- 5 new SSH commands: CreateSnapshot, DeleteSnapshot, GetSnapshot, ListSnapshots, RestoreSnapshot
- 4 CSI controller RPCs: CreateSnapshot, DeleteSnapshot, ListSnapshots, createVolumeFromSnapshot
- 21 controller unit test cases + CSI sanity snapshot tests
- Full RBAC and deployment manifests for snapshot support
- external-snapshotter v8.2.0 sidecar integration

### Phase 27: Documentation & Hardware Validation

**Goal**: Comprehensive documentation for hardware validation, testing, capabilities, and troubleshooting -- enabling operators to validate v0.9.0 against production hardware
**Depends on**: Phase 25.2 (v0.9.0 complete; Phase 26 NOT required -- DOC-07 deferred until snapshots ship)
**Requirements**: DOC-01, DOC-02, DOC-03, DOC-04, DOC-05, DOC-06 (DOC-07 deferred to post-Phase 26)
**Success Criteria** (what must be TRUE):
  1. Manual test scenarios are documented in HARDWARE_VALIDATION.md with step-by-step instructions for production RDS testing
  2. Testing guide for contributors (TESTING.md) documents how to run unit tests, integration tests, E2E tests, and sanity tests
  3. CSI capability gap analysis vs peer drivers (AWS EBS CSI, Longhorn) is documented in CAPABILITIES.md
  4. Known limitations are documented in README.md (RouterOS version compatibility, NVMe device timing assumptions, dual-IP architecture requirements)
  5. CI/CD integration guide documents how to add new test jobs and interpret test results
  6. Troubleshooting guide in TESTING.md covers common test failures with solutions (mock-reality divergence, device timing, cleanup issues)
**Plans**: 3 plans
**Status**: ✅ Complete (2026-02-05)
**Verification**: 6/6 must-haves verified

Plans:
- [x] 27-01-PLAN.md -- Hardware validation guide (HARDWARE_VALIDATION.md) with 7 test cases
- [x] 27-02-PLAN.md -- Capabilities gap analysis (CAPABILITIES.md) + README.md known limitations
- [x] 27-03-PLAN.md -- Testing guide updates (TESTING.md) + CI/CD integration guide (ci-cd.md)

**Key Deliverables:**
- HARDWARE_VALIDATION.md: 1565 lines, 7 test cases (TC-01 through TC-07), 155 executable commands
- TESTING.md: 530 lines, all 4 test types with troubleshooting for each
- CAPABILITIES.md: 357 lines, peer driver comparison with transparent gap explanations
- ci-cd.md: 413 lines, test job templates and result interpretation
- README.md: Known limitations with RouterOS/timing/dual-IP requirements

### Phase 28.1: Fix rds_csi_nvme_connections_active Metric Accuracy (INSERTED)

**Goal**: Fix production observability bug where rds_csi_nvme_connections_active metric incorrectly reports 0 instead of actual connection count
**Depends on**: Phase 27 (documentation complete)
**GitHub Issue**: #19
**Requirements**: METRIC-01
**Success Criteria** (what must be TRUE):
  1. rds_csi_nvme_connections_active gauge reports actual count of active volumes with NVMe/TCP connections from attachment manager state (counts volumes, not per-node attachments during migration dual-attach)
  2. Metric value persists correctly across controller restarts (not derived from ephemeral counters)
  3. Metric accurately reflects VolumeAttachment count in cluster (validates via kubectl comparison)
  4. Unit tests verify metric updates on attach/detach operations
  5. Unit test validates metric accuracy after controller restart (TestNVMeConnectionsActive_SurvivesRestart simulates restart scenario)
**Plans**: 1 plan
**Status**: ✅ Complete (2026-02-06)
**Verification**: 5/5 must-haves verified

Plans:
- [x] 28.1-01-PLAN.md -- Replace counter-derived gauge with GaugeFunc querying AttachmentManager + unit tests

**Root Cause**: Metric derived from attach/detach counters (attach_total - detach_total) instead of querying attachment manager current state. Counters reset on restart while attachments persist.

**Impact**: Unreliable monitoring dashboards, alerting, and debugging. Production cluster shows 16 active volumes but metric reports 0.

### Phase 28.2: RDS Health & Performance Monitoring (INSERTED)

**Goal**: Implement RDS storage health monitoring via SSH polling of /disk monitor-traffic, exposing IOPS, throughput, latency, and queue depth as Prometheus metrics
**Depends on**: Phase 28.1 (metric accuracy fixed, GaugeFunc pattern established)
**Requirements**: MON-01, MON-02, MON-03
**Success Criteria** (what must be TRUE):
  1. RouterOS /disk monitor-traffic command capabilities documented (IOPS, throughput, latency, queue depth metrics available)
  2. SNMP monitoring capabilities researched (OIDs for disk health, available MIBs documented)
  3. RouterOS API REST/socket capabilities investigated (alternative to SSH polling for real-time metrics)
  4. Polling approach recommendations documented (frequency, performance impact, metric selection)
  5. Metric naming conventions defined (rds_disk_* namespace, labels for slot/operation type)
  6. Implementation complexity assessed (simple SSH polling vs API integration vs SNMP agent)
  7. Production impact analysis (CPU/memory overhead of polling, SSH connection limits)
**Plans**: 2 plans
**Status**: ✅ Complete (2026-02-06)
**Verification**: 7/7 must-haves verified

Plans:
- [x] 28.2-01-PLAN.md -- RDS disk metrics SSH command, parsing, types, mock, and unit tests
- [x] 28.2-02-PLAN.md -- Prometheus GaugeFunc metric registration, driver wiring, and monitoring design doc

**Discovery**: User found `/disk monitor-traffic <slot>` command exposes:
- read-ops-per-second, write-ops-per-second (IOPS)
- read-rate, write-rate (throughput in bps)
- read-time, write-time, wait-time (latency indicators)
- in-flight-ops (queue depth)
- active-time (disk utilization)

**Approach**: SSH polling with GaugeFunc collectors (research concluded SSH > SNMP > API for this use case).

### Phase 28: Helm Chart

**Goal**: Helm chart enables one-command deployment of the RDS CSI driver with configurable values for RDS connection, storage classes, monitoring, and all component settings
**Depends on**: Phase 28.2 (monitoring research complete for decision: include in Helm or defer)
**Requirements**: HELM-01, HELM-02, HELM-03, HELM-04, HELM-05
**Success Criteria** (what must be TRUE):
  1. Helm chart deploys controller and node plugin with configurable values
  2. Chart supports customization of RDS connection parameters (address, SSH port, NVMe port)
  3. Chart includes RBAC, ServiceAccount, and Secret reference management (user creates Secret, chart references it)
  4. Chart supports multiple storage classes with different configurations
  5. Chart documentation includes installation instructions and configuration examples
  6. Chart distributed via git repository (users install from local clone or deploy/helm/ directory)
**Plans**: 3 plans
**Status**: ✅ Complete (2026-02-06)
**Verification**: 6/6 must-haves verified

Plans:
- [x] 28-01-PLAN.md -- Chart skeleton (Chart.yaml, values.yaml, values.schema.json, _helpers.tpl)
- [x] 28-02-PLAN.md -- Core templates (controller Deployment, node DaemonSet, RBAC, CSIDriver, ServiceAccount)
- [x] 28-03-PLAN.md -- Feature templates (StorageClass, VolumeSnapshotClass, ServiceMonitor, NOTES.txt, README)

**Key Deliverables:**
- Chart.yaml: apiVersion v2, version 1.0.0, appVersion 0.10.0
- values.yaml: 328 lines, flat structure with comprehensive defaults
- values.schema.json: 211 lines, JSON Schema validation with required fields
- _helpers.tpl: 116 lines, 13 named template helpers
- Controller Deployment: 6 containers (driver + 5 sidecars)
- Node DaemonSet: 3 containers with privileged security
- RBAC: Complete rules for controller and node
- StorageClass: Conditional rendering with array iteration
- ServiceMonitor: Triple-conditional with CRD detection
- README.md: 625 lines documenting all configuration options
- NOTES.txt: 184 lines with post-install validation commands

## Statistics

**Timeline:**
- Start: 2026-02-04
- End: 2026-02-06
- Duration: 3 days

**Git Activity:**
- Commits: 206 total
- First commit: e9c9e1f (Feb 4, 2026)
- Last commit: [tagged as v0.10.0]

**Code Changes:**
- Go LOC: 45,310 lines
- Documentation: 3,890 lines (HARDWARE_VALIDATION, TESTING, CAPABILITIES, ci-cd, Helm README)
- Helm templates: 1,197 lines
- Test code: 519 lines (E2E snapshot tests)

**Quality Metrics:**
- Phase verifications: 5/5 passed
- Must-haves verified: 33/33 (100%)
- Requirements satisfied: 45/45 v0.10.0 requirements
- Test coverage: 68.6% (exceeds 65% threshold)

## Tech Debt Resolved

During milestone completion:
1. ✅ Fixed TESTING.md line 145 to reflect Phase 26 completion
2. ✅ Added TC-08 to HARDWARE_VALIDATION.md (290 lines) for snapshot operations
3. ✅ Created test/e2e/snapshot_test.go with 6 test scenarios (519 lines)

All E2E tests passing: 30/30 (100%)

## Requirements Traceability

All v0.10.0 requirements satisfied:

**Volume Snapshots (SNAP-01 through SNAP-10):** Phase 26
**Documentation (DOC-01 through DOC-06):** Phase 27
**Metric Accuracy (METRIC-01):** Phase 28.1
**RDS Monitoring (MON-01 through MON-03):** Phase 28.2
**Helm Chart (HELM-01 through HELM-05):** Phase 28

## Lessons Learned

1. **Decimal phase insertion**: Successfully used for urgent production fixes (28.1, 28.2) without disrupting planned roadmap
2. **Milestone audit**: Tech debt items identified and resolved via quick tasks before completion
3. **Documentation-first approach**: Phase 27 provided foundation for Helm chart documentation in Phase 28
4. **Research-then-implement pattern**: Phase 28.2 research informed Phase 28 monitoring integration
5. **E2E test completeness**: Snapshot E2E tests caught metadata parsing bugs before production

## Next Milestone

See `.planning/ROADMAP.md` for v0.11.0+ planning.

---

*Archived: 2026-02-06*
*Original location: `.planning/ROADMAP.md` (Phases 26-28)*
