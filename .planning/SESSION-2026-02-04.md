# Session State: 2026-02-04 - Block Volume Investigation

## What We Accomplished

### 1. Fixed Block Volume Implementation (Commits)
- `d33e09a` - Block volumes follow AWS EBS pattern (no staging directory)
- `e2303ce` - Mount storm pre-detection (activate DetectDuplicateMounts before mount)
- `68b3d92` - STATE.md documentation

### 2. Deployment Validated
- All 6 nodes running new driver: `68b3d92` (includes both fixes)
- All CSI pods healthy and ready
- r740xd recovered from mount storm after reboot

### 3. Mount Storm Root Cause Confirmed
- r640 and r740xd both experienced mount storms
- **431 MB unreclaimable kernel slab memory** (mount namespaces)
- Soft lockups in `do_move_mount` → `mntput_no_expire` (CPU stuck 26-52s)
- OOM killed CSI driver and other processes
- Multiple findmnt processes stuck (mount storm detection couldn't complete)
- Triggered by old buggy driver code creating losetup retry storms

## Current Problem: Block Volumes Still Failing

### Symptoms
Test pod `test-block-clean-pod` stuck in ContainerCreating with error:
```
MapVolume.MapBlockVolume failed ... blkUtil.AttachFileDevice failed
globalMapPath:/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-.../dev
makeLoopDevice failed ... losetup -f ... failed: exit status 1
```

### What's Working
✅ CSI driver operations succeed:
- NodeStageVolume: Connects NVMe device (`nvme7n1`), no staging directory created
- NodePublishVolume: Finds device by NQN, creates target file, bind mounts
- Device connected on host (confirmed in kernel logs)
- No errors in CSI driver logs
- Operations are idempotent (kubelet retried 5+ times, all succeeded)

### What's Failing
❌ Kubelet MapBlockVolume/AttachFileDevice:
- Tries to run losetup on globalMapPath
- Fails with exit status 1
- This happens AFTER our CSI operations succeed

### Path Mismatch Discovery
We create bind mount at:
```
/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-.../pod-uid
```

Kubelet looks for device at:
```
/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-.../dev/pod-uid
```

These are **different paths**: `publish/` vs `dev/`

### The Architectural Issue

Kubelet is doing BOTH operations:
1. **CSI path**: Calls NodeStageVolume + NodePublishVolume (we handle this correctly)
2. **Legacy path**: Also calls MapBlockVolume/AttachFileDevice on globalMapPath (fails with losetup)

We advertise `STAGE_UNSTAGE_VOLUME` capability, which tells kubelet we handle staging. But kubelet STILL tries its own device mapping via losetup on the globalMapPath.

## Why This is Confusing

### AWS EBS Reference
- AWS EBS CSI driver: NodeStageVolume for block volumes returns success immediately (does nothing)
- They rely on controller AttachVolume API to make device appear on node
- Device is already at `/dev/xvdX` before NodeStageVolume is called
- NodePublishVolume binds `/dev/xvdX` to target path

### Our Implementation
- We DO need NodeStageVolume (to connect NVMe-oF device)
- Device appears as `/dev/nvmeXnY` after our NodeStageVolume
- NodePublishVolume finds device by NQN and bind mounts to target path
- But globalMapPath is never populated

## Theories for Next Steps

### Theory 1: Create Symlink at globalMapPath
Maybe NodeStageVolume should create a symlink at the globalMapPath location:
```go
// In NodeStageVolume for block volumes:
globalMapPath := deriveGlobalMapPath(stagingPath) // Need to figure out this path
symlinkPath := filepath.Join(globalMapPath, "device")
os.Symlink(devicePath, symlinkPath)
```

Problem: We don't receive globalMapPath in NodeStageVolume request

### Theory 2: Don't Advertise STAGE_UNSTAGE_VOLUME
Remove the capability so kubelet doesn't expect us to do staging.

Problems:
- Can't advertise capabilities per-volume-mode (block vs filesystem)
- Filesystem volumes NEED staging (for mount operations)
- Would break filesystem volumes

### Theory 3: This IS a K3s 1.34 Bug
Maybe kubelet shouldn't be calling MapBlockVolume for CSI volumes that advertise STAGE_UNSTAGE_VOLUME?

Evidence against: The code flow looks intentional in Kubernetes source

### Theory 4: Missing Understanding of CSI Block Volume Spec
Maybe there's a specific CSI spec requirement for what to create at globalMapPath that we're missing?

Need to research:
- What exactly is globalMapPath for CSI block volumes?
- Should NodeStageVolume create something there?
- Should NodePublishVolume create something there?
- How do other CSI drivers (besides AWS EBS) handle this?

## Test Environment

### Test Resources
- PVC: `test-block-clean` (1Gi, Block mode, Bound)
- Pod: `test-block-clean-pod` (on r740xd, ContainerCreating)
- Volume ID: `pvc-76031559-5785-48d1-a7a9-55c8aba190f9`
- Device: `/dev/nvme7n1` (connected successfully)

### Cluster State
- All CSI pods running new code (commit 68b3d92)
- No mount storms currently active
- Old test resources from previous attempts cleaned up

## Files Modified

### Code Changes
- `pkg/driver/node.go`:
  - NodeStageVolume: removed staging directory/symlink for block volumes
  - NodePublishVolume: find device by NQN instead of reading symlink
  - NodeUnstageVolume: detect block volumes by mount status
- `pkg/mount/mount.go`:
  - Added pre-mount storm detection (calls DetectDuplicateMounts before mount syscall)
  - Added context import

### Documentation
- `.planning/STATE.md`: Updated with decisions and current status

## Commands for Next Session

### Check Current Status
```bash
kubectl get pod test-block-clean-pod -n default -o wide
kubectl describe pod test-block-clean-pod -n default | tail -50
kubectl logs -n rds-csi rds-csi-node-68xdm -c rds-csi-driver --tail=20
```

### Cleanup Test Resources
```bash
kubectl delete pod test-block-clean-pod -n default --force --grace-period=0
kubectl delete pvc test-block-clean -n default
```

### Investigate globalMapPath
```bash
# On r740xd node, check what exists at these paths:
ssh r740xd 'ls -la /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/'
ssh r740xd 'find /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/ -name "*76031559*"'
```

## Key Questions to Research

1. **What is globalMapPath?** Where does it come from in the CSI block volume lifecycle?

2. **Who creates it?** Kubelet or CSI driver?

3. **What should be at globalMapPath?** Symlink? Device node? Bind mount?

4. **When should it be created?** NodeStageVolume? NodePublishVolume? Kubelet?

5. **Why does AWS EBS work?** Their device is at `/dev/xvdX` via controller AttachVolume. Do they create anything at globalMapPath?

6. **Do we need MapDevice operation?** Is there a CSI operation we're supposed to implement but haven't?

7. **Is STAGE_UNSTAGE_VOLUME the wrong capability for block volumes?** Should we advertise something different?

## Research Resources

- Kubernetes CSI block volume source: `pkg/volume/csi/csi_block.go`
- Kubernetes volume path handler: `pkg/volume/util/volumepathhandler/volume_path_handler_linux.go`
- CSI spec: https://kubernetes-csi.github.io/docs/raw-block.html
- AWS EBS CSI driver: https://github.com/kubernetes-sigs/aws-ebs-csi-driver
- GitHub issue about CSI block volumes: #68424, #73773

## Next Actions

1. **Research globalMapPath architecture** - understand the Kubernetes code that creates/uses it
2. **Check other CSI drivers** - how do drivers like Ceph RBD, iSCSI handle block volumes with STAGE_UNSTAGE_VOLUME?
3. **Try Theory 1** - attempt to create symlink at globalMapPath in NodeStageVolume
4. **Debug on node** - ssh to r740xd and examine actual filesystem state of kubelet paths
5. **Test without STAGE_UNSTAGE_VOLUME** - temporarily remove capability to see if that makes block volumes work (will break filesystem volumes)

## Open Questions

- Why does kubelet call BOTH CSI operations AND MapBlockVolume?
- Is this behavior specific to K3s or all Kubernetes 1.34?
- Should we file a Kubernetes bug report or is this expected behavior?
- Can we make STAGE_UNSTAGE_VOLUME conditional on volume mode?
