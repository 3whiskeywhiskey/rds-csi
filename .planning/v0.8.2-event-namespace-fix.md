# v0.8.2 Release: Event Namespace Fix

**Created:** 2026-02-04
**Issue:** Kubernetes event creation failing with empty namespace error
**Root Cause:** RecordEvent function not setting Event.ObjectMeta.Namespace
**Fix:** Copy namespace from InvolvedObject to Event ObjectMeta

---

## Problem Summary

After deploying v0.8.1 to hardware cluster, driver logs show continuous event creation failures:

```
E0204 23:44:14.623889 event.go:289] Unable to write event: '&v1.Event{
  TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""},
  ObjectMeta:v1.ObjectMeta{
    Name:"homelab-node-3-var.18912fb4bd09de4b",
    GenerateName:"",
    Namespace:"",  // <-- EMPTY NAMESPACE
    ...
  },
  InvolvedObject:v1.ObjectReference{
    Kind:"PersistentVolumeClaim",
    Namespace:"homelab-cluster",  // <-- HAS NAMESPACE
    Name:"homelab-node-3-var",
    ...
  },
  ...
}': 'an empty namespace may not be set during creation'
```

### Impact

**Functional:**
- âœ… Driver functionality NOT affected (events are optional)
- âœ… Volume operations continue working
- âœ… No data loss or availability issues

**Observability:**
- âŒ Events not recorded to Kubernetes API
- âŒ Operators don't see volume lifecycle events (VolumeDetached, MigrationCompleted, etc.)
- âŒ Logs polluted with retry errors
- âŒ `kubectl describe pvc` doesn't show driver events

**Affected Events:**
- `VolumeDetached` - Node plugin volume cleanup
- `MigrationCompleted` - KubeVirt live migration completion
- `VolumeAttached` - Volume attachment notifications
- All CSI driver events to PVCs

---

## Root Cause Analysis

### Finding 1: Kubernetes API Validation Change

**Recent Kubernetes versions** (1.26+) enforce stricter event validation:
- Events MUST have `ObjectMeta.Namespace` set
- Previously, Kubernetes would default it from `InvolvedObject.Namespace`
- Now, empty namespace explicitly rejected with error

### Finding 2: Driver's RecordEvent Function

**Current Implementation** (`pkg/driver/event.go`):

```go
func (d *Driver) RecordEvent(pvcNamespace, pvcName, eventType, reason, message string) {
    ref := &v1.ObjectReference{
        Kind:      "PersistentVolumeClaim",
        Namespace: pvcNamespace,  // âœ… Set correctly
        Name:      pvcName,
        // ...
    }

    event := &v1.Event{
        ObjectMeta: metav1.ObjectMeta{
            Name:      eventName,
            // Namespace: ???  // âŒ NOT SET
        },
        InvolvedObject: *ref,
        // ...
    }

    _, err := d.clientset.CoreV1().Events(???).Create(ctx, event, metav1.CreateOptions{})
    //                                        ^^^ Which namespace?
}
```

**Problem:**
- `Event.ObjectMeta.Namespace` is never set
- `Events(namespace)` client call likely uses empty string or wrong namespace
- Kubernetes rejects events with empty `ObjectMeta.Namespace`

### Finding 3: Affects All Event Types

All calls to `RecordEvent` are affected:
- `pkg/driver/node.go` - VolumeDetached, VolumeAttached
- `pkg/driver/controller.go` - Security events, errors
- `pkg/driver/migration.go` - MigrationCompleted
- `pkg/driver/reconciler.go` - Reconciliation events

---

## The Fix

### Primary: Set Event.ObjectMeta.Namespace

**Modified:** `pkg/driver/event.go`

**Changes:**

```go
func (d *Driver) RecordEvent(pvcNamespace, pvcName, eventType, reason, message string) {
    ref := &v1.ObjectReference{
        Kind:      "PersistentVolumeClaim",
        Namespace: pvcNamespace,
        Name:      pvcName,
        // ...
    }

    event := &v1.Event{
        ObjectMeta: metav1.ObjectMeta{
            Name:      eventName,
            Namespace: pvcNamespace,  // âœ… FIX: Copy from InvolvedObject
        },
        InvolvedObject: *ref,
        // ...
    }

    // Use the namespace from the involved object
    _, err := d.clientset.CoreV1().Events(pvcNamespace).Create(ctx, event, metav1.CreateOptions{})
}
```

**Rationale:**
- Events belong to the same namespace as their InvolvedObject
- This is Kubernetes convention (events are namespaced resources)
- Aligns with CSI driver best practices
- Simple, low-risk fix

### Secondary: Update RecordNodeEvent (if exists)

**Check for:** Node-level events (if driver creates them)

**Pattern:**
```go
func (d *Driver) RecordNodeEvent(nodeName, eventType, reason, message string) {
    // Node events should use the driver's deployment namespace
    // NOT the node namespace (nodes are cluster-scoped)

    event := &v1.Event{
        ObjectMeta: metav1.ObjectMeta{
            Name:      eventName,
            Namespace: d.config.Namespace,  // e.g., "rds-csi"
        },
        InvolvedObject: v1.ObjectReference{
            Kind: "Node",
            Name: nodeName,
            // Nodes don't have namespace
        },
        // ...
    }

    _, err := d.clientset.CoreV1().Events(d.config.Namespace).Create(ctx, event, metav1.CreateOptions{})
}
```

### Tertiary: Add Unit Tests

**Added:** `pkg/driver/event_test.go`

**New Tests:**
- `TestRecordEvent_SetsNamespace` - Verify namespace copied to ObjectMeta
- `TestRecordEvent_CreatesInCorrectNamespace` - Verify Events() client called with right namespace
- `TestRecordEvent_NodeEvent` - Verify node events use driver namespace (if applicable)

---

## Testing Strategy

### Unit Tests

**Test Case 1: Event Namespace Populated**
```go
func TestRecordEvent_SetsNamespace(t *testing.T) {
    // Create driver with fake clientset
    // Call RecordEvent("test-ns", "test-pvc", ...)
    // Verify created event has ObjectMeta.Namespace = "test-ns"
}
```

**Test Case 2: Events API Called With Correct Namespace**
```go
func TestRecordEvent_CreatesInCorrectNamespace(t *testing.T) {
    // Mock Events() client
    // Verify Events("test-ns").Create() was called, not Events("").Create()
}
```

### Integration Testing (Hardware Cluster)

**Test Procedure:**

1. Deploy v0.8.2 to cluster
2. Create test PVC and pod:
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-event-fix
  namespace: default
spec:
  accessModes: [ReadWriteOnce]
  storageClassName: rds-nvme
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-event-pod
  namespace: default
spec:
  containers:
  - name: test
    image: busybox
    command: [sleep, "300"]
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: test-event-fix
EOF
```

3. Wait for pod to be Running
4. Check for events on PVC:
```bash
kubectl describe pvc test-event-fix -n default | grep -A 10 Events
```

5. Delete pod and PVC:
```bash
kubectl delete pod test-event-pod -n default
kubectl delete pvc test-event-fix -n default
```

6. Verify events were recorded:
```bash
kubectl get events -n default | grep test-event-fix
# Should show: VolumeAttached, VolumeDetached, etc.
```

7. Check driver logs have NO event errors:
```bash
kubectl logs -n rds-csi -l app=rds-csi-controller -c rds-csi-driver | grep "Unable to write event"
# Should return: no matches
```

**Success Criteria:**
- âœ… `kubectl describe pvc` shows driver events
- âœ… `kubectl get events` includes CSI driver events
- âœ… No "unable to write event" errors in driver logs
- âœ… Events have correct namespace in ObjectMeta

---

## Files Changed

### Core Implementation
- **pkg/driver/event.go**
  - Set `Event.ObjectMeta.Namespace = pvcNamespace`
  - Ensure `Events(pvcNamespace).Create()` uses same namespace
  - Document namespace handling in function comments

### Tests
- **pkg/driver/event_test.go** (create if missing)
  - Add namespace population tests
  - Add client call verification tests
  - Mock clientset for isolated testing

### Documentation
- **CONVENTIONS.md** (if exists)
  - Document event namespace conventions
  - Explain PVC events vs Node events namespace handling

---

## Impact Analysis

### What's Fixed âœ…
- **Events recorded successfully** to Kubernetes API
- **Operator visibility** restored via `kubectl describe pvc`
- **Clean logs** - no more event retry errors
- **Observability** - migration, attach, detach events visible

### What's Not Changed ðŸ”’
- **Driver functionality** - was already working (events are optional)
- **Volume operations** - no changes to volume lifecycle
- **Performance** - no overhead added
- **API compatibility** - events always should have had namespace

### Backwards Compatibility âœ…
- **API-compatible** - No CSI interface changes
- **Config-compatible** - No new flags or settings
- **State-compatible** - No changes to volume state tracking
- **Deployment-compatible** - Drop-in replacement for v0.8.1

### Kubernetes Version Compatibility
- **1.26+:** Required - strict event validation
- **1.24-1.25:** Recommended - better event handling
- **1.23 and older:** Works but not tested

---

## Deployment Strategy

### Build v0.8.2

```bash
# Checkout main/dev branch with fix
git checkout dev

# Tag the release
git tag -a v0.8.2 -m "Fix event creation namespace validation errors"
git push origin v0.8.2

# Build Docker image
make docker IMAGE_TAG=v0.8.2

# Push to registry
docker push ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.2
```

### Deploy to Hardware Cluster

```bash
# Update controller
kubectl set image deployment/rds-csi-controller \
  rds-csi-driver=ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.2 \
  -n rds-csi

# Update node plugin
kubectl set image daemonset/rds-csi-node \
  rds-csi-driver=ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.2 \
  -n rds-csi

# Verify deployment
kubectl rollout status deployment/rds-csi-controller -n rds-csi
kubectl rollout status daemonset/rds-csi-node -n rds-csi
```

### Verification

```bash
# Watch logs for event errors (should be none)
kubectl logs -n rds-csi -l app=rds-csi-controller -c rds-csi-driver -f | grep -i event

# Trigger a volume operation
kubectl apply -f test-pvc.yaml

# Check events were recorded
kubectl describe pvc test-pvc | grep Events
kubectl get events -n default | grep test-pvc

# Should see: VolumeAttached, Provisioned, etc.
```

---

## Rollback Plan

If v0.8.2 introduces issues:

```bash
# Rollback to v0.8.1
kubectl set image deployment/rds-csi-controller \
  rds-csi-driver=ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.1 \
  -n rds-csi

kubectl set image daemonset/rds-csi-node \
  rds-csi-driver=ghcr.io/3whiskeywhiskey/rds-csi-driver:v0.8.1 \
  -n rds-csi
```

**Rollback Triggers:**
- Events created in wrong namespace
- Permission errors (RBAC issues)
- Event creation still failing
- New errors in logs

**Rollback Safety:**
- No data loss - events are informational only
- No volume disruption - fix doesn't touch volume operations
- Re-deployment safe - can reapply v0.8.2 after investigation

---

## Edge Cases to Consider

### 1. PVC Without Namespace (Should Never Happen)
```go
if pvcNamespace == "" {
    klog.Warning("RecordEvent called with empty namespace, defaulting to 'default'")
    pvcNamespace = "default"
}
```

### 2. Node Events (Cluster-Scoped InvolvedObject)
```go
// Nodes don't have namespace, so event should use driver's namespace
if involvedKind == "Node" {
    event.ObjectMeta.Namespace = d.config.Namespace  // e.g., "rds-csi"
} else {
    event.ObjectMeta.Namespace = involvedNamespace
}
```

### 3. Cross-Namespace References (Shouldn't Happen)
- Events MUST be in the same namespace as their InvolvedObject
- Kubernetes won't allow cross-namespace event references
- If attempted, API will reject with permission error

---

## Alternative Approaches Considered

### âŒ Option 1: Use Event Broadcaster
**Approach:** Switch to `record.EventBroadcaster` pattern
**Pros:** Kubernetes-native event handling, automatic namespace handling
**Cons:** Requires refactoring all event calls, higher risk
**Verdict:** Too much change for a bug fix, defer to v0.9.0

### âŒ Option 2: Suppress Event Errors
**Approach:** Catch and ignore event creation errors
**Pros:** Logs stay clean
**Cons:** Doesn't fix the root cause, events still not recorded
**Verdict:** Treats symptom not disease

### âœ… Option 3: Set Namespace Directly (CHOSEN)
**Approach:** Copy namespace from InvolvedObject to Event.ObjectMeta
**Pros:** Minimal change, low risk, fixes root cause
**Cons:** None identified
**Verdict:** Best approach for patch release

---

## Metrics and Monitoring

### Success Indicators
- âœ… Zero "unable to write event" errors in logs
- âœ… Events visible via `kubectl describe pvc`
- âœ… Events visible via `kubectl get events`
- âœ… Clean driver logs (no retry spam)

### Failure Indicators
- âŒ Continued event errors in logs
- âŒ No events showing in `kubectl describe`
- âŒ Events created in wrong namespace
- âŒ Permission errors (RBAC missing for namespace)

### Log Patterns to Monitor

**Before Fix:**
```
E0204 23:44:14 event.go:289] Unable to write event: 'an empty namespace may not be set during creation'
```

**After Fix:**
```
V(4): RecordEvent: Created event VolumeAttached for PVC default/test-pvc
```

---

## Next Steps

### Immediate (v0.8.2)
1. âœ… Document bug and fix approach (this file)
2. â³ Implement namespace fix in `pkg/driver/event.go`
3. â³ Write unit tests
4. â³ Test locally with unit tests
5. â³ Build and push v0.8.2 image
6. â³ Deploy to hardware cluster
7. â³ Verify events are recorded successfully
8. â³ Monitor for 24 hours
9. â³ Tag v0.8.2 release in git

### Future Enhancements (v0.9.0+)
- **Refactor to EventBroadcaster:** Use Kubernetes-native event pattern
- **Structured Events:** Add event series support for repeated events
- **Event Rate Limiting:** Prevent event spam in high-volume scenarios
- **Event Aggregation:** Combine similar events (e.g., multiple migrations)

---

## Lessons Learned

### What Went Wrong
1. **Insufficient testing:** Event creation not tested in unit tests
2. **Kubernetes version assumption:** Didn't anticipate stricter validation
3. **Incomplete observability:** Event failures should have been more obvious

### What Went Right
1. **Non-breaking:** Bug didn't affect volume operations
2. **Quick diagnosis:** Clear error message pointed to namespace issue
3. **Simple fix:** Minimal code change required

### Process Improvements
1. **Event testing:** Add event creation to unit test suite
2. **Integration tests:** Verify events in E2E tests
3. **Observability:** Add metric for event creation failures
4. **Validation:** Test against multiple Kubernetes versions

---

**Status:** Fix documented, ready for implementation
**Risk:** Low - Single line change, events are optional
**Confidence:** High - Clear root cause, well-understood fix
**Priority:** Medium - Affects observability but not functionality
